{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics of Analyzing Twitter Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up tweepy authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = 'X'\n",
    "consumer_secret = 'X'\n",
    "access_token = 'X'\n",
    "access_token_secret = 'X'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweepy import OAuthHandler\n",
    "from tweepy import API\n",
    "\n",
    "# Consumer key authentication\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "\n",
    "# Access key authentication\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "# Set up the API with the authentication handler\n",
    "api = API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting data on keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweepy.streaming import StreamListener\n",
    "import json\n",
    "import time\n",
    "import sys\n",
    "\n",
    "class SListener(StreamListener):\n",
    "    def __init__(self, api = None, fprefix = 'streamer'):\n",
    "        self.api = api or API()\n",
    "        self.counter = 0\n",
    "        self.fprefix = fprefix\n",
    "        self.output  = open('%s_%s.json' % (self.fprefix, time.strftime('%Y%m%d-%H%M%S')), 'w')\n",
    "\n",
    "\n",
    "    def on_data(self, data):\n",
    "        if  'in_reply_to_status' in data:\n",
    "            self.on_status(data)\n",
    "        elif 'delete' in data:\n",
    "            delete = json.loads(data)['delete']['status']\n",
    "            if self.on_delete(delete['id'], delete['user_id']) is False:\n",
    "                return False\n",
    "        elif 'limit' in data:\n",
    "            if self.on_limit(json.loads(data)['limit']['track']) is False:\n",
    "                return False\n",
    "        elif 'warning' in data:\n",
    "            warning = json.loads(data)['warnings']\n",
    "            print(\"WARNING: %s\" % warning['message'])\n",
    "            return\n",
    "\n",
    "\n",
    "    def on_status(self, status):\n",
    "        self.output.write(status)\n",
    "        self.counter += 1\n",
    "        if self.counter >= 20000:\n",
    "            self.output.close()\n",
    "            self.output  = open('%s_%s.json' % (self.fprefix, time.strftime('%Y%m%d-%H%M%S')), 'w')\n",
    "            self.counter = 0\n",
    "        return\n",
    "\n",
    "\n",
    "    def on_delete(self, status_id, user_id):\n",
    "        print(\"Delete notice\")\n",
    "        return\n",
    "\n",
    "\n",
    "    def on_limit(self, track):\n",
    "        print(\"WARNING: Limitation notice received, tweets missed: %d\" % track)\n",
    "        return\n",
    "\n",
    "\n",
    "    def on_error(self, status_code):\n",
    "        print('Encountered error with status code:', status_code)\n",
    "        return \n",
    "\n",
    "\n",
    "    def on_timeout(self):\n",
    "        print(\"Timeout, sleeping for 60 seconds...\")\n",
    "        time.sleep(60)\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweepy import Stream\n",
    "\n",
    "# Set up words to track\n",
    "keywords_to_track = ['#rstats', '#python']\n",
    "\n",
    "# Instantiate the SListener object \n",
    "listen = SListener(api)\n",
    "\n",
    "# Instantiate the Stream object\n",
    "stream = Stream(auth, listen)\n",
    "\n",
    "# Begin collecting data\n",
    "stream.filter(track = keywords_to_track)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and accessing tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON\n",
    "import json\n",
    "\n",
    "# Convert from JSON to Python object\n",
    "# tweet = json.loads(tweet_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = {'text': \"Writing out the script of my @DataCamp class and I can't help but mentally read it back to myself in @hugobowne's voice.\", 'in_reply_to_status_id': None, 'geo': None, 'in_reply_to_status_id_str': None, 'is_quote_status': False, 'in_reply_to_screen_name': None, 'id': 986973961295720449, 'in_reply_to_user_id_str': None, 'metadata': {'result_type': 'recent', 'iso_language_code': 'en'}, 'in_reply_to_user_id': None, 'lang': 'en', 'created_at': 'Thu Apr 19 14:25:04 +0000 2018', 'contributors': None, 'coordinates': None, 'favorited': False, 'retweet_count': 0, 'favorite_count': 1, 'user': {'statuses_count': 71840, 'follow_request_sent': False, 'time_zone': 'Eastern Time (US & Canada)', 'profile_use_background_image': False, 'profile_background_image_url': 'http://abs.twimg.com/images/themes/theme16/bg.gif', 'screen_name': 'alexhanna', 'translator_type': 'regular', 'lang': 'en', 'followers_count': 4267, 'verified': False, 'profile_sidebar_border_color': '666666', 'profile_text_color': '333333', 'id': 661613, 'following': False, 'is_translation_enabled': False, 'profile_sidebar_fill_color': 'CCCCCC', 'geo_enabled': True, 'created_at': 'Thu Jan 18 20:37:52 +0000 2007', 'notifications': False, 'profile_banner_url': 'https://pbs.twimg.com/profile_banners/661613/1514976085', 'protected': False, 'listed_count': 246, 'profile_background_color': '000000', 'contributors_enabled': False, 'url': 'https://t.co/WGddk8Cc6v', 'is_translator': False, 'favourites_count': 23387, 'location': 'Toronto, ON', 'friends_count': 2801, 'profile_image_url': 'http://pbs.twimg.com/profile_images/980799823900180483/J9CDOX_X_normal.jpg', 'has_extended_profile': False, 'profile_background_tile': False, 'profile_link_color': '0671B8', 'description': 'Assistant professor @UofT. Protest, media, computation. Trans. Roller derby athlete @TOROLLERDERBY (Kate Silver #538). She/her.', 'entities': {'url': {'urls': [{'display_url': 'alex-hanna.com', 'url': 'https://t.co/WGddk8Cc6v', 'expanded_url': 'http://alex-hanna.com', 'indices': [0, 23]}]}, 'description': {'urls': []}}, 'profile_image_url_https': 'https://pbs.twimg.com/profile_images/980799823900180483/J9CDOX_X_normal.jpg', 'default_profile': False, 'name': 'Alex Hanna, Data Witch', 'profile_background_image_url_https': 'https://abs.twimg.com/images/themes/theme16/bg.gif', 'utc_offset': -14400, 'default_profile_image': False, 'id_str': '661613'}, 'entities': {'urls': [], 'hashtags': [], 'user_mentions': [{'name': 'DataCamp', 'screen_name': 'DataCamp', 'indices': [29, 38], 'id': 1568606814, 'id_str': '1568606814'}, {'name': 'Hugo Bowne-Anderson', 'screen_name': 'hugobowne', 'indices': [101, 111], 'id': 1092509048, 'id_str': '1092509048'}], 'symbols': []}, 'place': None, 'truncated': False, 'retweeted': False, 'id_str': '986973961295720449'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print tweet text\n",
    "print(tweet['text'])\n",
    "\n",
    "# Print tweet id\n",
    "print(tweet['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing user data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print user handle\n",
    "print(tweet['user']['screen_name'])\n",
    "\n",
    "# Print user follower count\n",
    "print(tweet['user']['followers_count'])\n",
    "\n",
    "# Print user location\n",
    "print(tweet['user']['location'])\n",
    "\n",
    "# Print user description\n",
    "print(tweet['user']['description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing retweet data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt = {'text': \"RT @hannawallach: ICYMI: NIPS/ICML/ICLR are looking for a full-time programmer to run the conferences' submission/review processes. More inâ€¦\", 'in_reply_to_status_id': None, 'geo': None, 'in_reply_to_status_id_str': None, 'is_quote_status': False, 'in_reply_to_screen_name': None, 'id': 986949027123154944, 'in_reply_to_user_id_str': None, 'metadata': {'result_type': 'recent', 'iso_language_code': 'en'}, 'in_reply_to_user_id': None, 'lang': 'en', 'created_at': 'Thu Apr 19 12:45:59 +0000 2018', 'contributors': None, 'coordinates': None, 'favorited': False, 'retweet_count': 37, 'favorite_count': 0, 'user': {'statuses_count': 71840, 'follow_request_sent': False, 'time_zone': 'Eastern Time (US & Canada)', 'profile_use_background_image': False, 'profile_background_image_url': 'http://abs.twimg.com/images/themes/theme16/bg.gif', 'screen_name': 'alexhanna', 'translator_type': 'regular', 'lang': 'en', 'followers_count': 4267, 'verified': False, 'profile_sidebar_border_color': '666666', 'profile_text_color': '333333', 'id': 661613, 'following': False, 'is_translation_enabled': False, 'profile_sidebar_fill_color': 'CCCCCC', 'geo_enabled': True, 'created_at': 'Thu Jan 18 20:37:52 +0000 2007', 'notifications': False, 'profile_banner_url': 'https://pbs.twimg.com/profile_banners/661613/1514976085', 'protected': False, 'listed_count': 246, 'profile_background_color': '000000', 'contributors_enabled': False, 'url': 'https://t.co/WGddk8Cc6v', 'is_translator': False, 'favourites_count': 23387, 'location': 'Toronto, ON', 'friends_count': 2801, 'profile_image_url': 'http://pbs.twimg.com/profile_images/980799823900180483/J9CDOX_X_normal.jpg', 'has_extended_profile': False, 'profile_background_tile': False, 'profile_link_color': '0671B8', 'description': 'Assistant professor @UofT. Protest, media, computation. Trans. Roller derby athlete @TOROLLERDERBY (Kate Silver #538). She/her.', 'entities': {'url': {'urls': [{'display_url': 'alex-hanna.com', 'url': 'https://t.co/WGddk8Cc6v', 'expanded_url': 'http://alex-hanna.com', 'indices': [0, 23]}]}, 'description': {'urls': []}}, 'profile_image_url_https': 'https://pbs.twimg.com/profile_images/980799823900180483/J9CDOX_X_normal.jpg', 'default_profile': False, 'name': 'Alex Hanna, Data Witch', 'profile_background_image_url_https': 'https://abs.twimg.com/images/themes/theme16/bg.gif', 'utc_offset': -14400, 'default_profile_image': False, 'id_str': '661613'}, 'retweeted_status': {'text': \"ICYMI: NIPS/ICML/ICLR are looking for a full-time programmer to run the conferences' submission/review processes. Mâ€¦ https://t.co/aB9Y5tTyHT\", 'in_reply_to_status_id': None, 'geo': None, 'in_reply_to_status_id_str': None, 'is_quote_status': False, 'in_reply_to_screen_name': None, 'id': 971171213216239616, 'in_reply_to_user_id_str': None, 'metadata': {'result_type': 'recent', 'iso_language_code': 'en'}, 'in_reply_to_user_id': None, 'possibly_sensitive': False, 'lang': 'en', 'created_at': 'Tue Mar 06 23:50:35 +0000 2018', 'contributors': None, 'coordinates': None, 'favorited': False, 'retweet_count': 37, 'favorite_count': 52, 'user': {'statuses_count': 1505, 'follow_request_sent': False, 'time_zone': 'Eastern Time (US & Canada)', 'profile_use_background_image': False, 'profile_background_image_url': 'http://pbs.twimg.com/profile_background_images/521040468528754688/_Ayh3ZCE.jpeg', 'screen_name': 'hannawallach', 'translator_type': 'none', 'lang': 'en', 'followers_count': 10614, 'verified': False, 'profile_sidebar_border_color': 'FFFFFF', 'profile_text_color': '333333', 'id': 823957466, 'following': True, 'is_translation_enabled': False, 'profile_sidebar_fill_color': 'DDEEF6', 'geo_enabled': False, 'created_at': 'Fri Sep 14 20:38:24 +0000 2012', 'notifications': False, 'profile_banner_url': 'https://pbs.twimg.com/profile_banners/823957466/1347986011', 'protected': False, 'listed_count': 499, 'profile_background_color': 'CCCCCC', 'contributors_enabled': False, 'url': 'https://t.co/hrcIziHrkf', 'is_translator': False, 'favourites_count': 3507, 'location': 'Brooklyn, NY', 'friends_count': 865, 'profile_image_url': 'http://pbs.twimg.com/profile_images/2623320981/kinlr53ma1flkp9jerk4_normal.jpeg', 'has_extended_profile': False, 'profile_background_tile': False, 'profile_link_color': '999999', 'description': 'MSR NYC. Machine learning, computational social science, fairness/accountability/transparency in ML. NIPS 2018 program chair, WiML co-founder, sloth enthusiast.', 'entities': {'url': {'urls': [{'display_url': 'dirichlet.net', 'url': 'https://t.co/hrcIziHrkf', 'expanded_url': 'http://dirichlet.net/', 'indices': [0, 23]}]}, 'description': {'urls': []}}, 'profile_image_url_https': 'https://pbs.twimg.com/profile_images/2623320981/kinlr53ma1flkp9jerk4_normal.jpeg', 'default_profile': False, 'name': 'Hanna Wallach', 'profile_background_image_url_https': 'https://pbs.twimg.com/profile_background_images/521040468528754688/_Ayh3ZCE.jpeg', 'utc_offset': -14400, 'default_profile_image': False, 'id_str': '823957466'}, 'entities': {'urls': [{'display_url': 'twitter.com/i/web/status/9â€¦', 'url': 'https://t.co/aB9Y5tTyHT', 'expanded_url': 'https://twitter.com/i/web/status/971171213216239616', 'indices': [117, 140]}], 'hashtags': [], 'user_mentions': [], 'symbols': []}, 'place': None, 'truncated': True, 'retweeted': False, 'id_str': '971171213216239616'}, 'entities': {'urls': [], 'hashtags': [], 'user_mentions': [{'name': 'Hanna Wallach', 'screen_name': 'hannawallach', 'indices': [3, 16], 'id': 823957466, 'id_str': '823957466'}], 'symbols': []}, 'place': None, 'truncated': False, 'retweeted': False, 'id_str': '986949027123154944'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the text of the tweet\n",
    "print(rt['text'])\n",
    "\n",
    "# Print the text of tweet which has been retweeted\n",
    "print(rt['retweeted_status']['text'])\n",
    "\n",
    "# Print the user handle of the tweet\n",
    "print(rt['user']['screen_name'])\n",
    "\n",
    "# Print the user handle of the tweet which has been retweeted\n",
    "print(rt['retweeted_status']['user']['screen_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Twitter text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweet Items and Tweet Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quoted_tweet = {'text': 'maybe if I quote tweet this lil guy https://t.co/BzbLDz9j6g', 'in_reply_to_status_id': None, 'source': '<a href=\"https://about.twitter.com/products/tweetdeck\" rel=\"nofollow\">TweetDeck</a>', 'in_reply_to_status_id_str': None, 'is_quote_status': True, 'in_reply_to_screen_name': None, 'id': 989192330832891904, 'in_reply_to_user_id_str': None, 'in_reply_to_user_id': None, 'possibly_sensitive': False, 'lang': 'en', 'timestamp_ms': '1524676804632', 'created_at': 'Wed Apr 25 17:20:04 +0000 2018', 'quote_count': 0, 'contributors': None, 'coordinates': None, 'favorited': False, 'favorite_count': 0, 'retweet_count': 0, 'user': {'statuses_count': 71926, 'follow_request_sent': None, 'time_zone': 'Eastern Time (US & Canada)', 'profile_use_background_image': False, 'profile_background_image_url': 'http://abs.twimg.com/images/themes/theme16/bg.gif', 'screen_name': 'alexhanna', 'lang': 'en', 'friends_count': 2806, 'verified': False, 'profile_sidebar_border_color': '666666', 'profile_text_color': '333333', 'id': 661613, 'following': None, 'profile_sidebar_fill_color': 'CCCCCC', 'geo_enabled': True, 'created_at': 'Thu Jan 18 20:37:52 +0000 2007', 'notifications': None, 'profile_banner_url': 'https://pbs.twimg.com/profile_banners/661613/1524231456', 'protected': False, 'listed_count': 246, 'profile_background_color': '000000', 'contributors_enabled': False, 'url': 'http://alex-hanna.com', 'is_translator': False, 'favourites_count': 23526, 'location': 'Toronto, ON', 'followers_count': 4275, 'profile_image_url': 'http://pbs.twimg.com/profile_images/980799823900180483/J9CDOX_X_normal.jpg', 'translator_type': 'regular', 'profile_background_tile': False, 'profile_link_color': '0671B8', 'description': 'Assistant professor @UofT. Protest, media, computation. Trans. Roller derby athlete @TOROLLERDERBY (Kate Silver #538). She/her.', 'profile_image_url_https': 'https://pbs.twimg.com/profile_images/980799823900180483/J9CDOX_X_normal.jpg', 'default_profile': False, 'name': 'Alex Hanna, Data Witch', 'profile_background_image_url_https': 'https://abs.twimg.com/images/themes/theme16/bg.gif', 'utc_offset': -14400, 'default_profile_image': False, 'id_str': '661613'}, 'quoted_status_id': 989191655759663105, 'geo': None, 'filter_level': 'low', 'display_text_range': [0, 35], 'reply_count': 0, 'entities': {'urls': [{'display_url': 'twitter.com/alexhanna/statâ€¦', 'url': 'https://t.co/BzbLDz9j6g', 'expanded_url': 'https://twitter.com/alexhanna/status/989191655759663105', 'indices': [36, 59]}], 'hashtags': [], 'user_mentions': [], 'symbols': []}, 'quoted_status': {'text': 'O 280 characters, 280 characters! Wherefore art thou 280 characters?\\nDeny thy JSON and refuse thy key.\\nOr, if thouâ€¦ https://t.co/MlFg4qFnEC', 'in_reply_to_status_id': None, 'source': '<a href=\"https://about.twitter.com/products/tweetdeck\" rel=\"nofollow\">TweetDeck</a>', 'in_reply_to_status_id_str': None, 'is_quote_status': False, 'in_reply_to_screen_name': None, 'id': 989191655759663105, 'in_reply_to_user_id_str': None, 'in_reply_to_user_id': None, 'lang': 'en', 'created_at': 'Wed Apr 25 17:17:23 +0000 2018', 'quote_count': 0, 'contributors': None, 'coordinates': None, 'favorited': False, 'favorite_count': 1, 'retweet_count': 0, 'user': {'statuses_count': 71925, 'follow_request_sent': None, 'time_zone': 'Eastern Time (US & Canada)', 'profile_use_background_image': False, 'profile_background_image_url': 'http://abs.twimg.com/images/themes/theme16/bg.gif', 'screen_name': 'alexhanna', 'lang': 'en', 'friends_count': 2806, 'verified': False, 'profile_sidebar_border_color': '666666', 'profile_text_color': '333333', 'id': 661613, 'following': None, 'profile_sidebar_fill_color': 'CCCCCC', 'geo_enabled': True, 'created_at': 'Thu Jan 18 20:37:52 +0000 2007', 'notifications': None, 'profile_banner_url': 'https://pbs.twimg.com/profile_banners/661613/1524231456', 'protected': False, 'listed_count': 246, 'profile_background_color': '000000', 'contributors_enabled': False, 'url': 'http://alex-hanna.com', 'is_translator': False, 'favourites_count': 23526, 'location': 'Toronto, ON', 'followers_count': 4275, 'profile_image_url': 'http://pbs.twimg.com/profile_images/980799823900180483/J9CDOX_X_normal.jpg', 'translator_type': 'regular', 'profile_background_tile': False, 'profile_link_color': '0671B8', 'description': 'Assistant professor @UofT. Protest, media, computation. Trans. Roller derby athlete @TOROLLERDERBY (Kate Silver #538). She/her.', 'profile_image_url_https': 'https://pbs.twimg.com/profile_images/980799823900180483/J9CDOX_X_normal.jpg', 'default_profile': False, 'name': 'Alex Hanna, Data Witch', 'profile_background_image_url_https': 'https://abs.twimg.com/images/themes/theme16/bg.gif', 'utc_offset': -14400, 'default_profile_image': False, 'id_str': '661613'}, 'geo': None, 'extended_tweet': {'display_text_range': [0, 191], 'full_text': 'O 280 characters, 280 characters! Wherefore art thou 280 characters?\\nDeny thy JSON and refuse thy key.\\nOr, if thou wilt not, be but sworn my love,\\nAnd Iâ€™ll no longer be a 140 character tweet.', 'entities': {'urls': [], 'hashtags': [], 'user_mentions': [], 'symbols': []}}, 'filter_level': 'low', 'reply_count': 1, 'entities': {'urls': [{'display_url': 'twitter.com/i/web/status/9â€¦', 'url': 'https://t.co/MlFg4qFnEC', 'expanded_url': 'https://twitter.com/i/web/status/989191655759663105', 'indices': [116, 139]}], 'hashtags': [], 'user_mentions': [], 'symbols': []}, 'place': None, 'truncated': True, 'retweeted': False, 'id_str': '989191655759663105'}, 'place': None, 'truncated': False, 'retweeted': False, 'quoted_status_id_str': '989191655759663105', 'id_str': '989192330832891904'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the tweet text\n",
    "print(quoted_tweet['text'])\n",
    "\n",
    "# Print the quoted tweet text\n",
    "print(quoted_tweet['quoted_status']['text'])\n",
    "\n",
    "# Print the quoted tweet's extended (140+) text\n",
    "print(quoted_tweet['quoted_status']['extended_tweet']['full_text'])\n",
    "\n",
    "# Print the quoted user location\n",
    "print(quoted_tweet['quoted_status']['user']['location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the user screen_name in 'user-screen_name'\n",
    "quoted_tweet['user-screen_name'] = quoted_tweet['user']['screen_name']\n",
    "\n",
    "# Store the quoted_status text in 'quoted_status-text'\n",
    "quoted_tweet['quoted_status-text'] = quoted_tweet['quoted_status']['text']\n",
    "\n",
    "# Store the quoted tweet's extended (140+) text in \n",
    "# 'quoted_status-extended_tweet-full_text'\n",
    "quoted_tweet['quoted_status-extended_tweet-full_text'] = quoted_tweet['quoted_status']['extended_tweet']['full_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quoted_tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A tweet flattening function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_tweets(tweets_json):\n",
    "    \"\"\" Flattens out tweet dictionaries so relevant JSON\n",
    "        is in a top-level dictionary.\"\"\"\n",
    "    tweets_list = []\n",
    "    \n",
    "    # Iterate through each tweet\n",
    "    for tweet in tweets_json:\n",
    "        tweet_obj = json.loads(tweet)\n",
    "    \n",
    "        # Store the user screen name in 'user-screen_name'\n",
    "        tweet_obj['user-screen_name'] = tweet_obj['user']['screen_name']\n",
    "        \n",
    "        # Store the user location name in 'user-location'\n",
    "        tweet_obj['user-location'] = tweet_obj['user']['location'] \n",
    "        \n",
    "        # Check if this is a 140+ character tweet\n",
    "        if 'extended_tweet' in tweet_obj:\n",
    "            # Store the extended tweet text in 'extended_tweet-full_text'\n",
    "            tweet_obj['extended_tweet-full_text'] = tweet_obj['extended_tweet']['full_text']\n",
    "    \n",
    "        if 'retweeted_status' in tweet_obj:\n",
    "            # Store the retweet user screen name in 'retweeted_status-user-screen_name'\n",
    "            tweet_obj['retweeted_status-user-screen_name'] = tweet_obj['retweeted_status']['user']['screen_name']\n",
    "\n",
    "            # Store the retweet text in 'retweeted_status-text'\n",
    "            tweet_obj['retweeted_status-text'] =tweet_obj['retweeted_status']['text']\n",
    "            \n",
    "        tweets_list.append(tweet_obj)\n",
    "    return tweets_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading tweets into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Flatten the tweets and store in `tweets`\n",
    "tweets = flatten_tweets(data_science_json)\n",
    "\n",
    "# Create a DataFrame from `tweets`\n",
    "ds_tweets = pd.DataFrame(tweets)\n",
    "\n",
    "# Print out the first 5 tweets from this dataset\n",
    "print(ds_tweets['text'].values[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find mentions of #python in 'text'\n",
    "python = ds_tweets['text'].str.contains('#python', case = False)\n",
    "\n",
    "# Print proportion of tweets mentioning #python\n",
    "print(\"Proportion of #python tweets:\", np.sum(python) / len(ds_tweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking for text in all the wrong places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_word_in_tweet(word, data):\n",
    "    \"\"\"Checks if a word is in a Twitter dataset's text. \n",
    "    Checks text and extended tweet (140+ character tweets) for tweets,\n",
    "    retweets and quoted tweets.\n",
    "    Returns a logical pandas Series.\n",
    "    \"\"\"\n",
    "    contains_column = data['text'].str.contains(word, case = False)\n",
    "    contains_column |= data['extended_tweet-full_text'].str.contains(word, case = False)\n",
    "    contains_column |= data['quoted_status-text'].str.contains(word, case = False)\n",
    "    contains_column |= data['quoted_status-extended_tweet-full_text'].str.contains(word, case = False)\n",
    "    contains_column |= data['retweeted_status-text'].str.contains(word, case = False)\n",
    "    contains_column |= data['retweeted_status-extended_tweet-full_text'].str.contains(word, case = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing #python to #rstats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find mentions of #python in all text fields\n",
    "python = check_word_in_tweet('#python', ds_tweets)\n",
    "\n",
    "# Find mentions of #rstats in all text fields\n",
    "rstats = check_word_in_tweet('#rstats', ds_tweets)\n",
    "\n",
    "# Print proportion of tweets mentioning #python\n",
    "print(\"Proportion of #python tweets:\", np.sum(python) / ds_tweets.shape[0])\n",
    "\n",
    "# Print proportion of tweets mentioning #rstats\n",
    "print(\"Proportion of #rstats tweets:\", np.sum(rstats) / ds_tweets.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating time series data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print created_at to see the original format of datetime in Twitter data\n",
    "print(ds_tweets['created_at'].head())\n",
    "\n",
    "# Convert the created_at column to np.datetime object\n",
    "ds_tweets['created_at'] = pd.to_datetime(ds_tweets['created_at'])\n",
    "\n",
    "# Print created_at to see new format\n",
    "print(ds_tweets['created_at'].head())\n",
    "\n",
    "# Set the index of ds_tweets to created_at\n",
    "ds_tweets = ds_tweets.set_index('created_at')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating mean frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a python column\n",
    "ds_tweets['python'] = check_word_in_tweet('#python', ds_tweets)\n",
    "\n",
    "# Create an rstats column\n",
    "ds_tweets['rstats'] = check_word_in_tweet('#rstats', ds_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting mean frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average of python column by day\n",
    "mean_python = ds_tweets['python'].resample('D').mean()\n",
    "\n",
    "# Average of rstats column by day\n",
    "mean_rstats = ds_tweets['rstats'].resample('D').mean()\n",
    "\n",
    "# Plot mean python by day(green)/mean rstats by day(blue)\n",
    "plt.plot(mean_python.index.day, mean_python, color = 'green')\n",
    "plt.plot(mean_rstats.index.day, mean_rstats, color = 'blue')\n",
    "\n",
    "# Add labels and show\n",
    "plt.xlabel('Day'); plt.ylabel('Frequency')\n",
    "plt.title('Language mentions over time')\n",
    "plt.legend(('#python', '#rstats'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SentimentIntensityAnalyzer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Instantiate new SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Generate sentiment scores\n",
    "sentiment_scores = ds_tweets['text'].apply(sid.polarity_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating sentiment scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the text of a positive tweet\n",
    "print(ds_tweets[sentiment > 0.6]['text'].values)\n",
    "\n",
    "# Print out the text of a negative tweet\n",
    "print(ds_tweets[sentiment < 0.6]['text'].values)\n",
    "\n",
    "# Generate average sentiment scores for #python\n",
    "sentiment_py = sentiment[check_word_in_tweet('#python', ds_tweets)].resample('D').mean()\n",
    "\n",
    "# Generate average sentiment scores for #rstats\n",
    "sentiment_r = sentiment[check_word_in_tweet('#rstats', ds_tweets)].resample('D').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting sentiment scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot average #python sentiment per day\n",
    "plt.plot(sentiment_py.index.day, sentiment_py, color = 'green')\n",
    "\n",
    "# Plot average #rstats sentiment per day\n",
    "plt.plot(sentiment_r.index.day, sentiment_r, color = 'blue')\n",
    "\n",
    "plt.xlabel('Day')\n",
    "plt.ylabel('Sentiment')\n",
    "plt.title('Sentiment of data science languages')\n",
    "plt.legend(('#python', '#rstats'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating retweet network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import networkx\n",
    "import networkx as nx\n",
    "\n",
    "# Create retweet network from edgelist\n",
    "G_rt = nx.from_pandas_edgelist(\n",
    "    sotu_retweets,\n",
    "    source = 'user-screen_name',\n",
    "    target = 'retweeted_status-user-screen_name',\n",
    "    create_using = nx.DiGraph())\n",
    " \n",
    "# Print the number of nodes\n",
    "print('Nodes in RT network:', len(G_rt.nodes()))\n",
    "\n",
    "# Print the number of edges\n",
    "print('Edges in RT network:', len(G_rt.edges()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating reply network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import networkx\n",
    "import networkx as nx\n",
    "\n",
    "# Create reply network from edgelist\n",
    "G_reply = nx.from_pandas_edgelist(\n",
    "    sotu_replies,\n",
    "    source = 'user-screen_name',\n",
    "    target = 'in_reply_to_screen_name',\n",
    "    create_using = nx.DiGraph())\n",
    "    \n",
    "# Print the number of nodes\n",
    "print('Nodes in reply network:', len(G_reply.nodes()))\n",
    "\n",
    "# Print the number of edges\n",
    "print('Edges in reply network:', len(G_reply.edges()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing retweet network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random layout positions\n",
    "pos = nx.random_layout(G_rt)\n",
    "\n",
    "# Create size list\n",
    "sizes = [x[1] for x in G_rt.degree()]\n",
    "\n",
    "# Draw the network\n",
    "nx.draw_networkx(G_rt, pos, \n",
    "    with_labels = False, \n",
    "    node_size = sizes,\n",
    "    width = 0.1, alpha = 0.7,\n",
    "    arrowsize = 2, linewidths = 0)\n",
    "\n",
    "# Turn axis off and show\n",
    "plt.axis('off'); \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-degree centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['screen_name', 'betweenness_centrality']\n",
    "\n",
    "# Generate in-degree centrality for retweets \n",
    "rt_centrality = nx.in_degree_centrality(G_rt)\n",
    "\n",
    "# Generate in-degree centrality for replies \n",
    "reply_centrality = nx.in_degree_centrality(G_reply)\n",
    "\n",
    "# Store centralities in DataFrame\n",
    "rt = pd.DataFrame(list(rt_centrality.items()), columns = column_names)\n",
    "reply = pd.DataFrame(list(reply_centrality.items()), columns = column_names)\n",
    "\n",
    "# Print first five results in descending order of centrality\n",
    "print(rt.sort_values('degree_centrality', ascending = False).head())\n",
    "\n",
    "# Print first five results in descending order of centrality\n",
    "print(reply.sort_values('degree_centrality', ascending = False).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Betweenness Centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate betweenness centrality for retweets \n",
    "rt_centrality = nx.betweenness_centrality(G_rt)\n",
    "\n",
    "# Generate betweenness centrality for replies \n",
    "reply_centrality = nx.betweenness_centrality(G_reply)\n",
    "\n",
    "# Store centralities in data frames\n",
    "rt = pd.DataFrame(list(rt_centrality.items()), columns = column_names)\n",
    "reply = pd.DataFrame(list(reply_centrality.items()), columns = column_names)\n",
    "\n",
    "# Print first five results in descending order of centrality\n",
    "print(rt.sort_values('betweenness_centrality', ascending = False).head())\n",
    "\n",
    "# Print first five results in descending order of centrality\n",
    "print(reply.sort_values('betweenness_centrality', ascending = False).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['screen_name', 'degree']\n",
    "\n",
    "# Calculate in-degrees and store in DataFrame\n",
    "degree_rt = pd.DataFrame(list(G_rt.in_degree()), columns = column_names)\n",
    "degree_reply = pd.DataFrame(list(G_reply.in_degree()), columns = column_names)\n",
    "\n",
    "# Merge the two DataFrames on screen name\n",
    "ratio = degree_rt.merge(degree_reply, on = 'screen_name', suffixes = ('_rt', '_reply'))\n",
    "\n",
    "# Calculate the ratio\n",
    "ratio['ratio'] = ratio['degree_reply'] / ratio['degree_rt']\n",
    "\n",
    "# Exclude any tweets with less than 5 retweets\n",
    "ratio = ratio[ratio['degree_rt'] >= 5]\n",
    "\n",
    "# Print out first five with highest ratio\n",
    "print(ratio.sort_values('ratio', ascending = False).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting Twitter data on the map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing user-defined location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the location of a single tweet\n",
    "print(tweet_json['user']['location'])\n",
    "\n",
    "# Flatten and load the SOTU tweets into a dataframe\n",
    "tweets_sotu = pd.DataFrame(flatten_tweets(tweets_sotu_json))\n",
    "\n",
    "# Print out top five user-defined locations\n",
    "print(tweets_sotu['user-location'].value_counts().head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBoundingBox(place):\n",
    "    \"\"\" Returns the bounding box coordinates.\"\"\"\n",
    "    return place['bounding_box']['coordinates']\n",
    "\n",
    "# Apply the function which gets bounding box coordinates\n",
    "bounding_boxes = tweets_sotu['place'].apply(getBoundingBox)\n",
    "\n",
    "# Print out the first bounding box coordinates\n",
    "print(bounding_boxes.values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateCentroid(place):\n",
    "    \"\"\" Calculates the centroid from a bounding box.\"\"\"\n",
    "    # Obtain the coordinates from the bounding box.\n",
    "    coordinates = place['bounding_box']['coordinates'][0]\n",
    "        \n",
    "    longs = np.unique( [x[0] for x in coordinates] )\n",
    "    lats  = np.unique( [x[1] for x in coordinates] )\n",
    "\n",
    "    if len(longs) == 1 and len(lats) == 1:\n",
    "        # return a single coordinate\n",
    "        return (longs[0], lats[0])\n",
    "    elif len(longs) == 2 and len(lats) == 2:\n",
    "        # If we have two longs and lats, we have a box.\n",
    "        central_long = np.sum(longs) / 2\n",
    "        central_lat  = np.sum(lats) / 2\n",
    "    else:\n",
    "        raise ValueError(\"Non-rectangular polygon not supported: %s\" % \n",
    "            \",\".join(map(lambda x: str(x), coordinates)) )\n",
    "\n",
    "    return (central_long, central_lat)\n",
    "    \n",
    "# Calculate the centroids of place     \n",
    "centroids = tweets_sotu['place'].apply(calculateCentroid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Basemap map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Basemap\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set up the US bounding box\n",
    "us_boundingbox = [-125, 22, -64, 50] \n",
    "\n",
    "# Set up the Basemap object\n",
    "m = Basemap(llcrnrlon = us_boundingbox[0],\n",
    "            llcrnrlat = us_boundingbox[1],\n",
    "            urcrnrlon = us_boundingbox[2],\n",
    "            urcrnrlat = us_boundingbox[3],\n",
    "            projection='merc')\n",
    "\n",
    "# Draw continents in white,\n",
    "# coastlines and countries in gray\n",
    "m.fillcontinents(color='white')\n",
    "m.drawcoastlines(color='gray')\n",
    "m.drawcountries(color='gray')\n",
    "\n",
    "# Draw the states and show the plot\n",
    "m.drawstates(color='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting centroid coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the centroids for the dataset\n",
    "# and isolate longitudue and latitudes\n",
    "centroids = tweets_sotu['place'].apply(calculateCentroid)\n",
    "lon = [x[0] for x in centroids]\n",
    "lat = [x[1] for x in centroids]\n",
    "\n",
    "# Draw continents, coastlines, countries, and states\n",
    "m.fillcontinents(color='white', zorder = 0)\n",
    "m.drawcoastlines(color='gray')\n",
    "m.drawcountries(color='gray')\n",
    "m.drawstates(color='gray')\n",
    "\n",
    "# Draw the points and show the plot\n",
    "m.scatter(lat, lon, latlon = True, alpha = 0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coloring by sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sentiment scores\n",
    "sentiment_scores = tweets_sotu['text'].apply(sid.polarity_scores)\n",
    "\n",
    "# Isolate the compound element\n",
    "sentiment_scores = [x['compound'] for x in sentiment_scores]\n",
    "\n",
    "# Draw the points\n",
    "m.scatter(lon, lat, latlon = True, \n",
    "           c = sentiment_scores,\n",
    "           cmap = 'coolwarm', alpha = 0.7)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
