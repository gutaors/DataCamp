{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data \n",
    "grouped_purchase_data = purchase_data.groupby(by = ['device', 'gender'])\n",
    "\n",
    "# Aggregate the data\n",
    "purchase_summary = grouped_purchase_data.agg({'price': ['mean', 'median', 'std']})\n",
    "\n",
    "# Examine the results\n",
    "print(purchase_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_time = np.where( conv_sub_data.subscription_date.notnull(), \n",
    "                    (conv_sub_data.subscription_date - conv_sub_data.lapse_date).dt.days, \n",
    "                    pd.NaT)\n",
    "\n",
    "conv_sub_data['sub_time'] = sub_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute max_purchase_date\n",
    "max_purchase_date = current_date - timedelta(days=28)\n",
    "\n",
    "# Filter to only include users who registered before our max date\n",
    "purchase_data_filt = purchase_data[purchase_data.reg_date < max_purchase_date]\n",
    "\n",
    "# Filter to contain only purchases within the first 28 days of registration\n",
    "purchase_data_filt = purchase_data_filt[(purchase_data_filt.date <= \n",
    "                        purchase_data_filt.reg_date + timedelta(days=28))]\n",
    "\n",
    "# Output the mean price paid per purchase\n",
    "print(purchase_data_filt.price.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the max registration date to be one month before today\n",
    "max_reg_date = current_date - timedelta(days=28)\n",
    "\n",
    "# Find the month 1 values \n",
    "month1 = np.where((purchase_data.reg_date < max_reg_date) &\n",
    "                 (purchase_data.date <= purchase_data.reg_date + timedelta(days=28)),\n",
    "                 purchase_data.price, np.NaN)\n",
    "\n",
    "# Update the value in the DataFrame \n",
    "purchase_data['month1'] = month1\n",
    "\n",
    "# Group the data by gender and device \n",
    "purchase_data_upd = purchase_data.groupby(by=['gender', 'device'], as_index=False) \n",
    "\n",
    "# Aggregate the month1 and price data \n",
    "purchase_summary = purchase_data_upd.agg(\n",
    "                        {'month1': ['mean', 'median'],\n",
    "                        'price': ['mean', 'median']})\n",
    "\n",
    "# Examine the results \n",
    "print(purchase_summary)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1993-01-27 --\"%Y-%m-%d\"\n",
    "\n",
    "05/13/2017 05:45:37 -- \"%m/%d/%Y %H:%M:%S\"\n",
    "\n",
    "September 01, 2017 -- \"%B %d, %Y\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Second Module "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data and aggregate first_week_purchases\n",
    "user_purchases = user_purchases.groupby(by=['reg_date', 'uid']).agg({'first_week_purchases': ['sum']})\n",
    "\n",
    "# Reset the indexes\n",
    "user_purchases.columns = user_purchases.columns.droplevel(level=1)\n",
    "user_purchases.reset_index(inplace=True)\n",
    "\n",
    "# Find the average number of purchases per day by first-week users\n",
    "user_purchases = user_purchases.groupby(by=['reg_date']).agg({'first_week_purchases': ['mean']})\n",
    "user_purchases.columns = user_purchases.columns.droplevel(level=1)\n",
    "user_purchases.reset_index(inplace=True)\n",
    "\n",
    "# Plot the results\n",
    "user_purchases.plot(x='reg_date', y='first_week_purchases')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the data\n",
    "country_pivot = pd.pivot_table(user_purchases_device, values=['first_week_purchases'], columns=['country'], index=['reg_date'])\n",
    "print(country_pivot.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the data\n",
    "device_pivot = pd.pivot_table(user_purchases_device, values=['first_week_purchases'], columns=['device'], index=['reg_date'])\n",
    "print(device_pivot.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the average first week purchases for each country by registration date\n",
    "country_pivot.plot(x='reg_date', y=['USA', 'CAN', 'FRA', 'BRA', 'TUR', 'DEU'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the average first week purchases for each device by registration date\n",
    "device_pivot.plot(x='reg_date', y=['and', 'iOS'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Rolling moving Average \n",
    "\n",
    "# SEASONALITY AND MOVING AVERAGE \n",
    "\n",
    "# Compute 7_day_rev\n",
    "daily_revenue['7_day_rev'] = daily_revenue.revenue.rolling(window=7,center=False).mean()\n",
    "\n",
    "# Compute 28_day_rev\n",
    "daily_revenue['28_day_rev'] = daily_revenue.revenue.rolling(window=28,center=False).mean()\n",
    "    \n",
    "# Compute 365_day_rev\n",
    "daily_revenue['365_day_rev'] = daily_revenue.revenue.rolling(window=365,center=False).mean()\n",
    "    \n",
    "# Plot date, and revenue, along with the 3 rolling functions (in order)    \n",
    "daily_revenue.plot(x='date', y=['revenue', '7_day_rev', '28_day_rev', '365_day_rev', ])\n",
    "plt.show()\n",
    "\n",
    "# Alternative \n",
    "df.apply(lambda x: x.dropna().rolling(3).mean().reindex(x.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exponential rolling average\n",
    "\n",
    "# EXPONENTIAL ROLLING AVERAGE & OVER/UNDER SMOOTHING\n",
    "\n",
    "# Calculate 'small_scale'\n",
    "daily_revenue['small_scale'] = daily_revenue.revenue.ewm(span=10).mean()\n",
    "\n",
    "# Calculate 'medium_scale'\n",
    "daily_revenue['medium_scale'] = daily_revenue.revenue.ewm(span=100).mean()\n",
    "\n",
    "# Calculate 'large_scale'\n",
    "daily_revenue['large_scale'] = daily_revenue.revenue.ewm(span=500).mean()\n",
    "\n",
    "# Plot 'date' on the x-axis and, our three averages and 'revenue'\n",
    "# on the y-axis\n",
    "daily_revenue.plot(x = 'date', y =['revenue', 'small_scale', 'medium_scale', 'large_scale'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot user_revenue\n",
    "pivoted_data = pd.pivot_table(user_revenue, values ='revenue', columns=['device', 'gender'], index='month')\n",
    "pivoted_data = pivoted_data[1:(len(pivoted_data) -1 )]\n",
    "\n",
    "# Create and show the plot\n",
    "pivoted_data.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3 moldule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimental units: Revenue per user day\n",
    "    \n",
    "# Extract the 'day'; value from the timestamp\n",
    "purchase_data.date = purchase_data.date.dt.floor('d')\n",
    "\n",
    "# Replace the NaN price values with 0 \n",
    "purchase_data.price = np.where(np.isnan(purchase_data.price), 0, purchase_data.price)\n",
    "\n",
    "# Aggregate the data by 'uid' & 'date'\n",
    "purchase_data_agg = purchase_data.groupby(by=['uid', 'date'], as_index=False)\n",
    "revenue_user_day = purchase_data_agg.sum()\n",
    "\n",
    "# Calculate the final average\n",
    "revenue_user_day = revenue_user_day.price.mean()\n",
    "print(revenue_user_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge and group the datasets\n",
    "purchase_data = demographics_data.merge(paywall_views,  how='inner', on=['uid'])\n",
    "purchase_data.date = purchase_data.date.dt.floor('d')\n",
    "\n",
    "# Group and aggregate our combined dataset \n",
    "daily_purchase_data = purchase_data.groupby(by=['date'], as_index=False)\n",
    "daily_purchase_data = daily_purchase_data.agg({'purchase': ['sum', 'count']})\n",
    "\n",
    "# Find the mean of each field and then multiply by 1000 to scale the result\n",
    "daily_purchases = daily_purchase_data.purchase['sum'].mean()\n",
    "daily_paywall_views = daily_purchase_data.purchase['count'].mean()\n",
    "daily_purchases = daily_purchases * 1000\n",
    "daily_paywall_views = daily_paywall_views * 1000\n",
    "\n",
    "print(daily_purchases)\n",
    "print(daily_paywall_views)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "medium_sensitivity = 0.2\n",
    "\n",
    "# Find the conversion rate when increased by the percentage of the sensitivity above\n",
    "medium_conversion_rate = conversion_rate * (1 + medium_sensitivity) \n",
    "\n",
    "# Apply the new conversion rate to find how many more users per day that translates to\n",
    "medium_purchasers = daily_paywall_views * medium_conversion_rate\n",
    "\n",
    "# Subtract the initial daily_purcahsers number from this new value to see the lift\n",
    "purchaser_lift = medium_purchasers - daily_purchases\n",
    "\n",
    "print(medium_conversion_rate)\n",
    "print(medium_purchasers)\n",
    "print(purchaser_lift)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Find the number of paywall views \n",
    "n = purchase_data.purchase.count()\n",
    "\n",
    "# Calculate the quantitiy \"v\"\n",
    "v = conversion_rate * (1 - conversion_rate) \n",
    "\n",
    "# Calculate the variance and standard error of the estimate\n",
    "var = v / n \n",
    "se = var**0.5\n",
    "\n",
    "print(var)\n",
    "print(se)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring the power calculation\n",
    "\n",
    "# Look at the impact of sample size increase on power\n",
    "n_param_one = get_power(n=1000, p1=p1, p2=p2, cl=cl)\n",
    "n_param_two = get_power(n=2000, p1=p1, p2=p2, cl=cl)\n",
    "\n",
    "# Look at the impact of confidence level increase on power\n",
    "alpha_param_one = get_power(n=n1, p1=p1, p2=p2, cl=0.8)\n",
    "alpha_param_two = get_power(n=n1, p1=p1, p2=p2, cl=.95)\n",
    "    \n",
    "# Compare the ratios\n",
    "print(n_param_two / n_param_one)\n",
    "print(alpha_param_one / alpha_param_two)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the sample size\n",
    "\n",
    "def get_sample_size(power, p1, p2, cl, max_n=1000000):\n",
    "    n = 1 \n",
    "    while n <= max_n:\n",
    "        tmp_power = get_power(n, p1, p2, cl)\n",
    "\n",
    "        if tmp_power >= power: \n",
    "            return n \n",
    "        else: \n",
    "            n = n + 100\n",
    "\n",
    "    return \"Increase Max N Value\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the demographics and purchase data to only include paywall views\n",
    "purchase_data = demographics_data.merge(paywall_views, how='inner', on=['uid'])\n",
    "                            \n",
    "# Find the conversion rate\n",
    "conversion_rate = (sum(purchase_data.purchase) / purchase_data.purchase.count())\n",
    "\n",
    "# Desired Power: 0.95\n",
    "# CL 0.90\n",
    "# Percent Lift: 0.1\n",
    "p2 = conversion_rate * (1 + 0.1)\n",
    "sample_size = get_sample_size(0.95, conversion_rate, p2, 0.9)\n",
    "print(sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Module 4 \n",
    "## Analyzing A/B Testing Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the unique users in each group \n",
    "results = ab_test_results.groupby('group').agg({'uid': pd.Series.nunique}) \n",
    "\n",
    "# Find the overall number of unique users using \"len\" and \"unique\"\n",
    "unique_users = len(ab_test_results.uid.unique()) \n",
    "\n",
    "# Find the percentage in each group\n",
    "results = results / unique_users * 100\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the unique users in each group, by device and gender\n",
    "results = ab_test_results.groupby(by=['group', 'device', 'gender']).agg({'uid': pd.Series.nunique}) \n",
    "\n",
    "# Find the overall number of unique users using \"len\" and \"unique\"\n",
    "unique_users = len(ab_test_results.uid.unique())\n",
    "\n",
    "# Find the percentage in each group\n",
    "results = results / unique_users * 100\n",
    "print(results)\n",
    "\n",
    "                               uid\n",
    "    group device gender           \n",
    "    C     and    F       14.896625\n",
    "                 M       13.518289\n",
    "          iOS    F       11.309419\n",
    "                 M       10.196148\n",
    "    V     and    F       14.861283\n",
    "                 M       13.659657\n",
    "          iOS    F       10.920657\n",
    "                 M       10.637922"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Thinking critically about p-values\n",
    "\n",
    "Below are four statements about p-values. It is up to you to identify which one is true. This is important because p-values are an unintuitive concept and being able to reason about them correctly is extremely important in most statistical work.\n",
    "\n",
    "Answer: \n",
    "- The p-value is the probability of observing a value as or more extreme than the one observed under the Null Hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para calcular o intervalo de confiança\n",
    "# Two sided CI\n",
    "def get_ci(lift, alpha, sd):\n",
    "  val = abs(stats.norm.ppf((1 - alpha)/2))\n",
    "  lwr_bnd = lift - val * sd\n",
    "  upr_bnd = lift + val * sd\n",
    "  return_val = (lwr_bnd, upr_bnd)\n",
    "  return(return_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Intuition behind statistical significance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_pvalue(con_conv, test_conv, con_size, test_size):  \n",
    "    lift =  - abs(test_conv - con_conv)\n",
    "\n",
    "    scale_one = con_conv * (1 - con_conv) * (1 / con_size)\n",
    "    scale_two = test_conv * (1 - test_conv) * (1 / test_size)\n",
    "    scale_val = (scale_one + scale_two)**0.5\n",
    "\n",
    "    p_value = 2 * stats.norm.cdf(lift, loc = 0, scale = scale_val )\n",
    "    \n",
    "    # Check for statistical significance\n",
    "    if p_value >= 0.05:\n",
    "        print(\"Not Significant\")\n",
    "    else:\n",
    "        print(\"Significant Result\")\n",
    "\n",
    "    return p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the p-value\n",
    "p_value = get_pvalue(con_conv=0.1, test_conv=0.17, con_size=1000, test_size=1000)\n",
    "print(p_value)\n",
    "\n",
    "<script.py> output:\n",
    "    4.131297741047306e-06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the p-value\n",
    "p_value = get_pvalue(con_conv=0.1, test_conv=0.15, con_size=100, test_size=100)\n",
    "print(p_value)\n",
    "\n",
    "<script.py> output:\n",
    "    0.28366948940702086"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the p-value\n",
    "p_value = get_pvalue(con_conv=0.48, test_conv=0.50, con_size=1000, test_size=1000)\n",
    "print(p_value)\n",
    "\n",
    "<script.py> output:\n",
    "    0.370901935824383"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for statistical significance\n",
    "if p_value >= 0.05:\n",
    "    print(\"Not Significant\")\n",
    "else:\n",
    "    print(\"Significant Result\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the p-value\n",
    "p_value = get_pvalue(con_conv=cont_conv, test_conv=test_conv, con_size=cont_size, test_size=test_size)\n",
    "print(p_value)\n",
    "\n",
    "# Check for statistical significance\n",
    "if p_value >= 0.05:\n",
    "    print(\"Not Significant\")\n",
    "else:\n",
    "    print(\"Significant Result\")\n",
    "    \n",
    "# Compute the p-value\n",
    "p_value = get_pvalue(con_conv=cont_conv, test_conv=test_conv, con_size=cont_size, test_size=test_size)\n",
    "print(p_value)\n",
    "\n",
    "# Check for statistical significance\n",
    "if p_value >= 0.05:\n",
    "    print(\"Not Significant\")\n",
    "else:\n",
    "    print(\"Significant Result\")\n",
    "\n",
    "<script.py> output:\n",
    "    0.04900185792087508\n",
    "    Significant Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ci(value, cl, sd):\n",
    "  loc = sci.norm.ppf(1 - cl/2)\n",
    "  rng_val = sci.norm.cdf(loc - value/sd)\n",
    "\n",
    "  lwr_bnd = value - rng_val\n",
    "  upr_bnd = value + rng_val \n",
    "\n",
    "  return_val = (lwr_bnd, upr_bnd)\n",
    "  return(return_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and print the confidence interval\n",
    "confidence_interval  = get_ci(1, 0.975, 0.5)\n",
    "print(confidence_interval)\n",
    "\n",
    "(0.9755040421682947, 1.0244959578317054)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and print the confidence interval\n",
    "confidence_interval  = get_ci(1, .95, 2)\n",
    "print(confidence_interval)\n",
    "\n",
    "(0.6690506448818785, 1.3309493551181215)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and print the confidence interval\n",
    "confidence_interval  = get_ci(1, .95, 2)\n",
    "print(confidence_interval)\n",
    "\n",
    "(0.6690506448818785, 1.3309493551181215)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculating confidence intervals\n",
    "\n",
    "# Calculate the mean of our lift distribution \n",
    "lift_mean = abs(test_conv - cont_conv) \n",
    "\n",
    "# Calculate variance and standard deviation \n",
    "lift_variance = (1 - test_conv) * test_conv /test_size + (1 - cont_conv) * cont_conv / cont_size\n",
    "lift_sd = lift_variance**0.5\n",
    "\n",
    "# Find the confidence intervals with cl = 0.95\n",
    "confidence_interval = get_ci(lift_mean, 0.95, lift_sd)\n",
    "print(confidence_interval)\n",
    "\n",
    "(0.011039999822042502, 0.011040000177957487)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting the distribution\n",
    "\n",
    "# Compute the standard deviations\n",
    "control_sd = cont_var**0.5\n",
    "test_sd = test_var**0.5\n",
    "\n",
    "# Create the range of x values \n",
    "control_line = np.linspace( cont_conv - 3 * control_sd, cont_conv + 3 * control_sd , 100)\n",
    "test_line = np.linspace( test_conv - 3 * test_sd,  test_conv + 3 * test_sd , 100)\n",
    "\n",
    "# Plot the distribution \n",
    "plt.plot(control_line, mlab.normpdf(control_line, cont_conv, control_sd))\n",
    "plt.plot(test_line, mlab.normpdf(test_line,test_conv, test_sd))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting the difference distribution\n",
    "\n",
    "# Find the lift mean and standard deviation\n",
    "lift_mean = abs(test_conv - cont_conv)\n",
    "lift_sd = (test_var + cont_var) ** 0.5\n",
    "\n",
    "# Generate the range of x-values\n",
    "lift_line = np.linspace(lift_mean - 3 * lift_sd, lift_mean + 3 * lift_sd, 100)\n",
    "\n",
    "# Plot the lift distribution\n",
    "plt.plot(lift_line, mlab.normpdf(lift_line, lift_mean, lift_sd))\n",
    "\n",
    "# Add the annotation lines\n",
    "plt.axvline(x = lift_mean, color = 'green')\n",
    "plt.axvline(x = lwr_ci, color = 'red')\n",
    "plt.axvline(x = upr_ci, color = 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
