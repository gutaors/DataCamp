{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Machine Learning for Finance in Python"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Time series data is all around us; some examples are the weather, human behavioral patterns as consumers and members of society, and financial data. In this course, you'll learn how to calculate technical indicators from historical stock data, and how to create features and targets out of the historical stock data. You'll understand how to prepare our features for linear models, xgboost models, and neural network models. We will then use linear models, decision trees, random forests, and neural networks to predict the future price of stocks in the US markets. You will also learn how to evaluate the performance of the various models we train in order to optimize them, so our predictions have enough accuracy to make a stock trading strategy profitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  MODULE 1\n",
    "\n",
    "### Preparing data and a linear model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In this chapter, we will learn how machine learning can be used in finance. We will also explore some stock data, and prepare it for machine learning algorithms. Finally, we will fit our first machine learning model -- a linear model, in order to predict future price changes of stocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Explore the data with some EDA\n",
    "\n",
    "print(lng_df.head())  # examine the DataFrames\n",
    "print(spy_df.head())  # examine the SPY DataFrame\n",
    "\n",
    "# Plot the Adj_Close columns for SPY and LNG\n",
    "spy_df['Adj_Close'].plot(label='SPY', legend=True)\n",
    "lng_df['Adj_Close'].plot(label=\"LNG\", legend=True, secondary_y=True)\n",
    "plt.show()  # show the plot\n",
    "plt.clf()  # clear the plot space\n",
    "\n",
    "# Histogram of the daily price change percent of Adj_Close for LNG\n",
    "lng_df['Adj_Close'].pct_change().plot.hist(bins=50)\n",
    "plt.xlabel('adjusted close 1-day percent change')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Correlations\n",
    "\n",
    "# Create 5-day % changes of Adj_Close for the current day, and 5 days in the future\n",
    "lng_df['5d_future_close'] = lng_df['Adj_Close'].shift(-5)\n",
    "lng_df['5d_close_future_pct'] = lng_df['5d_future_close'].pct_change(5)\n",
    "lng_df['5d_close_pct'] = lng_df['Adj_Close'].pct_change(5)\n",
    "\n",
    "# Calculate the correlation matrix between the 5d close pecentage changes (current and future)\n",
    "corr = lng_df[['5d_close_pct', '5d_close_future_pct']].corr()\n",
    "print(corr)\n",
    "\n",
    "# Scatter the current 5-day percent change vs the future 5-day percent change\n",
    "plt.scatter(lng_df['5d_close_pct'], lng_df['5d_close_future_pct'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create moving average and RSI features\n",
    "\n",
    "feature_names = ['5d_close_pct']  # a list of the feature names for later\n",
    "\n",
    "# Create moving averages and rsi for timeperiods of 14, 30, 50, and 200\n",
    "for n in [14, 30, 50, 200]:\n",
    "\n",
    "    # Create the moving average indicator and divide by Adj_Close\n",
    "    lng_df['ma' + str(n)] = talib.SMA(lng_df['Adj_Close'].values,\n",
    "                              timeperiod=n) / lng_df['Adj_Close']\n",
    "    # Create the RSI indicator\n",
    "    lng_df['rsi' + str(n)] = talib.RSI(lng_df['Adj_Close'].values, timeperiod=n)\n",
    "    \n",
    "    # Add rsi and moving average to the feature name list\n",
    "    feature_names = feature_names + ['ma' + str(n), 'rsi' + str(n)]\n",
    "\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create features and targets\n",
    "\n",
    "# Drop all na values\n",
    "lng_df = lng_df.dropna()\n",
    "\n",
    "# Create features and targets\n",
    "# use feature_names for features; 5d_close_future_pct for targets\n",
    "features = lng_df[feature_names]\n",
    "targets = lng_df['5d_close_future_pct']\n",
    "\n",
    "# Create DataFrame from target column and feature columns\n",
    "feature_and_target_cols = ['5d_close_future_pct'] + feature_names\n",
    "feat_targ_df = lng_df[feature_and_target_cols]\n",
    "\n",
    "# Calculate correlation matrix\n",
    "corr = feat_targ_df.corr()\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check the correlations\n",
    "\n",
    "# Plot heatmap of correlation matrix\n",
    "sns.heatmap(corr, annot=True)\n",
    "plt.yticks(rotation=0); plt.xticks(rotation=90)  # fix ticklabel directions\n",
    "plt.tight_layout()  # fits plot area to the plot, \"tightly\"\n",
    "plt.show()  # show the plot\n",
    "plt.clf()  # clear the plot area\n",
    "\n",
    "# Create a scatter plot of the most highly correlated variable with the target\n",
    "plt.scatter(lng_df['ma200'], lng_df[\"5d_close_future_pct\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create train and test features\n",
    "\n",
    "# Import the statsmodels.api library with the alias sm\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Add a constant to the features\n",
    "linear_features = sm.add_constant(features)\n",
    "\n",
    "# Create a size for the training set that is 85% of the total number of samples\n",
    "train_size = int(0.85 * features.shape[0])\n",
    "train_features = linear_features[:train_size]\n",
    "train_targets = targets[:train_size]\n",
    "test_features = linear_features[train_size:]\n",
    "test_targets = targets[train_size:]\n",
    "\n",
    "print(linear_features.shape, train_features.shape, test_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fit a linear model\n",
    "\n",
    "# Create the linear model and complete the least squares fit\n",
    "model = sm.OLS(train_targets, train_features)\n",
    "results = model.fit()  # fit the model\n",
    "print(results.summary())\n",
    "\n",
    "# examine pvalues\n",
    "# Features with p <= 0.05 are typically considered significantly different from 0\n",
    "print(results.pvalues)\n",
    "\n",
    "# Make predictions from our model for train and test sets\n",
    "train_predictions = results.predict(train_features)\n",
    "test_predictions = results.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluate our results\n",
    "\n",
    "# Scatter the predictions vs the targets with 80% transparency\n",
    "plt.scatter(train_predictions, train_targets, alpha=0.2, color='b', label='train')\n",
    "plt.scatter(test_predictions, test_targets, alpha=0.2, color='r', label='test')\n",
    "\n",
    "# Plot the perfect prediction line\n",
    "xmin, xmax = plt.xlim()\n",
    "plt.plot(np.arange(xmin, xmax, 0.01), np.arange(xmin, xmax, 0.01), c='k')\n",
    "\n",
    "# Set the axis labels and show the plot\n",
    "plt.xlabel('predictions')\n",
    "plt.ylabel('actual')\n",
    "plt.legend()  # show the legend\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODULE 2\n",
    "\n",
    "### Machine learning tree methods"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Learn how to use tree-based machine learning models to predict future values of a stock's price, as well as how to use forest-based machine learning methods for regression and feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature engineering from volume\n",
    "\n",
    "# Create 2 new volume features, 1-day % change and 5-day SMA of the % change\n",
    "new_features = ['Adj_Volume_1d_change', 'Adj_Volume_1d_change_SMA']\n",
    "\n",
    "feature_names.extend(new_features)\n",
    "\n",
    "lng_df['Adj_Volume_1d_change'] = lng_df['Adj_Volume'].pct_change()\n",
    "lng_df['Adj_Volume_1d_change_SMA'] = talib.SMA(lng_df['Adj_Volume_1d_change'].values,\n",
    "                        timeperiod=5)\n",
    "\n",
    "# Plot histogram of volume % change data\n",
    "lng_df['Adj_Volume_1d_change_SMA'].plot(kind='hist', sharex=False, bins=50)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create day-of-week features\n",
    "\n",
    "# Use pandas' get_dummies function to get dummies for day of the week\n",
    "days_of_week = pd.get_dummies(lng_df.index.dayofweek,\n",
    "                              prefix='weekday',\n",
    "                              drop_first=True)\n",
    "\n",
    "# Set the index as the original dataframe index for merging\n",
    "days_of_week.index = lng_df.index\n",
    "\n",
    "# Join the dataframe with the days of week dataframe\n",
    "lng_df = pd.concat([lng_df, days_of_week], axis=1)\n",
    "\n",
    "# Add days of week to feature names\n",
    "feature_names.extend(['weekday_' + str(i) for i in range(1, 5)])\n",
    "lng_df.dropna(inplace=True)  # drop missing values in-place\n",
    "print(lng_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Examine correlations of the new features\n",
    "\n",
    "# Add the weekday labels to the new_features list\n",
    "new_features.extend([\"weekday_\" + str(i) for i in range(1, 5)])\n",
    "\n",
    "# Plot the correlations between the new features and the targets\n",
    "sns.heatmap(lng_df[new_features + ['5d_close_future_pct']].corr(), annot=True)\n",
    "plt.yticks(rotation=0)  # ensure y-axis ticklabels are horizontal\n",
    "plt.xticks(rotation=90)  # ensure x-axis ticklabels are vertical\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fit a decision tree\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Create a decision tree regression model with default arguments\n",
    "decision_tree = DecisionTreeRegressor()\n",
    "\n",
    "# Fit the model to the training features and targets\n",
    "decision_tree.fit(train_features, train_targets)\n",
    "\n",
    "# Check the score on train and test\n",
    "print(decision_tree.score(train_features, train_targets))\n",
    "print(decision_tree.score(test_features, test_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try different max depths\n",
    "\n",
    "# Loop through a few different max depths and check the performance\n",
    "for d in [3, 5, 10]:\n",
    "    # Create the tree and fit it\n",
    "    decision_tree = DecisionTreeRegressor(max_depth=d)\n",
    "    decision_tree.fit(train_features, train_targets)\n",
    "\n",
    "    # Print out the scores on train and test\n",
    "    print('max_depth=', str(d))\n",
    "    print(decision_tree.score(train_features, train_targets))\n",
    "    print(decision_tree.score(test_features, test_targets), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check our results\n",
    "\n",
    "# Use the best max_depth of 3 from last exercise to fit a decision tree\n",
    "decision_tree = DecisionTreeRegressor(max_depth=3)\n",
    "decision_tree.fit(train_features, train_targets)\n",
    "\n",
    "# Predict values for train and test\n",
    "train_predictions = decision_tree.predict(train_features)\n",
    "test_predictions = decision_tree.predict(test_features)\n",
    "\n",
    "# Scatter the predictions vs actual values\n",
    "plt.scatter(train_predictions, train_targets, label='train')\n",
    "plt.scatter(test_predictions, test_targets, label='test')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fit a random forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Create the random forest model and fit to the training data\n",
    "rfr = RandomForestRegressor(n_estimators=200)\n",
    "rfr.fit(train_features,train_targets)\n",
    "\n",
    "# Look at the R^2 scores on train and test\n",
    "print(rfr.score(train_features, train_targets))\n",
    "print(rfr.score(test_features, test_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tune random forest hyperparameters\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# Create a dictionary of hyperparameters to search\n",
    "grid = {'n_estimators':[200], 'max_depth': [3], 'max_features': [4,8], 'random_state': [42]}\n",
    "\n",
    "test_scores = []\n",
    "\n",
    "# Loop through the parameter grid, set the hyperparameters, and save the scores\n",
    "for g in ParameterGrid(grid):\n",
    "    rfr.set_params(**g)  # ** is \"unpacking\" the dictionary\n",
    "    rfr.fit(train_features, train_targets)\n",
    "    test_scores.append(rfr.score(test_features, test_targets))\n",
    "\n",
    "# Find best hyperparameters from the test score and print\n",
    "best_idx = np.argmax(test_scores)\n",
    "print(test_scores[best_idx], ParameterGrid(grid)[best_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluate performance\n",
    "\n",
    "# Use the best hyperparameters from before to fit a random forest model\n",
    "rfr = RandomForestRegressor(n_estimators=200, max_depth=3, max_features=4, random_state=42)\n",
    "rfr.fit(train_features, train_targets)\n",
    "\n",
    "# Make predictions with our model\n",
    "train_predictions = rfr.predict(train_features)\n",
    "test_predictions = rfr.predict(test_features)\n",
    "\n",
    "# Create a scatter plot with train and test actual vs predictions\n",
    "plt.scatter(train_targets, train_predictions, label='train')\n",
    "plt.scatter(test_targets, test_predictions, label='test')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Random forest feature importances\n",
    "\n",
    "# Get feature importances from our random forest model\n",
    "importances = rfr.feature_importances_\n",
    "\n",
    "# Get the index of importances from greatest importance to least\n",
    "sorted_index = np.argsort(importances)[::-1]\n",
    "x = range(len(importances))\n",
    "\n",
    "# Create tick labels \n",
    "labels = np.array(feature_names)[sorted_index]\n",
    "plt.bar(x, importances[sorted_index], tick_label=labels)\n",
    "\n",
    "# Rotate tick labels to vertical\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A gradient boosting model\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Create GB model -- hyperparameters have already been searched for you\n",
    "gbr = GradientBoostingRegressor(max_features=4,\n",
    "                                learning_rate=0.01,\n",
    "                                n_estimators=200,\n",
    "                                subsample=0.6,\n",
    "                                random_state=42)\n",
    "gbr.fit(train_features, train_targets)\n",
    "\n",
    "print(gbr.score(train_features, train_targets))\n",
    "print(gbr.score(test_features, test_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gradient boosting feature importances\n",
    "\n",
    "# Extract feature importances from the fitted gradient boosting model\n",
    "feature_importances = gbr.feature_importances_\n",
    "\n",
    "# Get the indices of the largest to smallest feature importances\n",
    "sorted_index = np.argsort(feature_importances)[::-1]\n",
    "x = range(features.shape[1])\n",
    "\n",
    "# Create tick labels \n",
    "labels = np.array(feature_names)[sorted_index]\n",
    "\n",
    "plt.bar(x, feature_importances[::-1], tick_label=labels)\n",
    "\n",
    "# Set the tick lables to be the feature names, according to the sorted feature_idx\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODULE 3\n",
    "\n",
    "## Neural networks and KNN"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We will learn how to normalize and scale data for use in KNN and neural network methods. Then we will learn how to use KNN and neural network regression to predict the future values of a stock's price (or any other regression problem)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Standardizing data\n",
    "\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "# Remove unimportant features (weekdays)\n",
    "train_features = train_features.iloc[:, :-4]\n",
    "test_features = test_features.iloc[:, :-4]\n",
    "\n",
    "# Standardize the train and test features\n",
    "scaled_train_features = scale(train_features)\n",
    "scaled_test_features = scale(test_features)\n",
    "\n",
    "# Plot histograms of the 14-day SMA RSI before and after scaling\n",
    "f, ax = plt.subplots(nrows=2, ncols=1)\n",
    "\n",
    "train_features.iloc[:, 2].hist(ax=ax[0])\n",
    "ax[1].hist(scaled_train_features[:, 2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Optimize n_neighbors\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "for n in range(2,13):\n",
    "    # Create and fit the KNN model\n",
    "    knn = KNeighborsRegressor(n_neighbors=n)\n",
    "    \n",
    "    # Fit the model to the training data\n",
    "    knn.fit(scaled_train_features, train_targets)\n",
    "    \n",
    "    # Print number of neighbors and the score to find the best value of n\n",
    "    print(\"n_neighbors =\", n)\n",
    "    print('train, test scores')\n",
    "    print(knn.score(scaled_train_features, train_targets))\n",
    "    print(knn.score(scaled_test_features, test_targets))\n",
    "    print()  # prints a blank line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluate KNN performance\n",
    "\n",
    "# Create the model with the best-performing n_neighbors of 5\n",
    "knn = KNeighborsRegressor(n_neighbors=5)\n",
    "\n",
    "# Fit the model\n",
    "knn.fit(scaled_train_features, train_targets)\n",
    "\n",
    "# Get predictions for train and test sets\n",
    "train_predictions = knn.predict(scaled_train_features)\n",
    "test_predictions = knn.predict(scaled_test_features)\n",
    "\n",
    "# Plot the actual vs predicted values\n",
    "plt.scatter(train_predictions, train_targets, label='train')\n",
    "plt.scatter(test_predictions, test_targets, label = 'test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build and fit a simple neural net\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Create the model\n",
    "model_1 = Sequential()\n",
    "model_1.add(Dense(100, input_dim=scaled_train_features.shape[1], activation='relu'))\n",
    "model_1.add(Dense(20, activation='relu'))\n",
    "model_1.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Fit the model\n",
    "model_1.compile(optimizer='adam', loss='mse')\n",
    "history = model_1.fit(scaled_train_features, train_targets, epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot losses\n",
    "\n",
    "# Plot the losses from the fit\n",
    "plt.plot(history.history['loss'])\n",
    "\n",
    "# Use the last loss as the title\n",
    "plt.title('loss:' + str(round(history.history['loss'][-1], 6)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Measure performance\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Calculate R^2 score\n",
    "train_preds = model_1.predict(scaled_train_features)\n",
    "test_preds = model_1.predict(scaled_test_features)\n",
    "print(r2_score(train_targets, train_preds))\n",
    "print(r2_score(test_targets, test_preds))\n",
    "\n",
    "# Plot predictions vs actual\n",
    "plt.scatter(train_preds, train_targets, label='train')\n",
    "plt.scatter(test_preds, test_targets, label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Custom loss function\n",
    "\n",
    "import keras.losses\n",
    "import tensorflow as tf\n",
    "\n",
    "# Create loss function\n",
    "def sign_penalty(y_true, y_predicted):\n",
    "    penalty = 100.\n",
    "    loss = tf.where(tf.less(y_true * y_pred, 0), \\\n",
    "                     penalty * tf.square(y_true - y_pred), \\\n",
    "                     tf.square(y_true - y_pred))\n",
    "\n",
    "    return tf.reduce_mean(loss, axis=-1)\n",
    "\n",
    "keras.losses.sign_penalty = sign_penalty  # enable use of loss with keras\n",
    "print(keras.losses.sign_penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fit neural net with custom loss function\n",
    "\n",
    "# Create the model\n",
    "model_2 = Sequential()\n",
    "model_2.add(Dense(100, input_dim = scaled_train_features.shape[1], activation='relu'))\n",
    "model_2.add(Dense(20, activation = 'relu'))\n",
    "model_2.add(Dense(1, activation = 'linear'))\n",
    "\n",
    "# Fit the model with our custom 'sign_penalty' loss function\n",
    "model_2.compile(optimizer='adam', loss=sign_penalty)\n",
    "history = model_2.fit(scaled_train_features, train_targets, epochs=25)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('loss:' + str(round(history.history['loss'][-1], 6)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize the results\n",
    "\n",
    "# Evaluate R^2 scores\n",
    "train_preds = model_2.predict(scaled_train_features)\n",
    "test_preds = model_2.predict(scaled_test_features)\n",
    "print(r2_score(train_targets, train_preds))\n",
    "print(r2_score(test_targets, test_preds))\n",
    "\n",
    "# Scatter the predictions vs actual -- this one is interesting!\n",
    "plt.scatter(train_preds, train_targets, label='train')\n",
    "plt.scatter(test_preds, test_targets, label = 'test')  # plot test set\n",
    "plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Combatting overfitting with dropout\n",
    "\n",
    "from keras.layers import Dropout\n",
    "\n",
    "# Create model with dropout\n",
    "model_3 = Sequential()\n",
    "model_3.add(Dense(100, input_dim=scaled_train_features.shape[1], activation='relu'))\n",
    "model_3.add(Dropout(.2))\n",
    "model_3.add(Dense(20, activation='relu'))\n",
    "model_3.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Fit model with mean squared error loss function\n",
    "model_3.compile(optimizer='adam', loss='mse')\n",
    "history = model_3.fit(scaled_train_features, train_targets, epochs=25)\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('loss:' + str(round(history.history['loss'][-1], 6)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ensembling models\n",
    "\n",
    "# Make predictions from the 3 neural net models\n",
    "train_pred1 = model_1.predict(scaled_train_features)\n",
    "test_pred1 = model_1.predict(scaled_test_features)\n",
    "\n",
    "train_pred2 = model_2.predict(scaled_train_features)\n",
    "test_pred2 = model_2.predict(scaled_test_features)\n",
    "\n",
    "train_pred3 = model_3.predict(scaled_train_features)\n",
    "test_pred3 = model_3.predict(scaled_test_features)\n",
    "\n",
    "# Horizontally stack predictions and take the average across rows\n",
    "train_preds = np.mean(np.hstack((train_pred1, train_pred2, train_pred3)), axis=1)\n",
    "test_preds = np.mean(np.hstack((test_pred1, test_pred2, test_pred3)), axis=1)\n",
    "\n",
    "print(test_preds[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## See how the ensemble performed\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Evaluate the R^2 scores\n",
    "print(r2_score(train_targets, train_preds))\n",
    "print(r2_score(test_targets, test_preds))\n",
    "\n",
    "# Scatter the predictions vs actual -- this one is interesting!\n",
    "plt.scatter(train_preds, train_targets, label='train')\n",
    "plt.scatter(test_preds, test_targets, label='test')\n",
    "plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODULE 4\n",
    "\n",
    "## Machine learning with modern portfolio theory"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In this chapter, you'll learn how to use modern portfolio theory (MPT) and the Sharpe ratio to plot and find optimal stock portfolios. You'll also use machine learning to predict the best portfolios. Finally, you'll evaluate performance of the ML-predicted portfolios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Join stock DataFrames and calculate returns\n",
    "\n",
    "# Join 3 stock dataframes together\n",
    "full_df = pd.concat([lng_df, spy_df, smlv_df], axis=1).dropna()\n",
    "\n",
    "# Resample the full dataframe to monthly timeframe\n",
    "monthly_df = full_df.resample('BMS').first()\n",
    "\n",
    "# Calculate daily returns of stocks\n",
    "returns_daily = full_df.pct_change()\n",
    "\n",
    "# Calculate monthly returns of the stocks\n",
    "returns_monthly = monthly_df.pct_change().dropna()\n",
    "print(returns_monthly.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate covariances for volatility\n",
    "\n",
    "# Daily covariance of stocks (for each monthly period)\n",
    "covariances = {}\n",
    "rtd_idx = returns_daily.index\n",
    "for i in returns_monthly.index:    \n",
    "    # Mask daily returns for each month and year, and calculate covariance\n",
    "    mask = (rtd_idx.month == i.month) & (rtd_idx.year == i.year)\n",
    "    \n",
    "    # Use the mask to get daily returns for the current month and year of monthy returns index\n",
    "    covariances[i] = returns_daily[mask].cov()\n",
    "\n",
    "print(covariances[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate portfolios\n",
    "\n",
    "portfolio_returns, portfolio_volatility, portfolio_weights = {}, {}, {}\n",
    "\n",
    "# Get portfolio performances at each month\n",
    "for date in sorted(covariances.keys()):\n",
    "    cov = covariances[date]\n",
    "    for portfolio in range(10):\n",
    "        weights = np.random.random(3)\n",
    "        weights /= np.sum(weights) # /= divides weights by their sum to normalize\n",
    "        returns = np.dot(weights, returns_monthly.loc[date])\n",
    "        volatility = np.sqrt(np.dot(weights.T, np.dot(cov, weights)))\n",
    "        portfolio_returns.setdefault(date, []).append(returns)\n",
    "        portfolio_volatility.setdefault(date, []).append(volatility)\n",
    "        portfolio_weights.setdefault(date, []).append(weights)\n",
    "        \n",
    "print(portfolio_weights[date][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot efficient frontier\n",
    "\n",
    "# Get latest date of available data\n",
    "date = sorted(covariances.keys())[-1]  \n",
    "\n",
    "# Plot efficient frontier\n",
    "# warning: this can take at least 10s for the plot to execute...\n",
    "plt.scatter(x=portfolio_volatility[date], y=portfolio_returns[date],  alpha=.1)\n",
    "plt.xlabel('Volatility')\n",
    "plt.ylabel('Returns')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get best Sharpe ratios\n",
    "\n",
    "# Empty dictionaries for sharpe ratios and best sharpe indexes by date\n",
    "sharpe_ratio, max_sharpe_idxs = {}, {}\n",
    "\n",
    "# Loop through dates and get sharpe ratio for each portfolio\n",
    "for date in portfolio_returns.keys():\n",
    "    for i, ret in enumerate(portfolio_returns[date]):\n",
    "    \n",
    "        # Divide returns by the volatility for the date and index, i\n",
    "        sharpe_ratio.setdefault(date, []).append(ret / portfolio_volatility[date][i])\n",
    "\n",
    "    # Get the index of the best sharpe ratio for each date\n",
    "    max_sharpe_idxs[date] = np.argmax(sharpe_ratio[date])\n",
    "\n",
    "print(portfolio_returns[date][max_sharpe_idxs[date]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate EWMAs\n",
    "\n",
    "# Calculate exponentially-weighted moving average of daily returns\n",
    "ewma_daily = returns_daily.ewm(span=30).mean()\n",
    "\n",
    "# Resample daily returns to first business day of the month with average for that month\n",
    "ewma_monthly = ewma_daily.resample('BMS').first()\n",
    "\n",
    "# Shift ewma for the month by 1 month forward so we can use it as a feature for future predictions \n",
    "ewma_monthly = ewma_monthly.shift(1).dropna()\n",
    "\n",
    "print(ewma_monthly.iloc[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make features and targets\n",
    "\n",
    "targets, features = [], []\n",
    "\n",
    "# Create features from price history and targets as ideal portfolio\n",
    "for date, ewma in ewma_monthly.iterrows():\n",
    "\n",
    "    # Get the index of the best sharpe ratio\n",
    "    best_idx = max_sharpe_idxs[date]\n",
    "    targets.append(portfolio_weights[date][best_idx])\n",
    "    features.append(ewma)  # add ewma to features\n",
    "\n",
    "targets = np.array(targets)\n",
    "features = np.array(features)\n",
    "print(targets[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot efficient frontier with best Sharpe ratio\n",
    "\n",
    "# Get most recent (current) returns and volatility\n",
    "date = sorted(covariances.keys())[-1]\n",
    "cur_returns = portfolio_returns[date]\n",
    "cur_volatility = portfolio_volatility[date]\n",
    "\n",
    "# Plot efficient frontier with sharpe as point\n",
    "plt.scatter(x=cur_volatility, y=cur_returns, alpha=0.1, color='blue')\n",
    "best_idx = max_sharpe_idxs[date]\n",
    "\n",
    "# Place an orange \"X\" on the point with the best Sharpe ratio\n",
    "plt.scatter(x=cur_volatility[best_idx], y=cur_returns[best_idx], marker='x', color='orange')\n",
    "plt.xlabel('Volatility')\n",
    "plt.ylabel('Returns')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make predictions with a random forest\n",
    "\n",
    "# Make train and test features\n",
    "train_size = int(0.85 * features.shape[0])\n",
    "train_features = features[:train_size]\n",
    "test_features = features[train_size:]\n",
    "train_targets = targets[:train_size]\n",
    "test_targets = targets[train_size:]\n",
    "\n",
    "# Fit the model and check scores on train and test\n",
    "rfr = RandomForestRegressor(n_estimators=300, random_state=42)\n",
    "rfr.fit(train_features, train_targets)\n",
    "print(rfr.score(train_features, train_targets))\n",
    "print(rfr.score(test_features, test_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get predictions and first evaluation\n",
    "\n",
    "# Get predictions from model on train and test\n",
    "train_predictions = rfr.predict(train_features)\n",
    "test_predictions = rfr.predict(test_features)\n",
    "\n",
    "# Calculate and plot returns from our RF predictions and the SPY returns\n",
    "test_returns = np.sum(returns_monthly.iloc[train_size:] * test_predictions, axis=1)\n",
    "plt.plot(test_returns, label='algo')\n",
    "plt.plot(returns_monthly['SPY'].iloc[train_size:], label='SPY')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluate returns\n",
    "\n",
    "# Plot the algo_cash and spy_cash to compare overall returns\n",
    "plt.plot(algo_cash, label='algo')\n",
    "plt.plot(spy_cash, label='SPY')\n",
    "plt.legend()  # show the legend\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
