{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Standard Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>checking_status</th>\n",
       "      <th>duration</th>\n",
       "      <th>credit_history</th>\n",
       "      <th>purpose</th>\n",
       "      <th>credit_amount</th>\n",
       "      <th>savings_status</th>\n",
       "      <th>employment</th>\n",
       "      <th>installment_commitment</th>\n",
       "      <th>personal_status</th>\n",
       "      <th>other_parties</th>\n",
       "      <th>...</th>\n",
       "      <th>property_magnitude</th>\n",
       "      <th>age</th>\n",
       "      <th>other_payment_plans</th>\n",
       "      <th>housing</th>\n",
       "      <th>existing_credits</th>\n",
       "      <th>job</th>\n",
       "      <th>num_dependents</th>\n",
       "      <th>own_telephone</th>\n",
       "      <th>foreign_worker</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'&lt;0'</td>\n",
       "      <td>6</td>\n",
       "      <td>'critical/other existing credit'</td>\n",
       "      <td>buy_radio_tv</td>\n",
       "      <td>1169</td>\n",
       "      <td>'no known savings'</td>\n",
       "      <td>'&gt;=7'</td>\n",
       "      <td>4</td>\n",
       "      <td>'male single'</td>\n",
       "      <td>none</td>\n",
       "      <td>...</td>\n",
       "      <td>'real estate'</td>\n",
       "      <td>67</td>\n",
       "      <td>none</td>\n",
       "      <td>own</td>\n",
       "      <td>2</td>\n",
       "      <td>skilled</td>\n",
       "      <td>1</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'0&lt;=X&lt;200'</td>\n",
       "      <td>48</td>\n",
       "      <td>'existing paid'</td>\n",
       "      <td>buy_radio_tv</td>\n",
       "      <td>5951</td>\n",
       "      <td>'&lt;100'</td>\n",
       "      <td>'1&lt;=X&lt;4'</td>\n",
       "      <td>2</td>\n",
       "      <td>'female div/dep/mar'</td>\n",
       "      <td>none</td>\n",
       "      <td>...</td>\n",
       "      <td>'real estate'</td>\n",
       "      <td>22</td>\n",
       "      <td>none</td>\n",
       "      <td>own</td>\n",
       "      <td>1</td>\n",
       "      <td>skilled</td>\n",
       "      <td>1</td>\n",
       "      <td>none</td>\n",
       "      <td>yes</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'no checking'</td>\n",
       "      <td>12</td>\n",
       "      <td>'critical/other existing credit'</td>\n",
       "      <td>education</td>\n",
       "      <td>2096</td>\n",
       "      <td>'&lt;100'</td>\n",
       "      <td>'4&lt;=X&lt;7'</td>\n",
       "      <td>2</td>\n",
       "      <td>'male single'</td>\n",
       "      <td>none</td>\n",
       "      <td>...</td>\n",
       "      <td>'real estate'</td>\n",
       "      <td>49</td>\n",
       "      <td>none</td>\n",
       "      <td>own</td>\n",
       "      <td>1</td>\n",
       "      <td>'unskilled resident'</td>\n",
       "      <td>2</td>\n",
       "      <td>none</td>\n",
       "      <td>yes</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  checking_status  duration                    credit_history       purpose  \\\n",
       "0            '<0'         6  'critical/other existing credit'  buy_radio_tv   \n",
       "1      '0<=X<200'        48                   'existing paid'  buy_radio_tv   \n",
       "2   'no checking'        12  'critical/other existing credit'     education   \n",
       "\n",
       "   credit_amount      savings_status employment  installment_commitment  \\\n",
       "0           1169  'no known savings'      '>=7'                       4   \n",
       "1           5951              '<100'   '1<=X<4'                       2   \n",
       "2           2096              '<100'   '4<=X<7'                       2   \n",
       "\n",
       "        personal_status other_parties  ...  property_magnitude age  \\\n",
       "0         'male single'          none  ...       'real estate'  67   \n",
       "1  'female div/dep/mar'          none  ...       'real estate'  22   \n",
       "2         'male single'          none  ...       'real estate'  49   \n",
       "\n",
       "   other_payment_plans housing existing_credits                   job  \\\n",
       "0                 none     own                2               skilled   \n",
       "1                 none     own                1               skilled   \n",
       "2                 none     own                1  'unskilled resident'   \n",
       "\n",
       "  num_dependents  own_telephone foreign_worker class  \n",
       "0              1            yes            yes  good  \n",
       "1              1           none            yes   bad  \n",
       "2              2           none            yes  good  \n",
       "\n",
       "[3 rows x 21 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit = pd.read_csv('credit.csv')\n",
    "\n",
    "# Inspect the first few lines of your data using head()\n",
    "credit.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_numeric_columns = ['checking_status', 'credit_history', 'purpose', 'savings_status', 'employment', 'personal_status', 'other_parties', 'property_magnitude', 'other_payment_plans', 'housing', 'job', 'own_telephone', 'foreign_worker']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking_status            int64\n",
      "duration                   int64\n",
      "credit_history             int64\n",
      "purpose                    int64\n",
      "credit_amount              int64\n",
      "savings_status             int64\n",
      "employment                 int64\n",
      "installment_commitment     int64\n",
      "personal_status            int64\n",
      "other_parties              int64\n",
      "residence_since            int64\n",
      "property_magnitude         int64\n",
      "age                        int64\n",
      "other_payment_plans        int64\n",
      "housing                    int64\n",
      "existing_credits           int64\n",
      "job                        int64\n",
      "num_dependents             int64\n",
      "own_telephone              int64\n",
      "foreign_worker             int64\n",
      "class                     object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Create a label encoder for each column. Encode the values\n",
    "for column in non_numeric_columns:\n",
    "    le = LabelEncoder()\n",
    "    credit[column] = le.fit_transform(credit[column])\n",
    "\n",
    "# Inspect the data types of the columns of the data frame\n",
    "print(credit.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your first pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = credit[['checking_status', 'duration', 'credit_history', 'purpose', 'credit_amount', 'savings_status', 'employment', 'installment_commitment', 'personal_status', 'other_parties', 'residence_since',\n",
    "       'property_magnitude', 'age', 'other_payment_plans', 'housing', 'existing_credits', 'job', 'num_dependents', 'own_telephone', 'foreign_worker']]\n",
    "y = credit[['class']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Split the data into train and test, with 20% as test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "  X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Create a random forest classifier, fixing the seed to 2\n",
    "rf_model = RandomForestClassifier(random_state=2, n_estimators = 10).fit(\n",
    "  X_train, y_train.values.ravel())\n",
    "\n",
    "# Use it to predict the labels of the test data\n",
    "rf_predictions = rf_model.predict(X_test)\n",
    "\n",
    "# Assess the accuracy of both classifiers\n",
    "accuracy_score(y_test, rf_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search CV for model complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 40}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# Set a range for n_estimators from 10 to 40 in steps of 10\n",
    "param_grid = {'n_estimators': range(10, 50, 10)}\n",
    "\n",
    "# Optimize for a RandomForestClassifier() using GridSearchCV\n",
    "grid = GridSearchCV(RandomForestClassifier(), param_grid, cv=3)\n",
    "grid.fit(X, y.values.ravel())\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 10}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "# Define a grid for n_estimators ranging from 1 to 10\n",
    "param_grid = {'n_estimators': range(1, 11)}\n",
    "\n",
    "# Optimize for a AdaBoostClassifier() using GridSearchCV\n",
    "grid = GridSearchCV(AdaBoostClassifier(), param_grid, cv=3)\n",
    "grid.fit(X, y.values.ravel())\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 50}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# Define a grid for n_neighbors with values 10, 50 and 100\n",
    "param_grid = {'n_neighbors': [10, 50, 100]}\n",
    "\n",
    "# Optimize for KNeighborsClassifier() using GridSearchCV\n",
    "grid = GridSearchCV(KNeighborsClassifier(), param_grid, cv=3)\n",
    "grid.fit(X, y.values.ravel())\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create numeric encoding for credit_history\n",
    "credit_history_num = LabelEncoder().fit_transform(\n",
    "  credit['credit_history'])\n",
    "\n",
    "# Create a new feature matrix including the numeric encoding\n",
    "X_num = pd.concat([X, pd.Series(credit_history_num)], axis = 1)\n",
    "\n",
    "# Create new feature matrix with dummies for credit_history\n",
    "X_hot = pd.concat(\n",
    "  [X, pd.get_dummies(credit['credit_history'])],axis = 1)\n",
    "\n",
    "# Compare the number of features of the resulting DataFrames\n",
    "X_hot.shape[1] > X_num.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True, False])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "# Function computing absolute difference from column mean\n",
    "def abs_diff(x):\n",
    "    return np.abs(x-np.mean(x))\n",
    "\n",
    "# Apply it to the credit amount and store to new column\n",
    "credit['diff'] = abs_diff(credit['credit_amount'])\n",
    "\n",
    "# Create a feature selector with chi2 that picks one feature\n",
    "sk = SelectKBest(chi2, k=1)\n",
    "\n",
    "# Use the selector to pick between credit_amount and diff\n",
    "sk.fit(credit[['credit_amount', 'diff']], credit['class'])\n",
    "\n",
    "# Inspect the results\n",
    "sk.get_support()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bringing it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best value for max_depth among values 2, 5 and 10\n",
    "grid_search = GridSearchCV(\n",
    "   RandomForestClassifier(random_state=1, n_estimators = 10), param_grid={'max_depth': [2, 5, 10]}, cv = 3)\n",
    "best_value = grid_search.fit(\n",
    "  X_train, y_train.values.ravel()).best_params_['max_depth']\n",
    "\n",
    "# Using the best value from above, fit a random forest\n",
    "clf = RandomForestClassifier(\n",
    "  random_state=1, max_depth=best_value, n_estimators = 10).fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# Apply SelectKBest with chi2 and pick top 100 features\n",
    "vt = SelectKBest(chi2, k=5).fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# Create a new dataset only containing the selected features\n",
    "X_train_reduced = vt.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Human in the Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is the source or the destination bad?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>duration</th>\n",
       "      <th>source_computer</th>\n",
       "      <th>source_port</th>\n",
       "      <th>destination_computer</th>\n",
       "      <th>destination_port</th>\n",
       "      <th>protocol</th>\n",
       "      <th>packet_count</th>\n",
       "      <th>byte_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>471692</td>\n",
       "      <td>0</td>\n",
       "      <td>C5808</td>\n",
       "      <td>N24128</td>\n",
       "      <td>C26871</td>\n",
       "      <td>N17023</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>471692</td>\n",
       "      <td>0</td>\n",
       "      <td>C5808</td>\n",
       "      <td>N2414</td>\n",
       "      <td>C26871</td>\n",
       "      <td>N19148</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>471692</td>\n",
       "      <td>0</td>\n",
       "      <td>C5808</td>\n",
       "      <td>N24156</td>\n",
       "      <td>C26871</td>\n",
       "      <td>N8001</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>471692</td>\n",
       "      <td>0</td>\n",
       "      <td>C5808</td>\n",
       "      <td>N24161</td>\n",
       "      <td>C26871</td>\n",
       "      <td>N18502</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>471692</td>\n",
       "      <td>0</td>\n",
       "      <td>C5808</td>\n",
       "      <td>N24162</td>\n",
       "      <td>C26871</td>\n",
       "      <td>N11309</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     time  duration source_computer source_port destination_computer  \\\n",
       "0  471692         0           C5808      N24128               C26871   \n",
       "1  471692         0           C5808       N2414               C26871   \n",
       "2  471692         0           C5808      N24156               C26871   \n",
       "3  471692         0           C5808      N24161               C26871   \n",
       "4  471692         0           C5808      N24162               C26871   \n",
       "\n",
       "  destination_port  protocol  packet_count  byte_count  \n",
       "0           N17023         6             1          60  \n",
       "1           N19148         6             1          60  \n",
       "2            N8001         6             1          60  \n",
       "3           N18502         6             1          60  \n",
       "4           N11309         6             1          60  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flows = pd.read_csv('lanl_flows.csv')\n",
    "flows.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize(df):\n",
    "    return {\n",
    "        'unique_ports': len(set(df['destination_port'])),\n",
    "        'average_packet': np.mean(df['packet_count']),\n",
    "        'average_duration': np.mean(df['duration'])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bads = {'C332', 'C3491', 'C7503', 'C9723', 'C1', 'C1632', 'C16088', 'C8209', 'C9006', 'C1567', 'C977', 'C1448', 'C115', 'C1980', 'C3422', 'C586', 'C2085', 'C1382', 'C2725', 'C17425', 'C477', 'C113', 'C20819', 'C1484', 'C15232', 'C177', 'C504', 'C8490', 'C4610', 'C5618', 'C2816', 'C923', 'C1906', 'C1006', 'C8751', 'C1482', 'C353', 'C19444', 'C1275', 'C3455', 'C17860', 'C17600', 'C1089', 'C1966', 'C3758', 'C370', 'C1737', 'C492', 'C16401', 'C3170', 'C15', 'C1952', 'C636', 'C92', 'C1944', 'C2597', 'C1961', 'C1173', 'C108', 'C4159', 'C685', 'C1581', 'C1042', 'C20966', 'C798', 'C1611', 'C1823', 'C553', 'C305', 'C21946', 'C528', 'C3288', 'C16563', 'C2578', 'C1570', 'C583', 'C779', 'C486', 'C3629', 'C19156', 'C3521', 'C18190', 'C633', 'C5693', 'C21814', 'C1555', 'C22275', 'C2254', 'C1810', 'C1438', 'C506', 'C849', 'C10817', 'C22409', 'C10005', 'C10405', 'C346', 'C22766', 'C2877', 'C721', 'C917', 'C3888', 'C359', 'C2378', 'C2196', 'C15197', 'C882', 'C12116', 'C2944', 'C1477', 'C4403', 'C400', 'C828', 'C18025', 'C46', 'C19932', 'C1710', 'C20455', 'C1936', 'C2341', 'C1191', 'C395', 'C366', 'C3601', 'C10', 'C3037', 'C6513', 'C13713', 'C1432', 'C3755', 'C706', 'C1085', 'C11194', 'C5439', 'C2609', 'C52', 'C3173', 'C7597', 'C3305', 'C1124', 'C2846', 'C17640', 'C965', 'C801', 'C20677', 'C385', 'C5343', 'C1509', 'C513', 'C368', 'C3435', 'C102', 'C18464', 'C458', 'C625', 'C90', 'C464', 'C3635', 'C22176', 'C3388', 'C21963', 'C20203', 'C5030', 'C17806', 'C742', 'C4280', 'C398', 'C7782', 'C1065', 'C3437', 'C3813', 'C4934', 'C1125', 'C4773', 'C3597', 'C18626', 'C1022', 'C2669', 'C881', 'C2844', 'C2079', 'C687', 'C231', 'C2058', 'C11039', 'C96', 'C19038', 'C61', 'C6487', 'C1302', 'C307', 'C294', 'C1183', 'C1776', 'C11727', 'C1046', 'C1732', 'C152', 'C1461', 'C3610', 'C467', 'C791', 'C1028', 'C1119', 'C8172', 'C4161', 'C1269', 'C452', 'C17776', 'C89', 'C19803', 'C529', 'C155', 'C886', 'C1479', 'C883', 'C19356', 'C1797', 'C5653', 'C1500', 'C22174', 'C728', 'C9692', 'C306', 'C423', 'C1506', 'C1319', 'C338', 'C3380', 'C8585', 'C2013', 'C4106', 'C1626', 'C2849', 'C1015', 'C12682', 'C12512', 'C2388', 'C78', 'C313', 'C3153', 'C12448', 'C1215', 'C16467', 'C17636', 'C2914', 'C10577', 'C5111', 'C12320', 'C626', 'C2648', 'C1784', 'C1224', 'C4554', 'C1096', 'C1503', 'C21664', 'C612', 'C3586', 'C2091', 'C148', 'C2604', 'C11178', 'C3303', 'C2519', 'C2057', 'C3199', 'C126', 'C143', 'C18113', 'C3249', 'C3019', 'C429', 'C1268', 'C21919', 'C7464', 'C1415', 'C853', 'C18872', 'C765', 'C1549', 'C21349', 'C3292', 'C17693', 'C4845', 'C3699', 'C9945', 'C1610', 'C457', 'C1964', 'C1493', 'C754', 'C42', 'C1616', 'C7131', 'C5453', 'C1003', 'C430', 'C1887', 'C2012', 'C243', 'C1014', 'C302', 'C246', 'C1222'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9361199939089387\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "#Â Group by source computer, and apply the feature extractor \n",
    "out = flows.groupby('source_computer').apply(featurize)\n",
    "\n",
    "# Convert the iterator to a dataframe by calling list on it\n",
    "X = pd.DataFrame(list(out), index=out.index)\n",
    "\n",
    "# Check which sourcesÂ in X.index are bad to create labels\n",
    "y = [x in bads for x in X.index]\n",
    "\n",
    "# Report the average accuracy of Adaboost over 3-fold CV\n",
    "print(np.mean(cross_val_score(AdaBoostClassifier(), X, y, cv = 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering on grouped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9377950357849856\n"
     ]
    }
   ],
   "source": [
    "# Create a feature counting unique protocols per source\n",
    "protocols = flows.groupby('source_computer').apply(\n",
    "  lambda df: len(set(df['protocol'])))\n",
    "\n",
    "# Convert this feature into a dataframe, naming the column\n",
    "protocols_DF = pd.DataFrame(\n",
    "  protocols, index=protocols.index, columns=['protocol'])\n",
    "\n",
    "# Now concatenate this feature with the previous dataset, X\n",
    "X_more = pd.concat([X, protocols_DF], axis=1)\n",
    "\n",
    "# Refit the classifier and report its accuracy\n",
    "print(np.mean(cross_val_score(\n",
    "  AdaBoostClassifier(), X_more, y, cv = 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turning a heuristic into a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9441340782122905\n"
     ]
    }
   ],
   "source": [
    "# Create a new dataset X_train_bad by subselecting bad hosts\n",
    "X_train_bad = X_train[y_train]\n",
    "\n",
    "# Calculate the average of unique_ports in bad examples\n",
    "avg_bad_ports = np.mean(X_train_bad['unique_ports'])\n",
    "\n",
    "# Label as positive sources that use more ports than that\n",
    "pred_port = X_test['unique_ports'] > avg_bad_ports\n",
    "\n",
    "# Print the accuracy of the heuristic\n",
    "print(accuracy_score(y_test, pred_port))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining heuristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9385474860335196\n"
     ]
    }
   ],
   "source": [
    "# Compute the mean of average_packet for bad sources\n",
    "avg_bad_packet = np.mean(X_train[y_train]['average_packet'])\n",
    "\n",
    "# Label as positive if average_packet is lower than that\n",
    "pred_packet = X_test['average_packet'] < avg_bad_packet\n",
    "\n",
    "# Find indices where pred_port and pred_packet both True\n",
    "pred_port = X_test['unique_ports'] > avg_bad_ports\n",
    "pred_both = pred_packet & pred_port\n",
    "\n",
    "# Ports only produced an accuracy of 0.919. Is this better?\n",
    "print(accuracy_score(y_test, pred_both))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with label noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_noisy = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08379888268156424\n",
      "0.0893854748603352\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "# Fit a Gaussian Naive Bayes classifier to the training data\n",
    "clf = GaussianNB().fit(X_train, y_train_noisy)\n",
    "\n",
    "# Report its accuracy on the test data\n",
    "print(accuracy_score(y_test, clf.predict(X_test)))\n",
    "\n",
    "# Assign half the weight to the first 100 noisy examples\n",
    "weights = [0.5]*100 + [1.0]*(len(y_train_noisy)-100)\n",
    "\n",
    "# Refit using weights and report accuracy. Has it improved?\n",
    "clf_weights = GaussianNB().fit(X_train, y_train_noisy, sample_weight=weights)\n",
    "print(accuracy_score(y_test, clf_weights.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reminder of performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09890109890109891\n",
      "0.05232558139534884\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, confusion_matrix\n",
    "print(f1_score(y_test, clf.predict(X_test)))\n",
    "print(precision_score(y_test, clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-world cost analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a random forest classifier to the training data\n",
    "clf = RandomForestClassifier(random_state=2, n_estimators = 10).fit(X_train, y_train)\n",
    "\n",
    "# Label the test data\n",
    "preds = clf.predict(X_test)\n",
    "\n",
    "# Get false positives/negatives from the confusion matrix\n",
    "tp, fp, fn, tn = confusion_matrix(y_test, preds).flatten()\n",
    "\n",
    "# Now compute the cost using the manager's advice\n",
    "cost = fp*10000 + fn*150000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score the test data using the given classifier\n",
    "scores = clf.predict_proba(X_test)\n",
    "\n",
    "# Get labels from the scores using the default threshold\n",
    "preds = [s[1] > 0.5 for s in scores]\n",
    "\n",
    "# Use the predict method to label the test data again\n",
    "preds_default = clf.predict(X_test)\n",
    "\n",
    "# Compare the two sets of predictions\n",
    "all(preds == preds_default)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing the threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huiren/anaconda3/envs/2019/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Create a range of equally spaced threshold values\n",
    "t_range = [0.0, 0.25, 0.5, 0.75, 1.0]\n",
    "\n",
    "# Store the predicted labels for each value of the threshold\n",
    "preds = [[s[1] > thr for s in scores] for thr in t_range]\n",
    "\n",
    "# Compute the accuracy for each threshold\n",
    "accuracies = [accuracy_score(y_test, p) for p in preds]\n",
    "\n",
    "# Compute the F1 score for each threshold\n",
    "f1_scores = [f1_score(y_test, p) for p in preds]\n",
    "\n",
    "# Report the optimal threshold for accuracy, and for F1\n",
    "print(t_range[np.argmax(accuracies)], t_range[np.argmax(f1_scores)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bringing it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>QRSduration</th>\n",
       "      <th>PRinterval</th>\n",
       "      <th>Q-Tinterval</th>\n",
       "      <th>Tinterval</th>\n",
       "      <th>Pinterval</th>\n",
       "      <th>QRS</th>\n",
       "      <th>...</th>\n",
       "      <th>chV6_QwaveAmp</th>\n",
       "      <th>chV6_RwaveAmp</th>\n",
       "      <th>chV6_SwaveAmp</th>\n",
       "      <th>chV6_RPwaveAmp</th>\n",
       "      <th>chV6_SPwaveAmp</th>\n",
       "      <th>chV6_PwaveAmp</th>\n",
       "      <th>chV6_TwaveAmp</th>\n",
       "      <th>chV6_QRSA</th>\n",
       "      <th>chV6_QRSTA</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>190</td>\n",
       "      <td>80</td>\n",
       "      <td>91</td>\n",
       "      <td>193</td>\n",
       "      <td>371</td>\n",
       "      <td>174</td>\n",
       "      <td>121</td>\n",
       "      <td>-16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2.9</td>\n",
       "      <td>23.3</td>\n",
       "      <td>49.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>165</td>\n",
       "      <td>64</td>\n",
       "      <td>81</td>\n",
       "      <td>174</td>\n",
       "      <td>401</td>\n",
       "      <td>149</td>\n",
       "      <td>39</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.1</td>\n",
       "      <td>20.4</td>\n",
       "      <td>38.8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>95</td>\n",
       "      <td>138</td>\n",
       "      <td>163</td>\n",
       "      <td>386</td>\n",
       "      <td>185</td>\n",
       "      <td>102</td>\n",
       "      <td>96</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>-2.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3.4</td>\n",
       "      <td>12.3</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>175</td>\n",
       "      <td>94</td>\n",
       "      <td>100</td>\n",
       "      <td>202</td>\n",
       "      <td>380</td>\n",
       "      <td>179</td>\n",
       "      <td>143</td>\n",
       "      <td>28</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.2</td>\n",
       "      <td>-2.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2.6</td>\n",
       "      <td>34.6</td>\n",
       "      <td>61.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>190</td>\n",
       "      <td>80</td>\n",
       "      <td>88</td>\n",
       "      <td>181</td>\n",
       "      <td>360</td>\n",
       "      <td>177</td>\n",
       "      <td>103</td>\n",
       "      <td>-16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.1</td>\n",
       "      <td>-3.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>3.9</td>\n",
       "      <td>25.4</td>\n",
       "      <td>62.8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 280 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  height  weight  QRSduration  PRinterval  Q-Tinterval  Tinterval  \\\n",
       "0   75    0     190      80           91         193          371        174   \n",
       "1   56    1     165      64           81         174          401        149   \n",
       "2   54    0     172      95          138         163          386        185   \n",
       "3   55    0     175      94          100         202          380        179   \n",
       "4   75    0     190      80           88         181          360        177   \n",
       "\n",
       "   Pinterval  QRS  ...  chV6_QwaveAmp  chV6_RwaveAmp  chV6_SwaveAmp  \\\n",
       "0        121  -16  ...            0.0            9.0           -0.9   \n",
       "1         39   25  ...            0.0            8.5            0.0   \n",
       "2        102   96  ...            0.0            9.5           -2.4   \n",
       "3        143   28  ...            0.0           12.2           -2.2   \n",
       "4        103  -16  ...            0.0           13.1           -3.6   \n",
       "\n",
       "   chV6_RPwaveAmp  chV6_SPwaveAmp  chV6_PwaveAmp  chV6_TwaveAmp  chV6_QRSA  \\\n",
       "0             0.0             0.0            0.9            2.9       23.3   \n",
       "1             0.0             0.0            0.2            2.1       20.4   \n",
       "2             0.0             0.0            0.3            3.4       12.3   \n",
       "3             0.0             0.0            0.4            2.6       34.6   \n",
       "4             0.0             0.0           -0.1            3.9       25.4   \n",
       "\n",
       "   chV6_QRSTA  class  \n",
       "0        49.4      0  \n",
       "1        38.8      0  \n",
       "2        49.0      0  \n",
       "3        61.6      1  \n",
       "4        62.8      0  \n",
       "\n",
       "[5 rows x 280 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arrh = pd.read_csv('arrh.csv')\n",
    "X = arrh[['age', 'sex', 'height', 'weight', 'QRSduration', 'PRinterval', 'Q-Tinterval', 'Tinterval', 'Pinterval', 'QRS', 'T', 'P', 'QRST', 'J', 'heartrate', 'chDI_Qwave', 'chDI_Rwave', 'chDI_Swave', 'chDI_RPwave', 'chDI_SPwave', 'chDI_intrinsicReflecttions', 'chDI_RRwaveExists', 'chDI_DD_RRwaveExists', 'chDI_RPwaveExists', 'chDI_DD_RPwaveExists', 'chDI_RTwaveExists', 'chDI_DD_RTwaveExists', 'chDII_Qwave', 'chDII_Rwave', 'chDII_Swave', 'chDII_RPwave', 'chDII_SPwave', 'chDII_intrinsicReflecttions', 'chDII_RRwaveExists', 'chDII_DD_RRwaveExists', 'chDII_RPwaveExists', 'chDII_DD_RPwaveExists', 'chDII_RTwaveExists', 'chDII_DD_RTwaveExists', 'chDIII_Qwave', 'chDIII_Rwave', 'chDIII_Swave', 'chDIII_RPwave', 'chDIII_SPwave', 'chDIII_intrinsicReflecttions', 'chDIII_RRwaveExists', 'chDIII_DD_RRwaveExists', 'chDIII_RPwaveExists', 'chDIII_DD_RPwaveExists', 'chDIII_RTwaveExists', 'chDIII_DD_RTwaveExists', 'chAVR_Qwave', 'chAVR_Rwave', 'chAVR_Swave', 'chAVR_RPwave', 'chAVR_SPwave', 'chAVR_intrinsicReflecttions', 'chAVR_RRwaveExists', 'chAVR_DD_RRwaveExists', 'chAVR_RPwaveExists', 'chAVR_DD_RPwaveExists', 'chAVR_RTwaveExists', 'chAVR_DD_RTwaveExists', 'chAVL_Qwave', 'chAVL_Rwave', 'chAVL_Swave', 'chAVL_RPwave', 'chAVL_SPwave', 'chAVL_intrinsicReflecttions', 'chAVL_RRwaveExists', 'chAVL_DD_RRwaveExists', 'chAVL_RPwaveExists', 'chAVL_DD_RPwaveExists', 'chAVL_RTwaveExists', 'chAVL_DD_RTwaveExists', 'chAVF_Qwave', 'chAVF_Rwave', 'chAVF_Swave', 'chAVF_RPwave', 'chAVF_SPwave', 'chAVF_intrinsicReflecttions', 'chAVF_RRwaveExists', 'chAVF_DD_RRwaveExists', 'chAVF_RPwaveExists', 'chAVF_DD_RPwaveExists', 'chAVF_RTwaveExists', 'chAVF_DD_RTwaveExists', 'chV1_Qwave', 'chV1_Rwave', 'chV1_Swave', 'chV1_RPwave', 'chV1_SPwave', 'chV1_intrinsicReflecttions', 'chV1_RRwaveExists', 'chV1_DD_RRwaveExists', 'chV1_RPwaveExists', 'chV1_DD_RPwaveExists', 'chV1_RTwaveExists', 'chV1_DD_RTwaveExists', 'chV2_Qwave', 'chV2_Rwave', 'chV2_Swave', 'chV2_RPwave', 'chV2_SPwave', 'chV2_intrinsicReflecttions', 'chV2_RRwaveExists', 'chV2_DD_RRwaveExists', 'chV2_RPwaveExists', 'chV2_DD_RPwaveExists', 'chV2_RTwaveExists', 'chV2_DD_RTwaveExists', 'chV3_Qwave', 'chV3_Rwave', 'chV3_Swave', 'chV3_RPwave', 'chV3_SPwave', 'chV3_intrinsicReflecttions', 'chV3_RRwaveExists', 'chV3_DD_RRwaveExists', 'chV3_RPwaveExists', 'chV3_DD_RPwaveExists', 'chV3_RTwaveExists', 'chV3_DD_RTwaveExists', 'chV4_Qwave', 'chV4_Rwave', 'chV4_Swave', 'chV4_RPwave', 'chV4_SPwave', 'chV4_intrinsicReflecttions', 'chV4_RRwaveExists', 'chV4_DD_RRwaveExists', 'chV4_RPwaveExists', 'chV4_DD_RPwaveExists', 'chV4_RTwaveExists', 'chV4_DD_RTwaveExists', 'chV5_Qwave', 'chV5_Rwave', 'chV5_Swave', 'chV5_RPwave', 'chV5_SPwave', 'chV5_intrinsicReflecttions', 'chV5_RRwaveExists', 'chV5_DD_RRwaveExists', 'chV5_RPwaveExists', 'chV5_DD_RPwaveExists', 'chV5_RTwaveExists', 'chV5_DD_RTwaveExists', 'chV6_Qwave', 'chV6_Rwave', 'chV6_Swave', 'chV6_RPwave', 'chV6_SPwave', 'chV6_intrinsicReflecttions', 'chV6_RRwaveExists', 'chV6_DD_RRwaveExists', 'chV6_RPwaveExists', 'chV6_DD_RPwaveExists', 'chV6_RTwaveExists', 'chV6_DD_RTwaveExists', 'chDI_JJwaveAmp', 'chDI_QwaveAmp', 'chDI_RwaveAmp', 'chDI_SwaveAmp', 'chDI_RPwaveAmp', 'chDI_SPwaveAmp', 'chDI_PwaveAmp', 'chDI_TwaveAmp', 'chDI_QRSA', 'chDI_QRSTA', 'chDII_JJwaveAmp', 'chDII_QwaveAmp', 'chDII_RwaveAmp', 'chDII_SwaveAmp', 'chDII_RPwaveAmp', 'chDII_SPwaveAmp', 'chDII_PwaveAmp', 'chDII_TwaveAmp', 'chDII_QRSA', 'chDII_QRSTA', 'chDIII_JJwaveAmp', 'chDIII_QwaveAmp', 'chDIII_RwaveAmp', 'chDIII_SwaveAmp', 'chDIII_RPwaveAmp', 'chDIII_SPwaveAmp', 'chDIII_PwaveAmp', 'chDIII_TwaveAmp', 'chDIII_QRSA', 'chDIII_QRSTA', 'chAVR_JJwaveAmp', 'chAVR_QwaveAmp', 'chAVR_RwaveAmp', 'chAVR_SwaveAmp', 'chAVR_RPwaveAmp', 'chAVR_SPwaveAmp', 'chAVR_PwaveAmp', 'chAVR_TwaveAmp', 'chAVR_QRSA', 'chAVR_QRSTA', 'chAVL_JJwaveAmp', 'chAVL_QwaveAmp', 'chAVL_RwaveAmp', 'chAVL_SwaveAmp', 'chAVL_RPwaveAmp', 'chAVL_SPwaveAmp', 'chAVL_PwaveAmp', 'chAVL_TwaveAmp', 'chAVL_QRSA', 'chAVL_QRSTA', 'chAVF_JJwaveAmp', 'chAVF_QwaveAmp', 'chAVF_RwaveAmp', 'chAVF_SwaveAmp', 'chAVF_RPwaveAmp', 'chAVF_SPwaveAmp', 'chAVF_PwaveAmp', 'chAVF_TwaveAmp', 'chAVF_QRSA', 'chAVF_QRSTA', 'chV1_JJwaveAmp', 'chV1_QwaveAmp', 'chV1_RwaveAmp', 'chV1_SwaveAmp', 'chV1_RPwaveAmp', 'chV1_SPwaveAmp', 'chV1_PwaveAmp', 'chV1_TwaveAmp', 'chV1_QRSA', 'chV1_QRSTA', 'chV2_JJwaveAmp', 'chV2_QwaveAmp', 'chV2_RwaveAmp', 'chV2_SwaveAmp', 'chV2_RPwaveAmp', 'chV2_SPwaveAmp', 'chV2_PwaveAmp', 'chV2_TwaveAmp', 'chV2_QRSA', 'chV2_QRSTA', 'chV3_JJwaveAmp', 'chV3_QwaveAmp', 'chV3_RwaveAmp', 'chV3_SwaveAmp', 'chV3_RPwaveAmp', 'chV3_SPwaveAmp', 'chV3_PwaveAmp', 'chV3_TwaveAmp', 'chV3_QRSA', 'chV3_QRSTA', 'chV4_JJwaveAmp', 'chV4_QwaveAmp', 'chV4_RwaveAmp', 'chV4_SwaveAmp', 'chV4_RPwaveAmp', 'chV4_SPwaveAmp', 'chV4_PwaveAmp', 'chV4_TwaveAmp', 'chV4_QRSA', 'chV4_QRSTA', 'chV5_JJwaveAmp', 'chV5_QwaveAmp', 'chV5_RwaveAmp', 'chV5_SwaveAmp', 'chV5_RPwaveAmp', 'chV5_SPwaveAmp', 'chV5_PwaveAmp', 'chV5_TwaveAmp', 'chV5_QRSA', 'chV5_QRSTA', 'chV6_JJwaveAmp', 'chV6_QwaveAmp', 'chV6_RwaveAmp', 'chV6_SwaveAmp', 'chV6_RPwaveAmp', 'chV6_SPwaveAmp', 'chV6_PwaveAmp', 'chV6_TwaveAmp', 'chV6_QRSA', 'chV6_QRSTA']]\n",
    "y = arrh[['class']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "arrh.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179.0\n",
      "140.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# Create a scorer assigning more cost to false positives\n",
    "def my_scorer(y_test, y_est, cost_fp=10.0, cost_fn=1.0):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_est).ravel()\n",
    "    return cost_fp*fp + cost_fn*fn\n",
    "\n",
    "# Fit a DecisionTreeClassifier to the data and compute the loss\n",
    "clf = DecisionTreeClassifier(random_state=2).fit(X_train, y_train)\n",
    "print(my_scorer(y_test, clf.predict(X_test)))\n",
    "\n",
    "# Refit, downweighting subjects whose weight is above 80\n",
    "weights = [0.5 if w > 80 else 1.0 for w in X_train.weight]\n",
    "clf_weighted = DecisionTreeClassifier().fit(\n",
    "  X_train, y_train, sample_weight=weights)\n",
    "print(my_scorer(y_test, clf_weighted.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Lifecycle Management\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your first pipeline - again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__n_estimators': 5, 'feature_selection__k': 20}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import f_classif\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Create pipeline with feature selector and classifier\n",
    "pipe = Pipeline([\n",
    "    ('feature_selection', SelectKBest(f_classif)),\n",
    "    ('clf', RandomForestClassifier(random_state=2))])\n",
    "\n",
    "# Create a parameter grid\n",
    "params = {\n",
    "   'feature_selection__k':[10, 20],\n",
    "   'clf__n_estimators':[2, 5]}\n",
    "\n",
    "# Initialise the grid search object\n",
    "grid_search = GridSearchCV(pipe, param_grid=params, cv = 3)\n",
    "\n",
    "# Fit it to the data and print the best value combination\n",
    "print(grid_search.fit(X_train, y_train).best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom scorers in pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__n_estimators': 5, 'feature_selection__k': 20}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, make_scorer\n",
    "\n",
    "# Create a custom scorer\n",
    "scorer = make_scorer(roc_auc_score)\n",
    "\n",
    "# Initialize the CV object\n",
    "gs = GridSearchCV(pipe, param_grid=params, scoring=scorer, cv = 3)\n",
    "\n",
    "# Fit it to the data and print the winning combination\n",
    "print(gs.fit(X_train, y_train).best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__n_estimators': 5, 'feature_selection__k': 10}\n"
     ]
    }
   ],
   "source": [
    "# Create a custom scorer\n",
    "scorer = make_scorer(f1_score)\n",
    "\n",
    "# Initialise the CV object\n",
    "gs = GridSearchCV(pipe, param_grid=params, scoring=scorer)\n",
    "\n",
    "# Fit it to the data and print the winning combination\n",
    "print(gs.fit(X_train, y_train).best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_metric(y_test, y_est, cost_fp=10.0, cost_fn=1.0):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_est).ravel()\n",
    "    return cost_fp * fp + cost_fn * fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__n_estimators': 5, 'feature_selection__k': 10}\n"
     ]
    }
   ],
   "source": [
    "# Create a custom scorer\n",
    "scorer = make_scorer(my_metric)\n",
    "\n",
    "# Initialise the CV object\n",
    "gs = GridSearchCV(pipe, param_grid=params, scoring=scorer)\n",
    "\n",
    "# Fit it to the data and print the winning combination\n",
    "print(gs.fit(X_train, y_train).best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Fit a random forest to the training set\n",
    "clf = RandomForestClassifier(random_state=42).fit(\n",
    "  X_train, y_train)\n",
    "\n",
    "# Save it to a file, to be pushed to production\n",
    "with open('model.pkl', 'wb') as file:\n",
    "    pickle.dump(clf, file=file)\n",
    "\n",
    "# Now load the model from file in the production environment\n",
    "with open('model.pkl', 'rb') as file:\n",
    "    clf_from_file = pickle.load(file)\n",
    "\n",
    "# Predict the labels of the test dataset\n",
    "preds = clf_from_file.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom function transformers in pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "# Define a feature extractor to flag very large values\n",
    "def more_than_average(X, multiplier=1.0):\n",
    "    Z = X.copy()\n",
    "    Z[:,1] = Z[:,1] > multiplier*np.mean(Z[:,1])\n",
    "    return Z\n",
    "\n",
    "# Convert your function so that it can be used in a pipeline\n",
    "pipe = Pipeline([\n",
    "  ('ft', FunctionTransformer(more_than_average)),\n",
    "  ('clf', RandomForestClassifier(random_state=2))])\n",
    "\n",
    "# Optimize the parameter multiplier using GridSearchCV\n",
    "params = {'ft__multiplier':[1, 2, 3]}\n",
    "grid_search = GridSearchCV(pipe, param_grid=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge the champion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8266666666666665\n",
      "0.7213114754098361\n"
     ]
    }
   ],
   "source": [
    "# Load the current model from disk\n",
    "champion = pickle.load(open('model.pkl', 'rb'))\n",
    "\n",
    "# Fit a Gaussian Naive Bayes to the training data\n",
    "challenger = GaussianNB().fit(X_train, y_train)\n",
    "\n",
    "# Print the F1 test scores of both champion and challenger\n",
    "print(f1_score(y_test, champion.predict(X_test)))\n",
    "print(f1_score(y_test, challenger.predict(X_test)))\n",
    "\n",
    "# Write back to disk the best-performing model\n",
    "with open('model.pkl', 'wb') as file:\n",
    "    pickle.dump(champion, file=file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   -0.237394\n",
      "1   -0.208875\n",
      "2   -0.245263\n",
      "3   -0.216790\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('feature_selection', SelectKBest(f_classif)),\n",
    "    ('clf', RandomForestClassifier(random_state=2))])\n",
    "\n",
    "params = {'feature_selection__k': [10, 20], 'clf__n_estimators': [2, 5]}\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('feature_selection', SelectKBest(k=10)), \n",
    "     ('clf', RandomForestClassifier())])\n",
    "\n",
    "# Fit your pipeline using GridSearchCV with three folds\n",
    "grid_search = GridSearchCV(pipe, params, cv=3, return_train_score=True)\n",
    "\n",
    "# Fit the grid search\n",
    "gs = grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Store the results of CV into a pandas dataframe\n",
    "results = pd.DataFrame(gs.cv_results_)\n",
    "\n",
    "# Print the difference between mean test and training scores\n",
    "print(results['mean_test_score']-results['mean_train_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning the window size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrange = range(10, 100, 10)\n",
    "t_now = 400\n",
    "               \n",
    "# Loop over window sizes\n",
    "for w_size in wrange:\n",
    "\n",
    "    # Define sliding window\n",
    "    sliding = arrh.loc[(t_now - w_size + 1):t_now]\n",
    "\n",
    "    # Extract X and y from the sliding window\n",
    "    X, y = sliding.drop('class', 1), sliding['class']\n",
    "    \n",
    "    # Fit the classifier and store the F1 score\n",
    "    preds = GaussianNB().fit(X, y).predict(X_test)\n",
    "    accuracies.append(f1_score(y_test, preds))\n",
    "\n",
    "# Estimate the best performing window size\n",
    "optimal_window = wrange[np.argmax(accuracies)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bringing it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline \n",
    "pipe = Pipeline([\n",
    "  ('ft', SelectKBest()), ('clf', RandomForestClassifier(random_state=2))])\n",
    "\n",
    "# Create a parameter grid\n",
    "grid = {'ft__k':[5, 10], 'clf__max_depth':[10, 20]}\n",
    "\n",
    "# Execute grid search CV on a dataset containing under 50s\n",
    "grid_search = GridSearchCV(pipe, param_grid=grid)\n",
    "arrh = arrh.iloc[np.where(arrh['age'] < 50)]\n",
    "grid_search.fit(arrh.drop('class', 1), arrh['class'])\n",
    "\n",
    "# Push the fitted pipeline to production\n",
    "with open('pipe.pkl', 'wb') as file:\n",
    "    pickle.dump(grid_search, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1 -1]\n"
     ]
    }
   ],
   "source": [
    "# Import the LocalOutlierFactor module\n",
    "from sklearn.neighbors import LocalOutlierFactor as lof\n",
    "\n",
    "# Create the list [1.0, 1.0, ..., 1.0, 10.0] as explained\n",
    "x = [1.0]*30\n",
    "x.append(10)\n",
    "\n",
    "# Cast to a data frame\n",
    "X = pd.DataFrame(x)\n",
    "\n",
    "# Fit the local outlier factor and print the outlier scores\n",
    "print(lof().fit_predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoF contamination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0]\n",
      " [13  0 49]\n",
      " [ 1  0 73]]\n"
     ]
    }
   ],
   "source": [
    "# Fit the local outlier factor and output predictions\n",
    "preds = lof().fit_predict(X_test)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(confusion_matrix(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0]\n",
      " [24  0 38]\n",
      " [ 3  0 71]]\n"
     ]
    }
   ],
   "source": [
    "# Set the contamination parameter to 0.2\n",
    "preds = lof(contamination=0.2).fit_predict(X_test)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(confusion_matrix(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Contamination to match outlier frequency in ground_truth\n",
    "# preds = lof(contamination=np.mean(y_test==-1.0)).fit_predict(X_test)\n",
    "\n",
    "# # Print the confusion matrix\n",
    "# print(confusion_matrix(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple novelty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1]\n"
     ]
    }
   ],
   "source": [
    "# Create a list of thirty 1s and cast to a dataframe\n",
    "X = pd.DataFrame([1.0]*30)\n",
    "\n",
    "# Create an instance of a lof novelty detector\n",
    "detector = lof(novelty=True)\n",
    "\n",
    "# Fit the detector to the data\n",
    "detector.fit(X)\n",
    "\n",
    "# Use it to predict the label of an example with value 10.0\n",
    "print(detector.predict(pd.DataFrame([10.0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Three novelty detectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the novelty detector\n",
    "from sklearn.svm import OneClassSVM as onesvm\n",
    "\n",
    "# Fit it to the training data and score the test data\n",
    "svm_detector = onesvm().fit(X_train)\n",
    "scores = svm_detector.score_samples(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the novelty detector\n",
    "\n",
    "from sklearn.ensemble import IsolationForest as isof\n",
    "\n",
    "# Fit it to the training data and score the test data\n",
    "isof_detector = isof().fit(X_train)\n",
    "scores = isof_detector.score_samples(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the novelty detector\n",
    "from sklearn.neighbors import LocalOutlierFactor as lof\n",
    "\n",
    "# Fit it to the training data and score the test data\n",
    "lof_detector = lof(novelty=True).fit(X_train)\n",
    "scores = lof_detector.score_samples(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contamination revisited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[57  5]\n",
      " [58 16]]\n"
     ]
    }
   ],
   "source": [
    "# Fit a one-class SVM detector and score the test data\n",
    "nov_det = onesvm().fit(X_train)\n",
    "scores = nov_det.score_samples(X_test)\n",
    "\n",
    "# Find the observed proportion of outliers in the test data\n",
    "prop = np.mean(y_test==1.0)\n",
    "\n",
    "# Compute the appropriate threshold\n",
    "threshold = np.quantile(scores, prop)\n",
    "\n",
    "# Print the confusion matrix for the thresholded scores\n",
    "print(confusion_matrix(y_test, scores > threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>AGE</th>\n",
       "      <th>SEX</th>\n",
       "      <th>STEROID</th>\n",
       "      <th>ANTIVIRALS</th>\n",
       "      <th>FATIGUE</th>\n",
       "      <th>MALAISE</th>\n",
       "      <th>ANOREXIA</th>\n",
       "      <th>LIVER BIG</th>\n",
       "      <th>LIVER FIRM</th>\n",
       "      <th>SPLEEN PALPABLE</th>\n",
       "      <th>SPIDERS</th>\n",
       "      <th>ASCITES</th>\n",
       "      <th>VARICES</th>\n",
       "      <th>BILIRUBIN</th>\n",
       "      <th>ALK PHOSPHATE</th>\n",
       "      <th>SGOT</th>\n",
       "      <th>ALBUMIN</th>\n",
       "      <th>PROTIME</th>\n",
       "      <th>HISTOLOGY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>95.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>78.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>85.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>249.0</td>\n",
       "      <td>3.7</td>\n",
       "      <td>54.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>81.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>3.9</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>57.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>78.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class   AGE  SEX  STEROID  ANTIVIRALS  FATIGUE  MALAISE  ANOREXIA  \\\n",
       "0    2.0  34.0  1.0      2.0         2.0      2.0      2.0       2.0   \n",
       "1    2.0  39.0  1.0      1.0         1.0      2.0      2.0       2.0   \n",
       "2    2.0  32.0  1.0      2.0         1.0      1.0      2.0       2.0   \n",
       "3    2.0  41.0  1.0      2.0         1.0      1.0      2.0       2.0   \n",
       "4    2.0  30.0  1.0      2.0         2.0      1.0      2.0       2.0   \n",
       "\n",
       "   LIVER BIG  LIVER FIRM  SPLEEN PALPABLE  SPIDERS  ASCITES  VARICES  \\\n",
       "0        2.0         2.0              2.0      2.0      2.0      2.0   \n",
       "1        1.0         1.0              2.0      2.0      2.0      2.0   \n",
       "2        2.0         1.0              2.0      1.0      2.0      2.0   \n",
       "3        2.0         1.0              2.0      2.0      2.0      2.0   \n",
       "4        2.0         1.0              2.0      2.0      2.0      2.0   \n",
       "\n",
       "   BILIRUBIN  ALK PHOSPHATE   SGOT  ALBUMIN  PROTIME  HISTOLOGY  \n",
       "0        0.9           95.0   28.0      4.0     75.0        1.0  \n",
       "1        1.3           78.0   30.0      4.4     85.0        1.0  \n",
       "2        1.0           59.0  249.0      3.7     54.0        1.0  \n",
       "3        0.9           81.0   60.0      3.9     52.0        1.0  \n",
       "4        2.2           57.0  144.0      4.9     78.0        1.0  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hep = pd.read_csv('hep.csv')\n",
    "hep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = hep[['AGE', 'SEX', 'STEROID', 'ANTIVIRALS', 'FATIGUE', 'MALAISE', 'ANOREXIA', 'LIVER BIG', 'LIVER FIRM', 'SPLEEN PALPABLE', 'SPIDERS', 'ASCITES', 'VARICES', 'BILIRUBIN', 'ALK PHOSPHATE', 'SGOT',\n",
    "       'ALBUMIN', 'PROTIME', 'HISTOLOGY']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import DistanceMetric as dm\n",
    "from sklearn.neighbors import DistanceMetric as dm\n",
    "\n",
    "# Find the Euclidean distance between all pairs\n",
    "dist_eucl = dm.get_metric('euclidean').pairwise(features)\n",
    "\n",
    "# Find the Hamming distance between all pairs\n",
    "dist_hamm = dm.get_metric('hamming').pairwise(features)\n",
    "\n",
    "# Find the Chebyshev distance between all pairs\n",
    "dist_cheb = dm.get_metric('chebyshev').pairwise(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not all metrics agree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# Compute outliers according to the euclidean metric\n",
    "out_eucl = lof(metric='euclidean').fit_predict(features)\n",
    "\n",
    "# Compute outliers according to the hamming metric\n",
    "out_hamm = lof(metric='hamming').fit_predict(features)\n",
    "\n",
    "# Compute outliers according to the jaccard metric\n",
    "out_jacc  = lof(metric='jaccard').fit_predict(features)\n",
    "\n",
    "# Find if the metrics agree on any one datapoint\n",
    "print(any(out_eucl+out_hamm+out_jacc ==-3 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restricted Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>seq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IMMUNE SYSTEM</td>\n",
       "      <td>LTKCQEEVSHIPAVHPGSFRPKCDENGNYLPLQCYGSIGYCWCVFP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IMMUNE SYSTEM</td>\n",
       "      <td>GPSVFLFPPKPKDTLMISRTPEVTCVVVDVSHEDPEVKFNWYVDGV...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IMMUNE SYSTEM</td>\n",
       "      <td>FNMQCQRRFYEALHDPNLNEEQRNAKIKSIRDDC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IMMUNE SYSTEM</td>\n",
       "      <td>EVQLVESGGGLVQPGGSLRLSCAASGFTFTDYTMDWVRQAPGKGLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IMMUNE SYSTEM</td>\n",
       "      <td>DIQMTQSPSSLSASVGDRVTITCKASQDVSIGVAWYQQKPGKAPKL...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           label                                                seq\n",
       "0  IMMUNE SYSTEM  LTKCQEEVSHIPAVHPGSFRPKCDENGNYLPLQCYGSIGYCWCVFP...\n",
       "1  IMMUNE SYSTEM  GPSVFLFPPKPKDTLMISRTPEVTCVVVDVSHEDPEVKFNWYVDGV...\n",
       "2  IMMUNE SYSTEM                 FNMQCQRRFYEALHDPNLNEEQRNAKIKSIRDDC\n",
       "3  IMMUNE SYSTEM  EVQLVESGGGLVQPGGSLRLSCAASGFTFTDYTMDWVRQAPGKGLE...\n",
       "4  IMMUNE SYSTEM  DIQMTQSPSSLSASVGDRVTITCKASQDVSIGVAWYQQKPGKAPKL..."
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proteins = pd.read_csv('proteins_exercises.csv')\n",
    "proteins.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.82\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import squareform\n",
    "from scipy.spatial.distance import pdist\n",
    "import stringdist\n",
    "# Wrap the RD-Levenshtein metric in a custom function\n",
    "def my_rdlevenshtein(u, v):\n",
    "    return stringdist.rdlevenshtein(u[0], v[0])\n",
    "\n",
    "# Reshape the array into a numpy matrix\n",
    "sequences = np.array(proteins['seq']).reshape(-1, 1)\n",
    "\n",
    "# Compute the pairwise distance matrix in square form\n",
    "M = squareform(pdist(sequences, my_rdlevenshtein))\n",
    "\n",
    "# Run a LoF algorithm on the precomputed distance matrix\n",
    "preds = lof(metric='precomputed').fit_predict(M)\n",
    "\n",
    "# Compute the accuracy of the outlier predictions\n",
    "print(accuracy_score(proteins['label'] == 'VIRUS', preds == -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bringing it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7265625\n",
      "0.5884375\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "# Create a feature that contains the length of the string\n",
    "proteins['len'] = proteins['seq'].apply(lambda x: len(x))\n",
    "\n",
    "# Create a feature encoding the first letter of the string\n",
    "proteins['first'] =  LabelEncoder().fit_transform(\n",
    "  proteins['seq'].apply(lambda x: list(x)[0]))\n",
    "\n",
    "lof_detector = lof(novelty=True).fit(proteins[['len', 'first']])\n",
    "\n",
    "# Extract scores from the fitted LoF object, compute its AUC\n",
    "scores_lof = lof_detector.negative_outlier_factor_\n",
    "print(roc_auc_score(proteins['label']=='IMMUNE SYSTEM', scores_lof))\n",
    "\n",
    "# Fit a 1-class SVM, extract its scores, and compute its AUC\n",
    "svm = onesvm().fit(proteins[['len', 'first']])\n",
    "scores_svm = svm.score_samples(proteins[['len', 'first']])\n",
    "print(roc_auc_score(proteins['label']=='IMMUNE SYSTEM', scores_svm))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
