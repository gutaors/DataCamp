{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1 - Unit testing basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your first unit test using pytest\n",
    "\n",
    "# Import the pytest package\n",
    "import pytest\n",
    "\n",
    "# Import the function convert_to_int()\n",
    "from preprocessing_helpers import convert_to_int\n",
    "\n",
    "# Complete the unit test name by adding a prefix\n",
    "def test_on_string_with_one_comma():\n",
    "  # Complete the assert statement\n",
    "  assert convert_to_int(\"2,081\") == 2081"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Spotting and fixing bugs\n",
    "\n",
    "!pytest test_convert_to_int.py\n",
    "___________________________________\n",
    "\n",
    "def convert_to_int(string_with_comma):\n",
    "    # Fix this line so that it returns an int, not a str\n",
    "    return int(string_with_comma.replace(\",\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Benefits of unit testing\n",
    "\n",
    "1 - Time savings, leading to faster development of new features.\n",
    "2 - Better user experience due to faster code execution.\n",
    "3 - Improved documentation, which will help new colleagues understand the code base better.\n",
    "4 - More user trust in the software product.\n",
    "6 - Better user experience due to reduced downtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2 - Intermediate unit testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write an informative test failure message\n",
    "\n",
    "import pytest\n",
    "from preprocessing_helpers import convert_to_int\n",
    "\n",
    "def test_on_string_with_one_comma():\n",
    "    test_argument = \"2,081\"\n",
    "    expected = 2081\n",
    "    actual = convert_to_int(test_argument)\n",
    "    # Format the string with the actual return value\n",
    "    message = \"convert_to_int('2,081') should return the int 2081, but it actually returned {0}\".format(actual)\n",
    "    # Write the assert statement which prints message on failure\n",
    "    assert (actual == expected), message\n",
    "\n",
    "_________________________________________\n",
    "\n",
    "!pytest test_convert_to_int.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing float return values\n",
    "\n",
    "import numpy as np\n",
    "import pytest\n",
    "from as_numpy import get_data_as_numpy_array\n",
    "\n",
    "def test_on_clean_file():\n",
    "  expected = np.array([[2081.0, 314942.0],\n",
    "                       [1059.0, 186606.0],\n",
    "  \t\t\t\t\t   [1148.0, 206186.0]\n",
    "                       ]\n",
    "                      )\n",
    "  actual = get_data_as_numpy_array(\"example_clean_data.txt\", num_columns=2)\n",
    "  message = \"Expected return value: {0}, Actual return value: {1}\".format(expected, actual)\n",
    "  # Complete the assert statement\n",
    "  assert actual == pytest.approx(expected), message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing with multiple assert statements\n",
    "\n",
    "def test_on_six_rows():\n",
    "    example_argument = np.array([[2081.0, 314942.0], [1059.0, 186606.0],\n",
    "                                [1148.0, 206186.0], [1506.0, 248419.0],\n",
    "                                [1210.0, 214114.0], [1697.0, 277794.0]]\n",
    "                               )\n",
    "    # Fill in with training array's expected number of rows\n",
    "    expected_training_array_num_rows = 4\n",
    "    # Fill in with testing array's expected number of rows\n",
    "    expected_testing_array_num_rows = example_argument.shape[0] - int(.75 * example_argument.shape[0])\n",
    "        actual = split_into_training_and_testing_sets(example_argument)\n",
    "    # Write the assert statement checking training array's number of rows\n",
    "    assert actual[0].shape[0] == expected_training_array_num_rows,\n",
    "            \"The actual number of rows in the training array is not {}\".format(expected_training_array_num_rows)\n",
    "    # Write the assert statement checking testing array's number of rows\n",
    "    assert actual[1].shape[0] == expected_testing_array_num_rows, \n",
    "            \"The actual number of rows in the testing array is not {}\".format(expected_testing_array_num_rows)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Practice the context manager\n",
    "\n",
    "import pytest\n",
    "\n",
    "# Fill in with a context manager that will silence the ValueError\n",
    "with pytest.raises(ValueError):\n",
    "    raise ValueError\n",
    "_____________________________\n",
    "\n",
    "try:\n",
    "    # Fill in with a context manager that raises Failed if no OSError is raised\n",
    "    with pytest.raises(OSError):\n",
    "        raise ValueError\n",
    "except:\n",
    "    print(\"pytest raised an exception because no OSError was raised in the context.\")\n",
    "_______________________________\n",
    "\n",
    "# Store the raised ValueError in the variable exc_info\n",
    "with pytest.raises(ValueError) as exc_info:\n",
    "    raise ValueError(\"Silence me!\")\n",
    "    \n",
    "_______________________________________\n",
    "\n",
    "with pytest.raises(ValueError) as exc_info:\n",
    "    raise ValueError(\"Silence me!\")\n",
    "# Check if the raised ValueError contains the correct message\n",
    "assert exc_info.match(\"Silence me!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing well: Boundary values\n",
    "\n",
    "import pytest\n",
    "from preprocessing_helpers import row_to_list\n",
    "\n",
    "def test_on_no_tab_no_missing_value():    # (0, 0) boundary value\n",
    "    # Assign actual to the return value for the argument \"123\\n\"\n",
    "    actual = row_to_list(\"123\\n\")\n",
    "    assert actual is None, \"Expected: None, Actual: {0}\".format(actual)\n",
    "    \n",
    "def test_on_two_tabs_no_missing_value():    # (2, 0) boundary value\n",
    "    actual = row_to_list(\"123\\t4,567\\t89\\n\")\n",
    "    # Complete the assert statement\n",
    "    assert actual is None, \"Expected: None, Actual: {0}\".format(actual)\n",
    "    \n",
    "def test_on_one_tab_with_missing_value():    # (1, 1) boundary value\n",
    "    actual = row_to_list(\"\\t4,567\\n\")\n",
    "    # Format the failure message\n",
    "    assert actual is None, \"Expected: None, Actual: {0}\".format(actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing well: Values triggering special logic\n",
    "\n",
    "import pytest\n",
    "from preprocessing_helpers import row_to_list\n",
    "\n",
    "def test_on_no_tab_with_missing_value():    # (0, 1) case\n",
    "    # Assign to the actual return value for the argument \"\\n\"\n",
    "    actual = \"\\n\"\n",
    "    # Write the assert statement with a failure message\n",
    "    assert row_to_list(actual) is None, \"Expected: None, Actual: {0}\".format(actual)\n",
    "\n",
    "def test_on_two_tabs_with_missing_value():    # (0, 1) case\n",
    "    # Assign to the actual return value for the argument \"123\\t\\t89\\n\"\n",
    "    actual = \"123\\t\\t89\\n\"\n",
    "    # Write the assert statement with a failure message\n",
    "    assert row_to_list(actual) is None, \"Expected: None, Actual: {0}\".format(actual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing well: Normal arguments\n",
    "\n",
    "import pytest\n",
    "from preprocessing_helpers import row_to_list\n",
    "\n",
    "def test_on_normal_argument_1():\n",
    "    actual = row_to_list(\"123\\t4,567\\n\")\n",
    "    # Fill in with the expected return value for the argument \"123\\t4,567\\n\"\n",
    "    expected = [\"123\", \"4,567\"]\n",
    "    assert actual == expected, \"Expected: {0}, Actual: {1}\".format(expected, actual)\n",
    "    \n",
    "def test_on_normal_argument_2():\n",
    "    actual = row_to_list(\"1,059\\t186,606\\n\")\n",
    "    expected = [\"1,059\", \"186,606\"]\n",
    "    # Write the assert statement along with a failure message\n",
    "    assert actual == expected, \"Expected: {0}, Actual: {1}\".format(actual, expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TDD: Tests for normal arguments\n",
    "\n",
    "def test_with_no_comma():\n",
    "    actual = convert_to_int(\"756\")\n",
    "    # Complete the assert statement\n",
    "    assert actual == 756, \"Expected: 756, Actual: {0}\".format(actual)\n",
    "    \n",
    "def test_with_one_comma():\n",
    "    actual = convert_to_int(\"2,081\")\n",
    "    # Complete the assert statement\n",
    "    assert actual == 2081, \"Expected: 2081, Actual: {0}\".format(actual)\n",
    "    \n",
    "def test_with_two_commas():\n",
    "    actual = convert_to_int(\"1,034,891\")\n",
    "    # Complete the assert statement\n",
    "    assert actual == 1034891, \"Expected: 1034891, Actual: {0}\".format(actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TDD: Requirement collection\n",
    "\n",
    "# Give a name to the test for an argument with missing comma\n",
    "def test_on_string_with_missing_comma():\n",
    "    actual = convert_to_int(\"178100,301\")\n",
    "    assert actual is None, \"Expected: None, Actual: {0}\".format(actual)\n",
    "\n",
    "def test_on_string_with_incorrectly_placed_comma():\n",
    "    # Assign to the actual return value for the argument \"12,72,891\"\n",
    "    actual = convert_to_int(\"12,72,891\")\n",
    "    assert actual is None, \"Expected: None, Actual: {0}\".format(actual)\n",
    "\n",
    "def test_on_float_valued_string():\n",
    "    actual = convert_to_int(\"23,816.92\")\n",
    "    # Complete the assert statement\n",
    "    assert actual is None, \"Expected: None, Actual: {0}\".format(actual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TDD: Implement the function\n",
    "\n",
    "def convert_to_int(integer_string_with_commas):\n",
    "    \n",
    "    comma_separated_parts = integer_string_with_commas.split(\",\")\n",
    "    \n",
    "    for i in range(len(comma_separated_parts)):\n",
    "        # Write an if statement for checking missing commas\n",
    "        if len(comma_separated_parts[i]) > 3:\n",
    "            return None\n",
    "        # Write the if statement for incorrectly placed commas\n",
    "        if i != 0 and len(comma_separated_parts[i]) != 3:\n",
    "            return None\n",
    "    integer_string_without_commas = \"\".join(comma_separated_parts)\n",
    "    \n",
    "    try:\n",
    "        return int(integer_string_without_commas)\n",
    "    # Fill in with the correct exception for float valued argument strings\n",
    "    except ValueError:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "3 - Test Organization and Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a test class\n",
    "\n",
    "import pytest\n",
    "import numpy as np\n",
    "\n",
    "from models.train import split_into_training_and_testing_sets\n",
    "\n",
    "# Declare the test class\n",
    "class TestSplitIntoTrainingAndTestingSets(object):\n",
    "    # Fill in with the correct mandatory argument\n",
    "    def test_on_one_row(self):\n",
    "        test_argument = np.array([[1382.0, 390167.0]])\n",
    "        with pytest.raises(ValueError) as exc_info:\n",
    "            split_into_training_and_testing_sets(test_argument)\n",
    "        expected_error_msg = \"Argument data_array must have at least 2 rows, it actually has just 1\"\n",
    "        assert exc_info.match(expected_error_msg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Running test classes\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def split_into_training_and_testing_sets(data_array):\n",
    "    \n",
    "    dim = data_array.ndim\n",
    "    \n",
    "    if dim != 2:\n",
    "        raise ValueError(\"Argument data_array must be two dimensional. Got {0} dimensional array instead!\".format(dim))\n",
    "    num_rows = data_array.shape[0]\n",
    "    \n",
    "    if num_rows < 2:\n",
    "        raise ValueError(\"Argument data_array must have at least 2 rows, it actually has just {0}\".format(num_rows))\n",
    "    \n",
    "    # Fill in with the correct float\n",
    "    num_training = int(num_rows * data_array.shape[0])\n",
    "    permuted_indices = np.random.permutation(data_array.shape[0])\n",
    "    return data_array[permuted_indices[:num_training], :], data_array[permuted_indices[num_training:], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Add a reason for the expected failure\n",
    "@pytest.mark.xfail(reason=\"Using TDD, model_test() has not yet been implemented\")\n",
    "class TestModelTest(object):\n",
    "    def test_on_linear_data(self):\n",
    "        test_input = np.array([[1.0, 3.0], [2.0, 5.0], [3.0, 7.0]])\n",
    "        expected = 1.0\n",
    "        actual = model_test(test_input, 2.0, 1.0)\n",
    "        message = \"model_test({0}) should return {1}, but it actually returned {2}\".format(test_input, expected, actual)\n",
    "        assert actual == pytest.approx(expected), message\n",
    "        \n",
    "    def test_on_one_dimensional_array(self):\n",
    "        test_input = np.array([1.0, 2.0, 3.0, 4.0])\n",
    "        with pytest.raises(ValueError) as exc_info:\n",
    "            model_test(test_input, 1.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Mark a test as conditionally skipped\n",
    "\n",
    "# Import the sys module\n",
    "import sys\n",
    "\n",
    "class TestGetDataAsNumpyArray(object):\n",
    "    # Add a reason for skipping the test\n",
    "    @pytest.mark.skipif(sys.version_info > (2, 7), _reason=\"Works only on Python 2.7 or lower\")\n",
    "    def test_on_clean_file(self):\n",
    "        expected = np.array([[2081.0, 314942.0],\n",
    "                             [1059.0, 186606.0],\n",
    "                             [1148.0, 206186.0]\n",
    "                             ]\n",
    "                            )\n",
    "        actual = get_data_as_numpy_array(\"example_clean_data.txt\", num_columns=2)\n",
    "        message = \"Expected return value: {0}, Actual return value: {1}\".format(expected, actual)\n",
    "        assert actual == pytest.approx(expected), message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reasoning in the test result report\n",
    "\n",
    "# What is the command that would only show the reason for expected failures in the test result report?\n",
    "!pytest -rx\n",
    "\n",
    "_____________________\n",
    "\n",
    "# What is the command that would only show the reason for skipped tests in the test result report?\n",
    "!pytest -rs\n",
    "____________________________\n",
    "\n",
    "# What is the command that would show the reason for both skipped tests and tests that are expected to fail in the test result report?\n",
    "!pytest -rsx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "4 - Testing Models, Plots and Much More"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use a fixture for a clean data file\n",
    "\n",
    "# Add a decorator to make this function a fixture\n",
    "@pytest.fixture\n",
    "def clean_data_file():\n",
    "    file_path = \"clean_data_file.txt\"\n",
    "    with open(file_path, \"w\") as f:\n",
    "        f.write(\"201\\t305671\\n7892\\t298140\\n501\\t738293\\n\")\n",
    "    yield file_path\n",
    "    os.remove(file_path)\n",
    "    \n",
    "# Pass the correct argument so that the test can use the fixture\n",
    "def test_on_clean_file(tmpdir):\n",
    "    expected = np.array([[201.0, 305671.0], [7892.0, 298140.0], [501.0, 738293.0]])\n",
    "    # Pass the clean data file path yielded by the fixture as the first argument\n",
    "    actual = get_data_as_numpy_array(tmpdir, 2)\n",
    "    assert actual == pytest.approx(expected), \"Expected: {0}, Actual: {1}\".format(expected, actual) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write a fixture for an empty data file\n",
    "\n",
    "@pytest.fixture\n",
    "def empty_file():\n",
    "    # Assign the file path \"empty.txt\" to the variable\n",
    "    file_path = \"empty.txt\"\n",
    "    open(file_path, \"w\").close()\n",
    "    # Yield the variable file_path\n",
    "    yield file_path\n",
    "    # Remove the file in the teardown\n",
    "    os.remove(file_path)\n",
    "    \n",
    "def test_on_empty_file(self, empty_file):\n",
    "    expected = np.empty((0, 2))\n",
    "    actual = get_data_as_numpy_array(empty_file, 2)\n",
    "    assert actual == pytest.approx(expected), \"Expected: {0}, Actual: {1}\".format(expected, actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fixture chaining using tmpdir\n",
    "\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture\n",
    "# Add the correct argument so that this fixture can chain with the tmpdir fixture\n",
    "def empty_file(tmpdir):\n",
    "    # Use the appropriate method to create an empty file in the temporary directory\n",
    "    file_path = tmpdir.join(\"empty.txt\")\n",
    "    open(file_path, \"w\").close()\n",
    "    yield file_path\n",
    "\n",
    "    \n",
    "## setup of tmpdir → setup of empty_file() → teardown of empty_file() → teardown of tmpdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Program a bug-free dependency\n",
    "\n",
    "# Define a function convert_to_int_bug_free\n",
    "def convert_to_int_bug_free(comma_separated_integer_string):\n",
    "    # Assign to the dict holding the correct return values \n",
    "    return_values = comma_separated_integer_string\n",
    "    # Return the correct result using the dict return_values\n",
    "    return return_values[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Mock a dependency\n",
    "\n",
    "# Add the correct argument to use the mocking fixture in this test\n",
    "def test_on_raw_data(self, raw_and_clean_data_file, mocker):\n",
    "    raw_path, clean_path = raw_and_clean_data_file\n",
    "    # Replace the dependency with the bug-free mock\n",
    "    convert_to_int_mock = mocker.patch(\"data.preprocessing_helpers.convert_to_int\",\n",
    "                                       side_effect=convert_to_int_bug_free)\n",
    "    preprocess(raw_path, clean_path)\n",
    "    # Check if preprocess() called the dependency correctly\n",
    "    assert convert_to_int_mock.call_args_list == [call(\"1,801\"), call(\"201,411\"), call(\"2,002\"), call(\"333,209\"), call(\"1990\"), call(\"782,911\"), call(\"1,285\"), call(\"389129\")]\n",
    "    with open(clean_path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    first_line = lines[0]\n",
    "    assert first_line == \"1801\\\\t201411\\\\n\"\n",
    "    second_line = lines[1]\n",
    "    assert second_line == \"2002\\\\t333209\\\\n\" \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing on linear data\n",
    "\n",
    "import numpy as np\n",
    "import pytest\n",
    "from models.train import model_test\n",
    "\n",
    "def test_on_perfect_fit():\n",
    "    # Assign to a NumPy array containing a linear testing set\n",
    "    test_argument = np.array([[1.0, 3.0], [2.0, 5.0], [3.0, 7.0]])\n",
    "    # Fill in with the expected value of r^2 in the case of perfect fit\n",
    "    expected = 1.0\n",
    "    # Fill in with the slope and intercept of the model\n",
    "    actual = model_test(test_argument, slope=2.0, intercept=1.0)\n",
    "    # Complete the assert statement\n",
    "    assert actual == pytest.approx(expected), \"Expected: {0}, Actual: {1}\".format(expected, actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing on circular data\n",
    "\n",
    "def test_on_circular_data(self):\n",
    "    theta = pi/4.0\n",
    "    # Assign to a NumPy array holding the circular testing data\n",
    "    test_argument = np.array([[1.0, 0.0], [cos(theta), sin(theta)],\n",
    "                              [0.0, 1.0],\n",
    "                              [cos(3 * theta), sin(3 * theta)],\n",
    "                              [-1.0, 0.0],\n",
    "                              [cos(5 * theta), sin(5 * theta)],\n",
    "                              [0.0, -1.0],\n",
    "                              [cos(7 * theta), sin(7 * theta)]]\n",
    "                             )\n",
    "    # Fill in with the slope and intercept of the straight line\n",
    "    actual = model_test(test_argument, slope=0.0, intercept=0.0)\n",
    "    # Complete the assert statement\n",
    "    assert actual == pytest.approx(0.0)\n",
    "\n",
    "__________________________________________________\n",
    "\n",
    "!pytest models/test_train.py::test_on_perfect_fit\n",
    "__________________________________________________\n",
    "        \n",
    "!pytest models/test_train.py::test_on_circular_data\n",
    "__________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate the baseline image\n",
    "\n",
    "# Add the pytest marker which generates baselines and compares images\n",
    "@pytest.mark.mpl_image_compare\n",
    "def test_plot_for_almost_linear_data():\n",
    "    slope = 5.0\n",
    "    intercept = -2.0\n",
    "    x_array = np.array([1.0, 2.0, 3.0])\n",
    "    y_array = np.array([3.0, 8.0, 11.0])\n",
    "    title = \"Test plot for almost linear data\"\n",
    "    # Return the matplotlib figure returned by the function under test\n",
    "    return get_plot_for_best_fit_line(slope, intercept, x_array, y_array, title)\n",
    "\n",
    "______________________________________________________________\n",
    "\n",
    "!pytest --mpl-generate-path visualization/baseline -k \"test_plot_for_almost_linear_data\"\n",
    "______________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fix the plotting function\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def get_plot_for_best_fit_line(slope, intercept, x_array, y_array, title):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(x_array, y_array, \".\")\n",
    "    ax.plot([0, np.max(x_array)], [intercept, slope * np.max(x_array) + intercept], \"-\")\n",
    "    # Fill in with axis labels so that they match the baseline\n",
    "    ax.set(xlabel='area (square feet)', ylabel=\"price (dollars)\", title=title)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu-2.0",
   "language": "python",
   "name": "tf_gpu2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
