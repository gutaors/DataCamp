{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification and Regression Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train your first classification tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "wbc = pd.read_csv('wbc.csv')\n",
    "X = wbc[['radius_mean', 'concave points_mean']]\n",
    "y = wbc['diagnosis'].apply(lambda x: 1 if x == 'M' else 0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Import DecisionTreeClassifier from sklearn.tree\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "\n",
    "SEED = 1 \n",
    "\n",
    "# Instantiate a DecisionTreeClassifier 'dt' with a maximum depth of 6\n",
    "dt = DecisionTreeClassifier(max_depth=6, random_state=SEED)\n",
    "\n",
    "# Fit dt to the training set\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = dt.predict(X_test)\n",
    "print(y_pred[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the classification tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.88\n"
     ]
    }
   ],
   "source": [
    "# Import accuracy_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "# Compute test set accuracy  \n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Test set accuracy: {:.2f}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression vs classification tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_regions(X, y, clf,\n",
    "                          feature_index=None,\n",
    "                          filler_feature_values=None,\n",
    "                          filler_feature_ranges=None,\n",
    "                          ax=None,\n",
    "                          X_highlight=None,\n",
    "                          res=0.02, legend=1,\n",
    "                          hide_spines=True,\n",
    "                          markers='s^oxv<>',\n",
    "                          colors='red,blue,limegreen,gray,cyan'):\n",
    "    \"\"\"Plot decision regions of a classifier.\n",
    "\n",
    "    Please note that this functions assumes that class labels are\n",
    "    labeled consecutively, e.g,. 0, 1, 2, 3, 4, and 5. If you have class\n",
    "    labels with integer labels > 4, you may want to provide additional colors\n",
    "    and/or markers as `colors` and `markers` arguments.\n",
    "    See http://matplotlib.org/examples/color/named_colors.html for more\n",
    "    information.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like, shape = [n_samples, n_features]\n",
    "        Feature Matrix.\n",
    "    y : array-like, shape = [n_samples]\n",
    "        True class labels.\n",
    "    clf : Classifier object.\n",
    "        Must have a .predict method.\n",
    "    feature_index : array-like (default: (0,) for 1D, (0, 1) otherwise)\n",
    "        Feature indices to use for plotting. The first index in\n",
    "        `feature_index` will be on the x-axis, the second index will be\n",
    "        on the y-axis.\n",
    "    filler_feature_values : dict (default: None)\n",
    "        Only needed for number features > 2. Dictionary of feature\n",
    "        index-value pairs for the features not being plotted.\n",
    "    filler_feature_ranges : dict (default: None)\n",
    "        Only needed for number features > 2. Dictionary of feature\n",
    "        index-value pairs for the features not being plotted. Will use the\n",
    "        ranges provided to select training samples for plotting.\n",
    "    ax : matplotlib.axes.Axes (default: None)\n",
    "        An existing matplotlib Axes. Creates\n",
    "        one if ax=None.\n",
    "    X_highlight : array-like, shape = [n_samples, n_features] (default: None)\n",
    "        An array with data points that are used to highlight samples in `X`.\n",
    "    res : float or array-like, shape = (2,) (default: 0.02)\n",
    "        Grid width. If float, same resolution is used for both the x- and\n",
    "        y-axis. If array-like, the first item is used on the x-axis, the\n",
    "        second is used on the y-axis. Lower values increase the resolution but\n",
    "        slow down the plotting.\n",
    "    hide_spines : bool (default: True)\n",
    "        Hide axis spines if True.\n",
    "    legend : int (default: 1)\n",
    "        Integer to specify the legend location.\n",
    "        No legend if legend is 0.\n",
    "    markers : str (default 's^oxv<>')\n",
    "        Scatterplot markers.\n",
    "    colors : str (default 'red,blue,limegreen,gray,cyan')\n",
    "        Comma separated list of colors.\n",
    "\n",
    "    Returns\n",
    "    ---------\n",
    "    ax : matplotlib.axes.Axes object\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    check_Xy(X, y, y_int=True)  # Validate X and y arrays\n",
    "    dim = X.shape[1]\n",
    "\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    if isinstance(res, float):\n",
    "        xres, yres = res, res\n",
    "    else:\n",
    "        try:\n",
    "            xres, yres = res\n",
    "        except ValueError:\n",
    "            raise ValueError('Unable to unpack res. Expecting '\n",
    "                             'array-like input of length 2.')\n",
    "\n",
    "    plot_testdata = True\n",
    "    if not isinstance(X_highlight, np.ndarray):\n",
    "        if X_highlight is not None:\n",
    "            raise ValueError('X_highlight must be a NumPy array or None')\n",
    "        else:\n",
    "            plot_testdata = False\n",
    "    elif len(X_highlight.shape) < 2:\n",
    "        raise ValueError('X_highlight must be a 2D array')\n",
    "\n",
    "    if feature_index is not None:\n",
    "        # Unpack and validate the feature_index values\n",
    "        if dim == 1:\n",
    "            raise ValueError(\n",
    "                'feature_index requires more than one training feature')\n",
    "        try:\n",
    "            x_index, y_index = feature_index\n",
    "        except ValueError:\n",
    "            raise ValueError(\n",
    "                'Unable to unpack feature_index. Make sure feature_index '\n",
    "                'only has two dimensions.')\n",
    "        try:\n",
    "            X[:, x_index], X[:, y_index]\n",
    "        except IndexError:\n",
    "            raise IndexError(\n",
    "                'feature_index values out of range. X.shape is {}, but '\n",
    "                'feature_index is {}'.format(X.shape, feature_index))\n",
    "    else:\n",
    "        feature_index = (0, 1)\n",
    "        x_index, y_index = feature_index\n",
    "\n",
    "    # Extra input validation for higher number of training features\n",
    "    if dim > 2:\n",
    "        if filler_feature_values is None:\n",
    "            raise ValueError('Filler values must be provided when '\n",
    "                             'X has more than 2 training features.')\n",
    "\n",
    "        if filler_feature_ranges is not None:\n",
    "            if not set(filler_feature_values) == set(filler_feature_ranges):\n",
    "                raise ValueError(\n",
    "                    'filler_feature_values and filler_feature_ranges must '\n",
    "                    'have the same keys')\n",
    "\n",
    "        # Check that all columns in X are accounted for\n",
    "        column_check = np.zeros(dim, dtype=bool)\n",
    "        for idx in filler_feature_values:\n",
    "            column_check[idx] = True\n",
    "        for idx in feature_index:\n",
    "            column_check[idx] = True\n",
    "        if not all(column_check):\n",
    "            missing_cols = np.argwhere(~column_check).flatten()\n",
    "            raise ValueError(\n",
    "                'Column(s) {} need to be accounted for in either '\n",
    "                'feature_index or filler_feature_values'.format(missing_cols))\n",
    "\n",
    "    marker_gen = cycle(list(markers))\n",
    "\n",
    "    n_classes = np.unique(y).shape[0]\n",
    "    colors = colors.split(',')\n",
    "    colors_gen = cycle(colors)\n",
    "    colors = [next(colors_gen) for c in range(n_classes)]\n",
    "\n",
    "    # Get minimum and maximum\n",
    "    x_min, x_max = X[:, x_index].min() - 1, X[:, x_index].max() + 1\n",
    "    if dim == 1:\n",
    "        y_min, y_max = -1, 1\n",
    "    else:\n",
    "        y_min, y_max = X[:, y_index].min() - 1, X[:, y_index].max() + 1\n",
    "\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, xres),\n",
    "                         np.arange(y_min, y_max, yres))\n",
    "\n",
    "    if dim == 1:\n",
    "        X_predict = np.array([xx.ravel()]).T\n",
    "    else:\n",
    "        X_grid = np.array([xx.ravel(), yy.ravel()]).T\n",
    "        X_predict = np.zeros((X_grid.shape[0], dim))\n",
    "        X_predict[:, x_index] = X_grid[:, 0]\n",
    "        X_predict[:, y_index] = X_grid[:, 1]\n",
    "        if dim > 2:\n",
    "            for feature_idx in filler_feature_values:\n",
    "                X_predict[:, feature_idx] = filler_feature_values[feature_idx]\n",
    "    Z = clf.predict(X_predict)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    # Plot decisoin region\n",
    "    ax.contourf(xx, yy, Z,\n",
    "                alpha=0.3,\n",
    "                colors=colors,\n",
    "                levels=np.arange(Z.max() + 2) - 0.5)\n",
    "\n",
    "    ax.axis(xmin=xx.min(), xmax=xx.max(), y_min=yy.min(), y_max=yy.max())\n",
    "\n",
    "    # Scatter training data samples\n",
    "    for idx, c in enumerate(np.unique(y)):\n",
    "        if dim == 1:\n",
    "            y_data = [0 for i in X[y == c]]\n",
    "            x_data = X[y == c]\n",
    "        elif dim == 2:\n",
    "            y_data = X[y == c, y_index]\n",
    "            x_data = X[y == c, x_index]\n",
    "        elif dim > 2 and filler_feature_ranges is not None:\n",
    "            class_mask = y == c\n",
    "            feature_range_mask = get_feature_range_mask(\n",
    "                            X, filler_feature_values=filler_feature_values,\n",
    "                            filler_feature_ranges=filler_feature_ranges)\n",
    "            y_data = X[class_mask & feature_range_mask, y_index]\n",
    "            x_data = X[class_mask & feature_range_mask, x_index]\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        ax.scatter(x=x_data,\n",
    "                   y=y_data,\n",
    "                   alpha=0.8,\n",
    "                   c=colors[idx],\n",
    "                   marker=next(marker_gen),\n",
    "                   edgecolor='black',\n",
    "                   label=c)\n",
    "\n",
    "    if hide_spines:\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['left'].set_visible(False)\n",
    "        ax.spines['bottom'].set_visible(False)\n",
    "    ax.yaxis.set_ticks_position('left')\n",
    "    ax.xaxis.set_ticks_position('bottom')\n",
    "    if dim == 1:\n",
    "        ax.axes.get_yaxis().set_ticks([])\n",
    "\n",
    "    if legend:\n",
    "        if dim > 2 and filler_feature_ranges is None:\n",
    "            pass\n",
    "        else:\n",
    "            handles, labels = ax.get_legend_handles_labels()\n",
    "            ax.legend(handles, labels,\n",
    "                      framealpha=0.3, scatterpoints=1, loc=legend)\n",
    "\n",
    "    if plot_testdata:\n",
    "        if dim == 1:\n",
    "            x_data = X_highlight\n",
    "            y_data = [0 for i in X_highlight]\n",
    "        elif dim == 2:\n",
    "            x_data = X_highlight[:, x_index]\n",
    "            y_data = X_highlight[:, y_index]\n",
    "        else:\n",
    "            feature_range_mask = get_feature_range_mask(\n",
    "                    X_highlight, filler_feature_values=filler_feature_values,\n",
    "                    filler_feature_ranges=filler_feature_ranges)\n",
    "            y_data = X_highlight[feature_range_mask, y_index]\n",
    "            x_data = X_highlight[feature_range_mask, x_index]\n",
    "\n",
    "        ax.scatter(x_data,\n",
    "                   y_data,\n",
    "                   c='',\n",
    "                   edgecolor='black',\n",
    "                   alpha=1.0,\n",
    "                   linewidths=1,\n",
    "                   marker='o',\n",
    "                   s=80)\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_labeled_decision_regions(X,y, models):    \n",
    "    '''\n",
    "    Function producing a scatter plot of the instances contained \n",
    "    in the 2D dataset (X,y) along with the decision \n",
    "    regions of two trained classification models contained in the\n",
    "    list 'models'.\n",
    "            \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: pandas DataFrame corresponding to two numerical features \n",
    "    y: pandas Series corresponding the class labels\n",
    "    models: list containing two trained classifiers \n",
    "    \n",
    "    '''\n",
    "    if len(models) != 2:\n",
    "        raise Exception('''\n",
    "        Models should be a list containing only two trained classifiers.\n",
    "        ''')\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        raise Exception('''\n",
    "        X has to be a pandas DataFrame with two numerical features.\n",
    "        ''')\n",
    "    if not isinstance(y, pd.Series):\n",
    "        raise Exception('''\n",
    "        y has to be a pandas Series corresponding to the labels.\n",
    "        ''')\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(6.0,2.7), sharey=True)\n",
    "    for i, model in enumerate(models):\n",
    "        plot_decision_regions(X.values,y.values, model, legend= 2, ax = ax[i])\n",
    "        ax[i].set_title(model.__class__.__name__)\n",
    "        ax[i].set_xlabel(X.columns[0])\n",
    "        if i == 0:\n",
    "            ax[i].set_ylabel(X.columns[1])\n",
    "        ax[i].set_ylim(X.values[:,1].min(), X.values[:,1].max())\n",
    "        ax[i].set_xlim(X.values[:,0].min(), X.values[:,0].max())\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LogisticRegression from sklearn.linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Instatiate logreg\n",
    "logreg = LogisticRegression(solver = 'liblinear', random_state=1)\n",
    "\n",
    "# Fit logreg to the training set\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Define a list called clfs containing the two classifiers logreg and dt\n",
    "clfs = [logreg, dt]\n",
    "\n",
    "# Review the decision regions of the two classifiers\n",
    "# plot_labeled_decision_regions(X_test, y_test, clfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using entropy as a criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = wbc[['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean',\n",
    "       'smoothness_mean', 'compactness_mean', 'concavity_mean',\n",
    "       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n",
    "       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n",
    "       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n",
    "       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n",
    "       'perimeter_worst', 'area_worst', 'smoothness_worst',\n",
    "       'compactness_worst', 'concavity_worst', 'concave points_worst',\n",
    "       'symmetry_worst', 'fractal_dimension_worst']]\n",
    "y = wbc['diagnosis'].apply(lambda x: 1 if x == 'M' else 0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=8,\n",
       "                       max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort=False,\n",
       "                       random_state=1, splitter='best')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import DecisionTreeClassifier from sklearn.tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Instantiate dt_entropy, set 'entropy' as the information criterion\n",
    "dt_entropy = DecisionTreeClassifier(max_depth = 8, criterion='entropy', random_state=1)\n",
    "\n",
    "# Fit dt_entropy to the training set\n",
    "dt_entropy.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=8,\n",
       "                       max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort=False,\n",
       "                       random_state=1, splitter='best')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate dt_gini, set 'gini' as the information criterion\n",
    "dt_gini = DecisionTreeClassifier(max_depth = 8, criterion='gini', random_state=1)\n",
    "\n",
    "# Fit dt_gini to the training set\n",
    "dt_gini.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy vs Gini index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy achieved by using entropy:  0.9210526315789473\n"
     ]
    }
   ],
   "source": [
    "# Import accuracy_score from sklearn.metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Use dt_entropy to predict test set labels\n",
    "y_pred = dt_entropy.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy_entropy\n",
    "accuracy_entropy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print accuracy_entropy\n",
    "print('Accuracy achieved by using entropy: ', accuracy_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy achieved by using the gini index:  0.8859649122807017\n"
     ]
    }
   ],
   "source": [
    "# Use dt_entropy to predict test set labels\n",
    "y_pred = dt_gini.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy_gini\n",
    "accuracy_gini = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print accuracy_gini\n",
    "print('Accuracy achieved by using the gini index: ', accuracy_gini)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train your first regression tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>displ</th>\n",
       "      <th>hp</th>\n",
       "      <th>weight</th>\n",
       "      <th>accel</th>\n",
       "      <th>size</th>\n",
       "      <th>origin_Asia</th>\n",
       "      <th>origin_Europe</th>\n",
       "      <th>origin_US</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>88</td>\n",
       "      <td>3139</td>\n",
       "      <td>14.5</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.0</td>\n",
       "      <td>304.0</td>\n",
       "      <td>193</td>\n",
       "      <td>4732</td>\n",
       "      <td>18.5</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36.1</td>\n",
       "      <td>91.0</td>\n",
       "      <td>60</td>\n",
       "      <td>1800</td>\n",
       "      <td>16.4</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18.5</td>\n",
       "      <td>250.0</td>\n",
       "      <td>98</td>\n",
       "      <td>3525</td>\n",
       "      <td>19.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34.3</td>\n",
       "      <td>97.0</td>\n",
       "      <td>78</td>\n",
       "      <td>2188</td>\n",
       "      <td>15.8</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mpg  displ   hp  weight  accel  size  origin_Asia  origin_Europe  \\\n",
       "0  18.0  250.0   88    3139   14.5  15.0            0              0   \n",
       "1   9.0  304.0  193    4732   18.5  20.0            0              0   \n",
       "2  36.1   91.0   60    1800   16.4  10.0            1              0   \n",
       "3  18.5  250.0   98    3525   19.0  15.0            0              0   \n",
       "4  34.3   97.0   78    2188   15.8  10.0            0              1   \n",
       "\n",
       "   origin_US  \n",
       "0          1  \n",
       "1          1  \n",
       "2          0  \n",
       "3          1  \n",
       "4          0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto = pd.read_csv('auto.csv')\n",
    "auto.columns\n",
    "auto_origin = pd.get_dummies(auto.origin)\n",
    "auto = pd.concat([auto, auto_origin], axis = 1).drop('origin', axis = 1)\n",
    "auto.columns = ['mpg', 'displ', 'hp', 'weight', 'accel', 'size', 'origin_Asia', 'origin_Europe', 'origin_US']\n",
    "auto.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = auto[['displ', 'hp', 'weight', 'accel', 'size', 'origin_Asia',\n",
    "       'origin_Europe', 'origin_US']]\n",
    "y = auto['mpg']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=8, max_features=None,\n",
       "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=0.13,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      presort=False, random_state=3, splitter='best')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import DecisionTreeRegressor from sklearn.tree\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Instantiate dt\n",
    "dt = DecisionTreeRegressor(max_depth=8,\n",
    "             min_samples_leaf=0.13,\n",
    "            random_state=3)\n",
    "\n",
    "# Fit dt to the training set\n",
    "dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the regression tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set RMSE of dt: 4.31\n"
     ]
    }
   ],
   "source": [
    "# Import mean_squared_error from sklearn.metrics as MSE\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Compute y_pred\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "# Compute mse_dt\n",
    "mse_dt = MSE(y_test, y_pred)\n",
    "\n",
    "# Compute rmse_dt\n",
    "rmse_dt = (mse_dt)**0.5\n",
    "\n",
    "# Print rmse_dt\n",
    "print(\"Test set RMSE of dt: {:.2f}\".format(rmse_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression vs regression tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression test set RMSE: 4.14\n"
     ]
    }
   ],
   "source": [
    "# Predict test set labels \n",
    "y_pred_lr = lr.predict(X_test)\n",
    "\n",
    "# Compute mse_lr\n",
    "mse_lr = MSE(y_test, y_pred_lr)\n",
    "\n",
    "# Compute rmse_lr\n",
    "rmse_lr = mse_lr**(1/2)\n",
    "\n",
    "# Print rmse_lr\n",
    "print('Linear Regression test set RMSE: {:.2f}'.format(rmse_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Bias-Variance Tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train_test_split from sklearn.model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set SEED for reproducibility\n",
    "SEED = 1\n",
    "\n",
    "# Split the data into 70% train and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=SEED)\n",
    "\n",
    "# Instantiate a DecisionTreeRegressor dt\n",
    "dt = DecisionTreeRegressor(max_depth = 4, min_samples_leaf = 0.26, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the 10-fold CV error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV RMSE: 5.14\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "# Compute the array containing the 10-folds CV MSEs\n",
    "MSE_CV_scores = - cross_val_score(dt, X_train, y_train, cv=10, \n",
    "                                  scoring='neg_mean_squared_error', \n",
    "                                  n_jobs=-1) \n",
    "\n",
    "# Compute the 10-folds CV RMSE\n",
    "RMSE_CV = (MSE_CV_scores.mean())**(1/2)\n",
    "\n",
    "# Print RMSE_CV\n",
    "print('CV RMSE: {:.2f}'.format(RMSE_CV))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the training error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE: 5.15\n"
     ]
    }
   ],
   "source": [
    "# Import mean_squared_error from sklearn.metrics as MSE\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Fit dt to the training set\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the training set\n",
    "y_pred_train = dt.predict(X_train)\n",
    "\n",
    "# Evaluate the training set RMSE of dt\n",
    "RMSE_train = (MSE(y_train, y_pred_train))**(0.5)\n",
    "\n",
    "# Print RMSE_train\n",
    "print('Train RMSE: {:.2f}'.format(RMSE_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age_std</th>\n",
       "      <th>Total_Bilirubin_std</th>\n",
       "      <th>Direct_Bilirubin_std</th>\n",
       "      <th>Alkaline_Phosphotase_std</th>\n",
       "      <th>Alamine_Aminotransferase_std</th>\n",
       "      <th>Aspartate_Aminotransferase_std</th>\n",
       "      <th>Total_Protiens_std</th>\n",
       "      <th>Albumin_std</th>\n",
       "      <th>Albumin_and_Globulin_Ratio_std</th>\n",
       "      <th>Is_male_std</th>\n",
       "      <th>Liver_disease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.247403</td>\n",
       "      <td>-0.420320</td>\n",
       "      <td>-0.495414</td>\n",
       "      <td>-0.428870</td>\n",
       "      <td>-0.355832</td>\n",
       "      <td>-0.319111</td>\n",
       "      <td>0.293722</td>\n",
       "      <td>0.203446</td>\n",
       "      <td>-0.147390</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.062306</td>\n",
       "      <td>1.218936</td>\n",
       "      <td>1.423518</td>\n",
       "      <td>1.675083</td>\n",
       "      <td>-0.093573</td>\n",
       "      <td>-0.035962</td>\n",
       "      <td>0.939655</td>\n",
       "      <td>0.077462</td>\n",
       "      <td>-0.648461</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.062306</td>\n",
       "      <td>0.640375</td>\n",
       "      <td>0.926017</td>\n",
       "      <td>0.816243</td>\n",
       "      <td>-0.115428</td>\n",
       "      <td>-0.146459</td>\n",
       "      <td>0.478274</td>\n",
       "      <td>0.203446</td>\n",
       "      <td>-0.178707</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.815511</td>\n",
       "      <td>-0.372106</td>\n",
       "      <td>-0.388807</td>\n",
       "      <td>-0.449416</td>\n",
       "      <td>-0.366760</td>\n",
       "      <td>-0.312205</td>\n",
       "      <td>0.293722</td>\n",
       "      <td>0.329431</td>\n",
       "      <td>0.165780</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.679294</td>\n",
       "      <td>0.093956</td>\n",
       "      <td>0.179766</td>\n",
       "      <td>-0.395996</td>\n",
       "      <td>-0.295731</td>\n",
       "      <td>-0.177537</td>\n",
       "      <td>0.755102</td>\n",
       "      <td>-0.930414</td>\n",
       "      <td>-1.713237</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Age_std  Total_Bilirubin_std  Direct_Bilirubin_std  \\\n",
       "0  1.247403            -0.420320             -0.495414   \n",
       "1  1.062306             1.218936              1.423518   \n",
       "2  1.062306             0.640375              0.926017   \n",
       "3  0.815511            -0.372106             -0.388807   \n",
       "4  1.679294             0.093956              0.179766   \n",
       "\n",
       "   Alkaline_Phosphotase_std  Alamine_Aminotransferase_std  \\\n",
       "0                 -0.428870                     -0.355832   \n",
       "1                  1.675083                     -0.093573   \n",
       "2                  0.816243                     -0.115428   \n",
       "3                 -0.449416                     -0.366760   \n",
       "4                 -0.395996                     -0.295731   \n",
       "\n",
       "   Aspartate_Aminotransferase_std  Total_Protiens_std  Albumin_std  \\\n",
       "0                       -0.319111            0.293722     0.203446   \n",
       "1                       -0.035962            0.939655     0.077462   \n",
       "2                       -0.146459            0.478274     0.203446   \n",
       "3                       -0.312205            0.293722     0.329431   \n",
       "4                       -0.177537            0.755102    -0.930414   \n",
       "\n",
       "   Albumin_and_Globulin_Ratio_std  Is_male_std  Liver_disease  \n",
       "0                       -0.147390            0              1  \n",
       "1                       -0.648461            1              1  \n",
       "2                       -0.178707            1              1  \n",
       "3                        0.165780            1              1  \n",
       "4                       -1.713237            1              1  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liver = pd.read_csv('indian_liver_patient/indian_liver_patient_preprocessed.csv', index_col = 0)\n",
    "X = liver.drop('Liver_disease', axis = 1)\n",
    "y = liver['Liver_disease']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=SEED)\n",
    "liver.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "# Set seed for reproducibility\n",
    "SEED=1\n",
    "\n",
    "# Instantiate lr\n",
    "lr = LogisticRegression(random_state=SEED, solver = 'liblinear')\n",
    "\n",
    "# Instantiate knn\n",
    "knn = KNN(n_neighbors=27)\n",
    "\n",
    "# Instantiate dt\n",
    "dt = DecisionTreeClassifier(min_samples_leaf=0.13, random_state=SEED)\n",
    "\n",
    "# Define the list classifiers\n",
    "classifiers = [('Logistic Regression', lr), ('K Nearest Neighbours', knn), ('Classification Tree', dt)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate individual classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression : 0.764\n",
      "K Nearest Neighbours : 0.701\n",
      "Classification Tree : 0.730\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the pre-defined list of classifiers\n",
    "for clf_name, clf in classifiers:    \n",
    " \n",
    "    # Fit clf to the training set\n",
    "    clf.fit(X_train, y_train)\n",
    "   \n",
    "    # Predict y_pred\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy =accuracy_score(y_test, y_pred)\n",
    "   \n",
    "    # Evaluate clf's accuracy on the test set\n",
    "    print('{:s} : {:.3f}'.format(clf_name, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Better performance with a Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Classifier: 0.770\n"
     ]
    }
   ],
   "source": [
    "# Import VotingClassifier from sklearn.ensemble\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Instantiate a VotingClassifier vc\n",
    "vc = VotingClassifier(estimators=classifiers)     \n",
    "\n",
    "# Fit vc to the training set\n",
    "vc.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the test set predictions\n",
    "y_pred = vc.predict(X_test)\n",
    "\n",
    "# Calculate accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Voting Classifier: {:.3f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging and Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the bagging classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Import BaggingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# Instantiate dt\n",
    "dt = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "# Instantiate bc\n",
    "bc = BaggingClassifier(base_estimator=dt, n_estimators=50, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Bagging performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy of bc: 0.69\n"
     ]
    }
   ],
   "source": [
    "# Fit bc to the training set\n",
    "bc.fit(X_train, y_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = bc.predict(X_test)\n",
    "\n",
    "# Evaluate acc_test\n",
    "acc_test = accuracy_score(y_test, y_pred)\n",
    "print('Test set accuracy of bc: {:.2f}'.format(acc_test)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the ground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Import BaggingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# Instantiate dt\n",
    "dt = DecisionTreeClassifier(min_samples_leaf=8, random_state=1)\n",
    "\n",
    "# Instantiate bc\n",
    "bc = BaggingClassifier(base_estimator=dt, \n",
    "            n_estimators=50,\n",
    "            oob_score=True,\n",
    "            random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OOB Score vs Test Set Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.718, OOB accuracy: 0.686\n"
     ]
    }
   ],
   "source": [
    "# Fit bc to the training set \n",
    "bc.fit(X_train, y_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = bc.predict(X_test)\n",
    "\n",
    "# Evaluate test set accuracy\n",
    "acc_test = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Evaluate OOB accuracy\n",
    "acc_oob = bc.oob_score_\n",
    "\n",
    "# Print acc_test and acc_oob\n",
    "print('Test set accuracy: {:.3f}, OOB accuracy: {:.3f}'.format(acc_test, acc_oob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train an RF regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hr</th>\n",
       "      <th>holiday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>temp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>cnt</th>\n",
       "      <th>instant</th>\n",
       "      <th>mnth</th>\n",
       "      <th>yr</th>\n",
       "      <th>Clear to partly cloudy</th>\n",
       "      <th>Light Precipitation</th>\n",
       "      <th>Misty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>149</td>\n",
       "      <td>13004</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.1343</td>\n",
       "      <td>93</td>\n",
       "      <td>13005</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.0896</td>\n",
       "      <td>90</td>\n",
       "      <td>13006</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.1343</td>\n",
       "      <td>33</td>\n",
       "      <td>13007</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.1940</td>\n",
       "      <td>4</td>\n",
       "      <td>13008</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   hr  holiday  workingday  temp   hum  windspeed  cnt  instant  mnth  yr  \\\n",
       "0   0        0           0  0.76  0.66     0.0000  149    13004     7   1   \n",
       "1   1        0           0  0.74  0.70     0.1343   93    13005     7   1   \n",
       "2   2        0           0  0.72  0.74     0.0896   90    13006     7   1   \n",
       "3   3        0           0  0.72  0.84     0.1343   33    13007     7   1   \n",
       "4   4        0           0  0.70  0.79     0.1940    4    13008     7   1   \n",
       "\n",
       "   Clear to partly cloudy  Light Precipitation  Misty  \n",
       "0                       1                    0      0  \n",
       "1                       1                    0      0  \n",
       "2                       1                    0      0  \n",
       "3                       1                    0      0  \n",
       "4                       1                    0      0  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bike = pd.read_csv('bikes.csv')\n",
    "X = bike[['hr', 'holiday', 'workingday', 'temp', 'hum', 'windspeed', 'instant',\n",
    "       'mnth', 'yr', 'Clear to partly cloudy', 'Light Precipitation', 'Misty']]\n",
    "y = bike['cnt']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)\n",
    "bike.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "                      max_features='auto', max_leaf_nodes=None,\n",
       "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                      min_samples_leaf=1, min_samples_split=2,\n",
       "                      min_weight_fraction_leaf=0.0, n_estimators=25,\n",
       "                      n_jobs=None, oob_score=False, random_state=2, verbose=0,\n",
       "                      warm_start=False)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Instantiate rf\n",
    "rf = RandomForestRegressor(n_estimators=25,\n",
    "            random_state=2)\n",
    "            \n",
    "# Fit rf to the training set    \n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the RF regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set RMSE of rf: 51.84\n"
     ]
    }
   ],
   "source": [
    "# Import mean_squared_error as MSE\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Predict the test set labels\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Evaluate the test set RMSE\n",
    "rmse_test = (MSE(y_test, y_pred))**0.5\n",
    "\n",
    "# Print rmse_test\n",
    "print('Test set RMSE of rf: {:.2f}'.format(rmse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing features importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a pd.Series of features importances\n",
    "importances = pd.Series(data=rf.feature_importances_,\n",
    "                        index= X_train.columns)\n",
    "\n",
    "# Sort importances\n",
    "importances_sorted = importances.sort_values()\n",
    "\n",
    "# Draw a horizontal barplot of importances_sorted\n",
    "importances_sorted.plot(kind = 'barh', color = 'lightgreen')\n",
    "plt.title('Features Importances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the AdaBoost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Import AdaBoostClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Instantiate dt\n",
    "dt = DecisionTreeClassifier(max_depth = 2, random_state=1)\n",
    "\n",
    "# Instantiate ada\n",
    "ada = AdaBoostClassifier(base_estimator=dt, n_estimators=180, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the AdaBoost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = liver.drop('Liver_disease', axis = 1)\n",
    "y = liver['Liver_disease']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit ada to the training set\n",
    "ada.fit(X_train, y_train)\n",
    "\n",
    "# Compute the probabilities of obtaining the positive class\n",
    "y_pred_proba = ada.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the AdaBoost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC score: 0.64\n"
     ]
    }
   ],
   "source": [
    "# Import roc_auc_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Evaluate test-set roc_auc_score\n",
    "ada_roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# Print roc_auc_score\n",
    "print('ROC AUC score: {:.2f}'.format(ada_roc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the GB regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import GradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor \n",
    "\n",
    "# Instantiate gb\n",
    "gb = GradientBoostingRegressor(max_depth = 4, \n",
    "            n_estimators = 200,\n",
    "            random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the GB regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = bike[['hr', 'holiday', 'workingday', 'temp', 'hum', 'windspeed', 'instant',\n",
    "       'mnth', 'yr', 'Clear to partly cloudy', 'Light Precipitation', 'Misty']]\n",
    "y = bike['cnt']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit gb to the training set\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = gb.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the GB regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set RMSE of gb: 43.132\n"
     ]
    }
   ],
   "source": [
    "# Import mean_squared_error as MSE\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Compute MSE\n",
    "mse_test = MSE(y_test, y_pred)\n",
    "\n",
    "# Compute RMSE\n",
    "rmse_test = mse_test ** 0.5\n",
    "\n",
    "# Print RMSE\n",
    "print('Test set RMSE of gb: {:.3f}'.format(rmse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression with SGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import GradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Instantiate sgbr\n",
    "sgbr = GradientBoostingRegressor(max_depth=4, \n",
    "            subsample=0.9,\n",
    "            max_features=0.75,\n",
    "            n_estimators=200,                                \n",
    "            random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the SGB regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit sgbr to the training set\n",
    "sgbr.fit(X_train, y_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = sgbr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the SGB regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set RMSE of sgbr: 44.747\n"
     ]
    }
   ],
   "source": [
    "# Import mean_squared_error as MSE\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Compute test set MSE\n",
    "mse_test = MSE(y_test, y_pred)\n",
    "\n",
    "# Compute test set RMSE\n",
    "rmse_test = mse_test ** 0.5\n",
    "\n",
    "# Print rmse_test\n",
    "print('Test set RMSE of sgbr: {:.3f}'.format(rmse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the tree's hyperparameter grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define params_dt\n",
    "params_dt = {'max_depth': [2, 3, 4], 'min_samples_leaf': [0.12, 0.14, 0.16, 0.18]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search for the optimal tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Instantiate grid_dt\n",
    "grid_dt = GridSearchCV(estimator=dt,\n",
    "                       param_grid=params_dt,\n",
    "                       scoring='roc_auc',\n",
    "                       cv=5,\n",
    "                       n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the optimal tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huiren/anaconda3/envs/2019/lib/python3.6/site-packages/sklearn/model_selection/_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=DecisionTreeClassifier(class_weight=None,\n",
       "                                              criterion='gini', max_depth=2,\n",
       "                                              max_features=None,\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samples_split=2,\n",
       "                                              min_weight_fraction_leaf=0.0,\n",
       "                                              presort=False, random_state=1,\n",
       "                                              splitter='best'),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'max_depth': [2, 3, 4],\n",
       "                         'min_samples_leaf': [0.12, 0.14, 0.16, 0.18]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='roc_auc', verbose=0)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = liver.drop('Liver_disease', axis = 1)\n",
    "y = liver['Liver_disease']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)\n",
    "grid_dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set ROC AUC score: 0.731\n"
     ]
    }
   ],
   "source": [
    "# Import roc_auc_score from sklearn.metrics \n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Extract the best estimator\n",
    "best_model = grid_dt.best_estimator_\n",
    "\n",
    "# Predict the test set probabilities of the positive class\n",
    "y_pred_proba = best_model.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Compute test_roc_auc\n",
    "test_roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# Print test_roc_auc\n",
    "print('Test set ROC AUC score: {:.3f}'.format(test_roc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the hyperparameter grid of RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dictionary 'params_rf'\n",
    "params_rf = {'n_estimators': [100, 350, 500], 'max_features': ['log2', 'auto', 'sqrt'], 'min_samples_leaf': [2, 10, 30]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search for the optimal forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
    "           max_features='auto', max_leaf_nodes=None,\n",
    "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "           min_samples_leaf=1, min_samples_split=2,\n",
    "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=-1,\n",
    "           oob_score=False, random_state=2, verbose=0, warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import GridSearchCV\n",
    "from sklearn.model_selection import  GridSearchCV\n",
    "\n",
    "# Instantiate grid_rf\n",
    "grid_rf = GridSearchCV(estimator=rf,\n",
    "                       param_grid=params_rf,\n",
    "                       scoring='neg_mean_squared_error',\n",
    "                       cv=3,\n",
    "                       verbose=1,\n",
    "                       n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   10.0s\n",
      "[Parallel(n_jobs=-1)]: Done  81 out of  81 | elapsed:   17.8s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "             estimator=RandomForestRegressor(bootstrap=True, criterion='mse',\n",
       "                                             max_depth=None,\n",
       "                                             max_features='auto',\n",
       "                                             max_leaf_nodes=None,\n",
       "                                             min_impurity_decrease=0.0,\n",
       "                                             min_impurity_split=None,\n",
       "                                             min_samples_leaf=1,\n",
       "                                             min_samples_split=2,\n",
       "                                             min_weight_fraction_leaf=0.0,\n",
       "                                             n_estimators=10, n_jobs=-1,\n",
       "                                             oob_score=False, random_state=2,\n",
       "                                             verbose=0, warm_start=False),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'max_features': ['log2', 'auto', 'sqrt'],\n",
       "                         'min_samples_leaf': [2, 10, 30],\n",
       "                         'n_estimators': [100, 350, 500]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='neg_mean_squared_error', verbose=1)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = bike[['hr', 'holiday', 'workingday', 'temp', 'hum', 'windspeed', 'instant',\n",
    "       'mnth', 'yr', 'Clear to partly cloudy', 'Light Precipitation', 'Misty']]\n",
    "y = bike['cnt']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)\n",
    "grid_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the optimal forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE of best model: 51.755\n"
     ]
    }
   ],
   "source": [
    "# Import mean_squared_error from sklearn.metrics as MSE \n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Extract the best estimator\n",
    "best_model = grid_rf.best_estimator_\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Compute rmse_test\n",
    "rmse_test = MSE(y_test, y_pred)**0.5\n",
    "\n",
    "# Print rmse_test\n",
    "print('Test RMSE of best model: {:.3f}'.format(rmse_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
