{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification and Regression Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train your first classification tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 importa o train_test_split\n",
    "# 2 lê o csv\n",
    "# 3 diz quais colunas são o x\n",
    "# 4 diz qual coluna é o y e já faz um apply de uma função lambda que se x = 'M' muda pra 1, else muda pra 0\n",
    "# 5 executa o train_test_split dizendo o tamanho da fatia de teste\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "wbc = pd.read_csv('wbc.csv')\n",
    "X = wbc[['radius_mean', 'concave points_mean']]\n",
    "y = wbc['diagnosis'].apply(lambda x: 1 if x == 'M' else 0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Import DecisionTreeClassifier from sklearn.tree\n",
    "# cria variável semente com valor da semente = 1 \n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "\n",
    "SEED = 1 \n",
    "\n",
    "# Instantiate a DecisionTreeClassifier 'dt' with a maximum depth of 6\n",
    "dt = DecisionTreeClassifier(max_depth=6, random_state=SEED)\n",
    "\n",
    "# Fit dt to the training set x e y, obviamente ambos de treino\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Predict test set labels, pega o predict e aplica no X teste, este cara nos dá o y_pred\n",
    "# lembrando que é uma classificacao, resultando em 0s e 1s\n",
    "y_pred = dt.predict(X_test)\n",
    "print(y_pred[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the classification tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.89\n"
     ]
    }
   ],
   "source": [
    "# Import accuracy_score - MÉTRICA ACURÁCIA - CLASSIFICACAO DÁ 0s e 1s, logo quantos % bateram\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "# Compute test set accuracy  \n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Test set accuracy: {:.2f}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression vs classification tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_regions(X, y, clf,\n",
    "                          feature_index=None,\n",
    "                          filler_feature_values=None,\n",
    "                          filler_feature_ranges=None,\n",
    "                          ax=None,\n",
    "                          X_highlight=None,\n",
    "                          res=0.02, legend=1,\n",
    "                          hide_spines=True,\n",
    "                          markers='s^oxv<>',\n",
    "                          colors='red,blue,limegreen,gray,cyan'):\n",
    "    \"\"\"Plot decision regions of a classifier.\n",
    "\n",
    "    Please note that this functions assumes that class labels are\n",
    "    labeled consecutively, e.g,. 0, 1, 2, 3, 4, and 5. If you have class\n",
    "    labels with integer labels > 4, you may want to provide additional colors\n",
    "    and/or markers as `colors` and `markers` arguments.\n",
    "    See http://matplotlib.org/examples/color/named_colors.html for more\n",
    "    information.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like, shape = [n_samples, n_features]\n",
    "        Feature Matrix.\n",
    "    y : array-like, shape = [n_samples]\n",
    "        True class labels.\n",
    "    clf : Classifier object.\n",
    "        Must have a .predict method.\n",
    "    feature_index : array-like (default: (0,) for 1D, (0, 1) otherwise)\n",
    "        Feature indices to use for plotting. The first index in\n",
    "        `feature_index` will be on the x-axis, the second index will be\n",
    "        on the y-axis.\n",
    "    filler_feature_values : dict (default: None)\n",
    "        Only needed for number features > 2. Dictionary of feature\n",
    "        index-value pairs for the features not being plotted.\n",
    "    filler_feature_ranges : dict (default: None)\n",
    "        Only needed for number features > 2. Dictionary of feature\n",
    "        index-value pairs for the features not being plotted. Will use the\n",
    "        ranges provided to select training samples for plotting.\n",
    "    ax : matplotlib.axes.Axes (default: None)\n",
    "        An existing matplotlib Axes. Creates\n",
    "        one if ax=None.\n",
    "    X_highlight : array-like, shape = [n_samples, n_features] (default: None)\n",
    "        An array with data points that are used to highlight samples in `X`.\n",
    "    res : float or array-like, shape = (2,) (default: 0.02)\n",
    "        Grid width. If float, same resolution is used for both the x- and\n",
    "        y-axis. If array-like, the first item is used on the x-axis, the\n",
    "        second is used on the y-axis. Lower values increase the resolution but\n",
    "        slow down the plotting.\n",
    "    hide_spines : bool (default: True)\n",
    "        Hide axis spines if True.\n",
    "    legend : int (default: 1)\n",
    "        Integer to specify the legend location.\n",
    "        No legend if legend is 0.\n",
    "    markers : str (default 's^oxv<>')\n",
    "        Scatterplot markers.\n",
    "    colors : str (default 'red,blue,limegreen,gray,cyan')\n",
    "        Comma separated list of colors.\n",
    "\n",
    "    Returns\n",
    "    ---------\n",
    "    ax : matplotlib.axes.Axes object\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    check_Xy(X, y, y_int=True)  # Validate X and y arrays\n",
    "    dim = X.shape[1]\n",
    "\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    if isinstance(res, float):\n",
    "        xres, yres = res, res\n",
    "    else:\n",
    "        try:\n",
    "            xres, yres = res\n",
    "        except ValueError:\n",
    "            raise ValueError('Unable to unpack res. Expecting '\n",
    "                             'array-like input of length 2.')\n",
    "\n",
    "    plot_testdata = True\n",
    "    if not isinstance(X_highlight, np.ndarray):\n",
    "        if X_highlight is not None:\n",
    "            raise ValueError('X_highlight must be a NumPy array or None')\n",
    "        else:\n",
    "            plot_testdata = False\n",
    "    elif len(X_highlight.shape) < 2:\n",
    "        raise ValueError('X_highlight must be a 2D array')\n",
    "\n",
    "    if feature_index is not None:\n",
    "        # Unpack and validate the feature_index values\n",
    "        if dim == 1:\n",
    "            raise ValueError(\n",
    "                'feature_index requires more than one training feature')\n",
    "        try:\n",
    "            x_index, y_index = feature_index\n",
    "        except ValueError:\n",
    "            raise ValueError(\n",
    "                'Unable to unpack feature_index. Make sure feature_index '\n",
    "                'only has two dimensions.')\n",
    "        try:\n",
    "            X[:, x_index], X[:, y_index]\n",
    "        except IndexError:\n",
    "            raise IndexError(\n",
    "                'feature_index values out of range. X.shape is {}, but '\n",
    "                'feature_index is {}'.format(X.shape, feature_index))\n",
    "    else:\n",
    "        feature_index = (0, 1)\n",
    "        x_index, y_index = feature_index\n",
    "\n",
    "    # Extra input validation for higher number of training features\n",
    "    if dim > 2:\n",
    "        if filler_feature_values is None:\n",
    "            raise ValueError('Filler values must be provided when '\n",
    "                             'X has more than 2 training features.')\n",
    "\n",
    "        if filler_feature_ranges is not None:\n",
    "            if not set(filler_feature_values) == set(filler_feature_ranges):\n",
    "                raise ValueError(\n",
    "                    'filler_feature_values and filler_feature_ranges must '\n",
    "                    'have the same keys')\n",
    "\n",
    "        # Check that all columns in X are accounted for\n",
    "        column_check = np.zeros(dim, dtype=bool)\n",
    "        for idx in filler_feature_values:\n",
    "            column_check[idx] = True\n",
    "        for idx in feature_index:\n",
    "            column_check[idx] = True\n",
    "        if not all(column_check):\n",
    "            missing_cols = np.argwhere(~column_check).flatten()\n",
    "            raise ValueError(\n",
    "                'Column(s) {} need to be accounted for in either '\n",
    "                'feature_index or filler_feature_values'.format(missing_cols))\n",
    "\n",
    "    marker_gen = cycle(list(markers))\n",
    "\n",
    "    n_classes = np.unique(y).shape[0]\n",
    "    colors = colors.split(',')\n",
    "    colors_gen = cycle(colors)\n",
    "    colors = [next(colors_gen) for c in range(n_classes)]\n",
    "\n",
    "    # Get minimum and maximum\n",
    "    x_min, x_max = X[:, x_index].min() - 1, X[:, x_index].max() + 1\n",
    "    if dim == 1:\n",
    "        y_min, y_max = -1, 1\n",
    "    else:\n",
    "        y_min, y_max = X[:, y_index].min() - 1, X[:, y_index].max() + 1\n",
    "\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, xres),\n",
    "                         np.arange(y_min, y_max, yres))\n",
    "\n",
    "    if dim == 1:\n",
    "        X_predict = np.array([xx.ravel()]).T\n",
    "    else:\n",
    "        X_grid = np.array([xx.ravel(), yy.ravel()]).T\n",
    "        X_predict = np.zeros((X_grid.shape[0], dim))\n",
    "        X_predict[:, x_index] = X_grid[:, 0]\n",
    "        X_predict[:, y_index] = X_grid[:, 1]\n",
    "        if dim > 2:\n",
    "            for feature_idx in filler_feature_values:\n",
    "                X_predict[:, feature_idx] = filler_feature_values[feature_idx]\n",
    "    Z = clf.predict(X_predict)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    # Plot decisoin region\n",
    "    ax.contourf(xx, yy, Z,\n",
    "                alpha=0.3,\n",
    "                colors=colors,\n",
    "                levels=np.arange(Z.max() + 2) - 0.5)\n",
    "\n",
    "    ax.axis(xmin=xx.min(), xmax=xx.max(), y_min=yy.min(), y_max=yy.max())\n",
    "\n",
    "    # Scatter training data samples\n",
    "    for idx, c in enumerate(np.unique(y)):\n",
    "        if dim == 1:\n",
    "            y_data = [0 for i in X[y == c]]\n",
    "            x_data = X[y == c]\n",
    "        elif dim == 2:\n",
    "            y_data = X[y == c, y_index]\n",
    "            x_data = X[y == c, x_index]\n",
    "        elif dim > 2 and filler_feature_ranges is not None:\n",
    "            class_mask = y == c\n",
    "            feature_range_mask = get_feature_range_mask(\n",
    "                            X, filler_feature_values=filler_feature_values,\n",
    "                            filler_feature_ranges=filler_feature_ranges)\n",
    "            y_data = X[class_mask & feature_range_mask, y_index]\n",
    "            x_data = X[class_mask & feature_range_mask, x_index]\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        ax.scatter(x=x_data,\n",
    "                   y=y_data,\n",
    "                   alpha=0.8,\n",
    "                   c=colors[idx],\n",
    "                   marker=next(marker_gen),\n",
    "                   edgecolor='black',\n",
    "                   label=c)\n",
    "\n",
    "    if hide_spines:\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['left'].set_visible(False)\n",
    "        ax.spines['bottom'].set_visible(False)\n",
    "    ax.yaxis.set_ticks_position('left')\n",
    "    ax.xaxis.set_ticks_position('bottom')\n",
    "    if dim == 1:\n",
    "        ax.axes.get_yaxis().set_ticks([])\n",
    "\n",
    "    if legend:\n",
    "        if dim > 2 and filler_feature_ranges is None:\n",
    "            pass\n",
    "        else:\n",
    "            handles, labels = ax.get_legend_handles_labels()\n",
    "            ax.legend(handles, labels,\n",
    "                      framealpha=0.3, scatterpoints=1, loc=legend)\n",
    "\n",
    "    if plot_testdata:\n",
    "        if dim == 1:\n",
    "            x_data = X_highlight\n",
    "            y_data = [0 for i in X_highlight]\n",
    "        elif dim == 2:\n",
    "            x_data = X_highlight[:, x_index]\n",
    "            y_data = X_highlight[:, y_index]\n",
    "        else:\n",
    "            feature_range_mask = get_feature_range_mask(\n",
    "                    X_highlight, filler_feature_values=filler_feature_values,\n",
    "                    filler_feature_ranges=filler_feature_ranges)\n",
    "            y_data = X_highlight[feature_range_mask, y_index]\n",
    "            x_data = X_highlight[feature_range_mask, x_index]\n",
    "\n",
    "        ax.scatter(x_data,\n",
    "                   y_data,\n",
    "                   c='',\n",
    "                   edgecolor='black',\n",
    "                   alpha=1.0,\n",
    "                   linewidths=1,\n",
    "                   marker='o',\n",
    "                   s=80)\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_labeled_decision_regions(X,y, models):    \n",
    "    '''\n",
    "    Function producing a scatter plot of the instances contained \n",
    "    in the 2D dataset (X,y) along with the decision \n",
    "    regions of two trained classification models contained in the\n",
    "    list 'models'.\n",
    "            \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: pandas DataFrame corresponding to two numerical features \n",
    "    y: pandas Series corresponding the class labels\n",
    "    models: list containing two trained classifiers \n",
    "    \n",
    "    '''\n",
    "    if len(models) != 2:\n",
    "        raise Exception('''\n",
    "        Models should be a list containing only two trained classifiers.\n",
    "        ''')\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        raise Exception('''\n",
    "        X has to be a pandas DataFrame with two numerical features.\n",
    "        ''')\n",
    "    if not isinstance(y, pd.Series):\n",
    "        raise Exception('''\n",
    "        y has to be a pandas Series corresponding to the labels.\n",
    "        ''')\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(6.0,2.7), sharey=True)\n",
    "    for i, model in enumerate(models):\n",
    "        plot_decision_regions(X.values,y.values, model, legend= 2, ax = ax[i])\n",
    "        ax[i].set_title(model.__class__.__name__)\n",
    "        ax[i].set_xlabel(X.columns[0])\n",
    "        if i == 0:\n",
    "            ax[i].set_ylabel(X.columns[1])\n",
    "        ax[i].set_ylim(X.values[:,1].min(), X.values[:,1].max())\n",
    "        ax[i].set_xlim(X.values[:,0].min(), X.values[:,0].max())\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LogisticRegression from sklearn.linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Instatiate logreg\n",
    "logreg = LogisticRegression(solver = 'liblinear', random_state=1)\n",
    "\n",
    "# Fit logreg to the training set\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Define a list called clfs containing the two classifiers logreg and dt\n",
    "clfs = [logreg, dt]\n",
    "\n",
    "# Review the decision regions of the two classifiers\n",
    "# plot_labeled_decision_regions(X_test, y_test, clfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using entropy as a criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = wbc[['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean',\n",
    "       'smoothness_mean', 'compactness_mean', 'concavity_mean',\n",
    "       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n",
    "       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n",
    "       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n",
    "       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n",
    "       'perimeter_worst', 'area_worst', 'smoothness_worst',\n",
    "       'compactness_worst', 'concavity_worst', 'concave points_worst',\n",
    "       'symmetry_worst', 'fractal_dimension_worst']]\n",
    "y = wbc['diagnosis'].apply(lambda x: 1 if x == 'M' else 0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(criterion='entropy', max_depth=8, random_state=1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import DecisionTreeClassifier from sklearn.tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Instantiate dt_entropy, set 'entropy' as the information criterion\n",
    "dt_entropy = DecisionTreeClassifier(max_depth = 8, criterion='entropy', random_state=1)\n",
    "\n",
    "# Fit dt_entropy to the training set\n",
    "dt_entropy.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=8, random_state=1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate dt_gini, set 'gini' as the information criterion\n",
    "dt_gini = DecisionTreeClassifier(max_depth = 8, criterion='gini', random_state=1)\n",
    "\n",
    "# Fit dt_gini to the training set\n",
    "dt_gini.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy vs Gini index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy achieved by using entropy:  0.9298245614035088\n"
     ]
    }
   ],
   "source": [
    "# Import accuracy_score from sklearn.metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Use dt_entropy to predict test set labels\n",
    "y_pred = dt_entropy.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy_entropy\n",
    "accuracy_entropy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print accuracy_entropy\n",
    "print('Accuracy achieved by using entropy: ', accuracy_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy achieved by using the gini index:  0.9122807017543859\n"
     ]
    }
   ],
   "source": [
    "# Use dt_entropy to predict test set labels\n",
    "y_pred = dt_gini.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy_gini\n",
    "accuracy_gini = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print accuracy_gini\n",
    "print('Accuracy achieved by using the gini index: ', accuracy_gini)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train your first regression tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>displ</th>\n",
       "      <th>hp</th>\n",
       "      <th>weight</th>\n",
       "      <th>accel</th>\n",
       "      <th>size</th>\n",
       "      <th>origin_Asia</th>\n",
       "      <th>origin_Europe</th>\n",
       "      <th>origin_US</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>88</td>\n",
       "      <td>3139</td>\n",
       "      <td>14.5</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.0</td>\n",
       "      <td>304.0</td>\n",
       "      <td>193</td>\n",
       "      <td>4732</td>\n",
       "      <td>18.5</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36.1</td>\n",
       "      <td>91.0</td>\n",
       "      <td>60</td>\n",
       "      <td>1800</td>\n",
       "      <td>16.4</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18.5</td>\n",
       "      <td>250.0</td>\n",
       "      <td>98</td>\n",
       "      <td>3525</td>\n",
       "      <td>19.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34.3</td>\n",
       "      <td>97.0</td>\n",
       "      <td>78</td>\n",
       "      <td>2188</td>\n",
       "      <td>15.8</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mpg  displ   hp  weight  accel  size  origin_Asia  origin_Europe  \\\n",
       "0  18.0  250.0   88    3139   14.5  15.0            0              0   \n",
       "1   9.0  304.0  193    4732   18.5  20.0            0              0   \n",
       "2  36.1   91.0   60    1800   16.4  10.0            1              0   \n",
       "3  18.5  250.0   98    3525   19.0  15.0            0              0   \n",
       "4  34.3   97.0   78    2188   15.8  10.0            0              1   \n",
       "\n",
       "   origin_US  \n",
       "0          1  \n",
       "1          1  \n",
       "2          0  \n",
       "3          1  \n",
       "4          0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto = pd.read_csv('auto.csv')\n",
    "auto.columns\n",
    "auto_origin = pd.get_dummies(auto.origin)\n",
    "auto = pd.concat([auto, auto_origin], axis = 1).drop('origin', axis = 1)\n",
    "auto.columns = ['mpg', 'displ', 'hp', 'weight', 'accel', 'size', 'origin_Asia', 'origin_Europe', 'origin_US']\n",
    "auto.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = auto[['displ', 'hp', 'weight', 'accel', 'size', 'origin_Asia',\n",
    "       'origin_Europe', 'origin_US']]\n",
    "y = auto['mpg']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=8, min_samples_leaf=0.13, random_state=3)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import DecisionTreeRegressor from sklearn.tree\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Instantiate dt\n",
    "dt = DecisionTreeRegressor(max_depth=8,\n",
    "             min_samples_leaf=0.13,\n",
    "            random_state=3)\n",
    "\n",
    "# Fit dt to the training set\n",
    "dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the regression tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set RMSE of dt: 3.46\n"
     ]
    }
   ],
   "source": [
    "# Import mean_squared_error from sklearn.metrics as MSE\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Compute y_pred\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "# Compute mse_dt (mean squared error)\n",
    "mse_dt = MSE(y_test, y_pred)\n",
    "\n",
    "# Compute rmse_dt (raiz quadrada do erro médio)\n",
    "rmse_dt = (mse_dt)**0.5\n",
    "\n",
    "# Print rmse_dt\n",
    "print(\"Test set RMSE of dt: {:.2f}\".format(rmse_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression vs regression tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression test set RMSE: 3.72\n"
     ]
    }
   ],
   "source": [
    "# Predict test set labels \n",
    "y_pred_lr = lr.predict(X_test)\n",
    "\n",
    "# Compute mse_lr\n",
    "mse_lr = MSE(y_test, y_pred_lr)\n",
    "\n",
    "# Compute rmse_lr\n",
    "rmse_lr = mse_lr**(1/2)\n",
    "\n",
    "# Print rmse_lr\n",
    "print('Linear Regression test set RMSE: {:.2f}'.format(rmse_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Bias-Variance Tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train_test_split from sklearn.model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set SEED for reproducibility\n",
    "SEED = 1\n",
    "\n",
    "# Split the data into 70% train and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=SEED)\n",
    "\n",
    "# Instantiate a DecisionTreeRegressor dt\n",
    "dt = DecisionTreeRegressor(max_depth = 4, min_samples_leaf = 0.26, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the 10-fold CV error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV RMSE: 5.14\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "# Compute the array containing the 10-folds CV MSEs\n",
    "MSE_CV_scores = - cross_val_score(dt, X_train, y_train, cv=10, \n",
    "                                  scoring='neg_mean_squared_error', \n",
    "                                  n_jobs=-1) \n",
    "\n",
    "# Compute the 10-folds CV RMSE\n",
    "RMSE_CV = (MSE_CV_scores.mean())**(1/2)\n",
    "\n",
    "# Print RMSE_CV\n",
    "print('CV RMSE: {:.2f}'.format(RMSE_CV))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the training error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE: 5.15\n"
     ]
    }
   ],
   "source": [
    "# Import mean_squared_error from sklearn.metrics as MSE\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Fit dt to the training set\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the training set\n",
    "y_pred_train = dt.predict(X_train)\n",
    "\n",
    "# Evaluate the training set RMSE of dt\n",
    "RMSE_train = (MSE(y_train, y_pred_train))**(0.5)\n",
    "\n",
    "# Print RMSE_train\n",
    "print('Train RMSE: {:.2f}'.format(RMSE_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age_std</th>\n",
       "      <th>Total_Bilirubin_std</th>\n",
       "      <th>Direct_Bilirubin_std</th>\n",
       "      <th>Alkaline_Phosphotase_std</th>\n",
       "      <th>Alamine_Aminotransferase_std</th>\n",
       "      <th>Aspartate_Aminotransferase_std</th>\n",
       "      <th>Total_Protiens_std</th>\n",
       "      <th>Albumin_std</th>\n",
       "      <th>Albumin_and_Globulin_Ratio_std</th>\n",
       "      <th>Is_male_std</th>\n",
       "      <th>Liver_disease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.247403</td>\n",
       "      <td>-0.420320</td>\n",
       "      <td>-0.495414</td>\n",
       "      <td>-0.428870</td>\n",
       "      <td>-0.355832</td>\n",
       "      <td>-0.319111</td>\n",
       "      <td>0.293722</td>\n",
       "      <td>0.203446</td>\n",
       "      <td>-0.147390</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.062306</td>\n",
       "      <td>1.218936</td>\n",
       "      <td>1.423518</td>\n",
       "      <td>1.675083</td>\n",
       "      <td>-0.093573</td>\n",
       "      <td>-0.035962</td>\n",
       "      <td>0.939655</td>\n",
       "      <td>0.077462</td>\n",
       "      <td>-0.648461</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.062306</td>\n",
       "      <td>0.640375</td>\n",
       "      <td>0.926017</td>\n",
       "      <td>0.816243</td>\n",
       "      <td>-0.115428</td>\n",
       "      <td>-0.146459</td>\n",
       "      <td>0.478274</td>\n",
       "      <td>0.203446</td>\n",
       "      <td>-0.178707</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.815511</td>\n",
       "      <td>-0.372106</td>\n",
       "      <td>-0.388807</td>\n",
       "      <td>-0.449416</td>\n",
       "      <td>-0.366760</td>\n",
       "      <td>-0.312205</td>\n",
       "      <td>0.293722</td>\n",
       "      <td>0.329431</td>\n",
       "      <td>0.165780</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.679294</td>\n",
       "      <td>0.093956</td>\n",
       "      <td>0.179766</td>\n",
       "      <td>-0.395996</td>\n",
       "      <td>-0.295731</td>\n",
       "      <td>-0.177537</td>\n",
       "      <td>0.755102</td>\n",
       "      <td>-0.930414</td>\n",
       "      <td>-1.713237</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Age_std  Total_Bilirubin_std  Direct_Bilirubin_std  \\\n",
       "0  1.247403            -0.420320             -0.495414   \n",
       "1  1.062306             1.218936              1.423518   \n",
       "2  1.062306             0.640375              0.926017   \n",
       "3  0.815511            -0.372106             -0.388807   \n",
       "4  1.679294             0.093956              0.179766   \n",
       "\n",
       "   Alkaline_Phosphotase_std  Alamine_Aminotransferase_std  \\\n",
       "0                 -0.428870                     -0.355832   \n",
       "1                  1.675083                     -0.093573   \n",
       "2                  0.816243                     -0.115428   \n",
       "3                 -0.449416                     -0.366760   \n",
       "4                 -0.395996                     -0.295731   \n",
       "\n",
       "   Aspartate_Aminotransferase_std  Total_Protiens_std  Albumin_std  \\\n",
       "0                       -0.319111            0.293722     0.203446   \n",
       "1                       -0.035962            0.939655     0.077462   \n",
       "2                       -0.146459            0.478274     0.203446   \n",
       "3                       -0.312205            0.293722     0.329431   \n",
       "4                       -0.177537            0.755102    -0.930414   \n",
       "\n",
       "   Albumin_and_Globulin_Ratio_std  Is_male_std  Liver_disease  \n",
       "0                       -0.147390            0              1  \n",
       "1                       -0.648461            1              1  \n",
       "2                       -0.178707            1              1  \n",
       "3                        0.165780            1              1  \n",
       "4                       -1.713237            1              1  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liver = pd.read_csv('indian_liver_patient/indian_liver_patient_preprocessed.csv', index_col = 0)\n",
    "X = liver.drop('Liver_disease', axis = 1)\n",
    "y = liver['Liver_disease']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=SEED)\n",
    "liver.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "# Set seed for reproducibility\n",
    "SEED=1\n",
    "\n",
    "# Instantiate lr\n",
    "lr = LogisticRegression(random_state=SEED, solver = 'liblinear')\n",
    "\n",
    "# Instantiate knn\n",
    "knn = KNN(n_neighbors=27)\n",
    "\n",
    "# Instantiate dt\n",
    "dt = DecisionTreeClassifier(min_samples_leaf=0.13, random_state=SEED)\n",
    "\n",
    "# Define the list classifiers\n",
    "classifiers = [('Logistic Regression', lr), ('K Nearest Neighbours', knn), ('Classification Tree', dt)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate individual classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression : 0.764\n",
      "K Nearest Neighbours : 0.701\n",
      "Classification Tree : 0.730\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the pre-defined list of classifiers\n",
    "for clf_name, clf in classifiers:    \n",
    " \n",
    "    # Fit clf to the training set\n",
    "    clf.fit(X_train, y_train)\n",
    "   \n",
    "    # Predict y_pred\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy =accuracy_score(y_test, y_pred)\n",
    "   \n",
    "    # Evaluate clf's accuracy on the test set\n",
    "    print('{:s} : {:.3f}'.format(clf_name, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Better performance with a Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Classifier: 0.770\n"
     ]
    }
   ],
   "source": [
    "# Import VotingClassifier from sklearn.ensemble\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Instantiate a VotingClassifier vc\n",
    "vc = VotingClassifier(estimators=classifiers)     \n",
    "\n",
    "# Fit vc to the training set\n",
    "vc.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the test set predictions\n",
    "y_pred = vc.predict(X_test)\n",
    "\n",
    "# Calculate accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Voting Classifier: {:.3f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging and Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the bagging classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Import BaggingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# Instantiate dt\n",
    "dt = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "# Instantiate bc\n",
    "bc = BaggingClassifier(base_estimator=dt, n_estimators=50, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Bagging performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy of bc: 0.70\n"
     ]
    }
   ],
   "source": [
    "# Fit bc to the training set\n",
    "bc.fit(X_train, y_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = bc.predict(X_test)\n",
    "\n",
    "# Evaluate acc_test\n",
    "acc_test = accuracy_score(y_test, y_pred)\n",
    "print('Test set accuracy of bc: {:.2f}'.format(acc_test)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the ground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Import BaggingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# Instantiate dt\n",
    "dt = DecisionTreeClassifier(min_samples_leaf=8, random_state=1)\n",
    "\n",
    "# Instantiate bc\n",
    "bc = BaggingClassifier(base_estimator=dt, \n",
    "            n_estimators=50,\n",
    "            oob_score=True,\n",
    "            random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OOB Score vs Test Set Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.707, OOB accuracy: 0.677\n"
     ]
    }
   ],
   "source": [
    "# Fit bc to the training set \n",
    "bc.fit(X_train, y_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = bc.predict(X_test)\n",
    "\n",
    "# Evaluate test set accuracy\n",
    "acc_test = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Evaluate OOB accuracy\n",
    "acc_oob = bc.oob_score_\n",
    "\n",
    "# Print acc_test and acc_oob\n",
    "print('Test set accuracy: {:.3f}, OOB accuracy: {:.3f}'.format(acc_test, acc_oob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train an RF regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hr</th>\n",
       "      <th>holiday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>temp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>cnt</th>\n",
       "      <th>instant</th>\n",
       "      <th>mnth</th>\n",
       "      <th>yr</th>\n",
       "      <th>Clear to partly cloudy</th>\n",
       "      <th>Light Precipitation</th>\n",
       "      <th>Misty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>149</td>\n",
       "      <td>13004</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.1343</td>\n",
       "      <td>93</td>\n",
       "      <td>13005</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.0896</td>\n",
       "      <td>90</td>\n",
       "      <td>13006</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.1343</td>\n",
       "      <td>33</td>\n",
       "      <td>13007</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.1940</td>\n",
       "      <td>4</td>\n",
       "      <td>13008</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   hr  holiday  workingday  temp   hum  windspeed  cnt  instant  mnth  yr  \\\n",
       "0   0        0           0  0.76  0.66     0.0000  149    13004     7   1   \n",
       "1   1        0           0  0.74  0.70     0.1343   93    13005     7   1   \n",
       "2   2        0           0  0.72  0.74     0.0896   90    13006     7   1   \n",
       "3   3        0           0  0.72  0.84     0.1343   33    13007     7   1   \n",
       "4   4        0           0  0.70  0.79     0.1940    4    13008     7   1   \n",
       "\n",
       "   Clear to partly cloudy  Light Precipitation  Misty  \n",
       "0                       1                    0      0  \n",
       "1                       1                    0      0  \n",
       "2                       1                    0      0  \n",
       "3                       1                    0      0  \n",
       "4                       1                    0      0  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bike = pd.read_csv('bikes.csv')\n",
    "X = bike[['hr', 'holiday', 'workingday', 'temp', 'hum', 'windspeed', 'instant',\n",
    "       'mnth', 'yr', 'Clear to partly cloudy', 'Light Precipitation', 'Misty']]\n",
    "y = bike['cnt']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)\n",
    "bike.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(n_estimators=25, random_state=2)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Instantiate rf\n",
    "rf = RandomForestRegressor(n_estimators=25,\n",
    "            random_state=2)\n",
    "            \n",
    "# Fit rf to the training set    \n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the RF regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set RMSE of rf: 51.84\n"
     ]
    }
   ],
   "source": [
    "# Import mean_squared_error as MSE\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Predict the test set labels\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Evaluate the test set RMSE\n",
    "rmse_test = (MSE(y_test, y_pred))**0.5\n",
    "\n",
    "# Print rmse_test\n",
    "print('Test set RMSE of rf: {:.2f}'.format(rmse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing features importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAc8AAAEICAYAAAA5lX8nAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAlE0lEQVR4nO3deZwdVZ3+8c9jYARsBCRBkcVWQBlACHJBRUBAxlFcQEVZHBFQMi4jOD/RQcdxUMcRRQejuAUGEUVBcUOUZQTCvnUgG4uCEAREkmBYgoCQPL8/6sRcbnq5lXTf251+3q/XffWpU6dOfas66W+fquo6sk1ERES07xndDiAiImKsSfKMiIioKckzIiKipiTPiIiImpI8IyIiakryjIiIqCnJMyIioqYkz4gukzRP0mOSFjd9nj8Mfe4zXDGuQhy9kixpjW7HAlBi2bLbccTYl+QZMTq8yXZP0+eP3QxmtCS74bK6HU90X5JnxCglaT1J/yvpPkn3SvovSRPKui0kXSzpAUkLJZ0haf2y7nvA5sAvyyj2Y5L2lHRPS/9/G51KOk7S2ZK+L+lh4LAh9r+lpEslPVT2f1abx3SapG9IOq/EdqWk50n6iqRFkm6VtGNLjB+XdHNZ/x1JazWtP1LS7ZL+LOmc5hF7GWV+UNJtwG2SLiurZpV9HyhpA0nnSlpQ+j9X0qZNfUyX9NkS5yOSLpQ0sWn9bpKukvSgpLslHVbqnynpS5L+IOl+Sd+StHZZN7Hs58ES9+WS8rN4jMk3LGL0+i7wFLAlsCPwWuC9ZZ2AzwPPB/4e2Aw4DsD2u4A/sHw0+8U297cfcDawPnDGEPv/LHAhsAGwKfC1Gsf1DuCTwETgCeBq4IayfDbwPy3t3wn8I7AF8OKyLZL2pjoH7wA2Bu4CzmzZdn/g5cA2tvcodTuU83IW1c/A7wAvoPqF4zHgpJY+DgEOBzYC/g44pux/c+C8cuyTgMnAzLLNF0qsk6nO3ybAp8q6jwD3lG2eC3wCyHtSxxrb+eSTTxc/wDxgMfBg+fyc6ofqE8DaTe0OBi4ZoI/9gRtb+tynaXlP4J5+9rtPKR8HXNa0btD9A6cD04BNhzi2XqrEsEZZPg04uWn9h4BbmpZfCjzYEuP7mpb3BX5fyv8LfLFpXQ/wJNBblg3s3RKPgS0HiXcysKhpeTrwyablDwDnl/LHgZ/104eAR4EtmupeCdxZyp8BfjFYHPmM/k/uA0SMDvvb/s2yBUm7AGsC90laVv0M4O6yfiPgq8DuwLpl3aJVjOHupvILBts/8DGq0ed1khYBX7Z9apv7ub+p/Fg/yz2DxHUX1Wib8vWGZStsL5b0ANUob14/265A0jrAicDrqEbRAOtKmmB7SVn+U9Mmf2mKbzPg9/10OwlYB5jRdO4ETCjlE6h+WbmwrJ9m+/jB4ozRJ8kzYnS6m2rkN9H2U/2s/zzVKGp72w9I2p+nX25svQz4KNUPdADKvctJLW2atxl0/7b/BBxZ+toN+I2ky2zf3sax1bVZU3lzYNnDVH+kSvKUOJ4FbAjc2xzqEH1/BHgJ8HLbf5I0GbiRKtkN5W5gl37qF1L9ErCt7XtbV9p+pOz3I5K2BS6RdL3ti9rYZ4wSuecZMQrZvo/qnuKXJT1b0jPKQ0KvLk3WpVzqlbQJ8NGWLu4HXtS0/DtgLUlvkLQm1X3DZ67s/iW9venBmkVUSWrJAN2tqg9K2lTSc6juDy57OOkHwOGSJkt6JvDfwLW25w3SV+t5WZcq0T1Y+v/PGnGdAewj6R2S1pC0oaTJtpcCJwMnlisESNpE0j+W8hvLA1cCHqY6byN17mKEJHlGjF6HUj2gcjNVgjqb6sEYgE8DLwMeAn4F/LRl288DnyxPdB5j+yGq+3WnUI3MHqV6aGVl978zcK2kxcA5wNG271zJ4xzKD6gS+R3l818AZaT2H8BPgPuoHig6aIi+jgO+W87LO4CvAGtTjRavAc5vNyjbf6C6B/sR4M9UDwvtUFb/G3A7cE15evk3VCNcgK3K8mKqh6W+YXt6u/uN0UF2HvKKiNFJ0jzgvc33gyNGg4w8IyIiakryjIiIqCmXbSMiImrKyDMiIqKm/J3nODFx4kT39vZ2O4yIiDFlxowZC223/k10kud40dvbS19fX7fDiIgYUyTd1V99LttGRETUlOQZERFRU5JnRERETbnnOU7MXzKfqYumdjuMiIiOOnqDo0ek34w8xwBJvZLmdjuOiIioJHmuJiTlKkJERIckeY4dEySdLOkmSRdKWlvSdEn/LelSYGSuTURExAqSPMeOrYCv294WeBB4W6lf3/arbX+5dQNJUyT1SepbvHBxB0ONiFi9JXmOHXfanlnKM4DeUj6r39aA7Wm2G7YbPRN7Rji8iIjxI8lz7HiiqbyE5U9KP9qFWCIixrUkz4iIiJqSPCMiImrKfJ7jRKPRcF4MHxFRj6QZthut9Rl5RkRE1JTkGRERUVOSZ0RERE1JnhERETUleUZERNSU5BkREVFTkmdERERNSZ4RERE1ZQ7IcWL+kvlMXTS122GMCiM1s3xEjB8ZedYgaZ6kif3UXzXS+4iIiNEjybNNkiYMtM72rp2MJSIiumtcJE9JH5N0VCmfKOniUn6NpO9LOljSHElzJX2habvFkj4j6VrglU31a0s6X9KRy9qVr3tKmi7pbEm3SjpDksq6fUvdFZK+KuncUr+hpAsl3Sjp24Ca9vNzSTMk3SRpSql7j6QTm9ocKel/Ru7sRUREq3GRPIHLgN1LuQH0SFoT2A24DfgCsDcwGdhZ0v6l7bOAubZfbvuKUtcD/BL4ge2T+9nXjsCHgW2AFwGvkrQW8G3g9bZ3AyY1tf9P4ArbOwLnAJs3rTvC9k4l5qMkbQicCby5xA9wOPCdeqcjIiJWxXhJnjOAnSStSzWp9NVUCWl34EFguu0Ftp8CzgD2KNstAX7S0tcvgO/YPn2AfV1n+x7bS4GZQC+wNXCH7TtLmx82td8D+D6A7V8Bi5rWHSVpFnANsBmwle1HgYuBN0raGljT9pz+ApE0RVKfpL7FCxcPEG5ERNQ1LpKn7SeBeVSjtKuAy4G9gC2APwyy6eO2l7TUXQm8ftnl2H480VReQvVE80Bt/xZia4WkPYF9gFfa3gG4EVirrD4FOIwhRp22p9lu2G70TOwZIoSIiGjXuEiexWXAMeXr5cD7qEaG1wCvljSxPBR0MHDpIP18CngA+EaNfd8KvEhSb1k+sCWudwJIej2wQalfD1hk+y9lhPmKZRvYvpZqJHoITx/FRkREB4yn5Hk5sDFwte37gceBy23fB3wcuASYBdxg+xdD9PVhYC1JX2xnx7YfAz4AnC/pCuB+4KGy+tPAHpJuAF7L8pHw+cAakmYDn6VK8s1+BFxpexEREdFRsle4YhgjQFKP7cXlcu/XgdtsnzjUdoP0dy5wou2L2mnfaDTc19e3sruLiBiXJM2w3WitH08jz247UtJM4CaqS7LfXplOJK0v6XfAY+0mzoiIGF55PV+HlFHmSo80m/p5EHjxKgcUERErLSPPiIiImpI8IyIiakryjIiIqCnJMyIioqYkz4iIiJqSPCMiImrKn6qME/OXzGfqoqlDtjt6g6M7EE1ExNiWkWcXSOqVNLfbcURExMpJ8oyIiKgpybN7Jkg6WdJNki6UtLak6ZIaAGWWl3mlfJikn0v6paQ7Jf2LpP8n6UZJ10h6TlePJCJinEny7J6tgK/b3pZqQu63DdF+O6opyHYBPgf8xfaOVBN7HzqCcUZERIskz+650/bMUp4B9A7R/hLbj9heQDWd2S9L/ZyBtpU0RVKfpL7FCxevesQREQEkeXbTE03lJVRPPj/F8u/JWoO0X9q0vJQBnpq2Pc12w3ajZ2LPqkccERFAkudoMw/YqZQP6GIcERExiCTP0eVLwPslXQVM7HYwERHRP9nudgzRAY1Gw319fd0OIyJiTJE0w3ajtT4jz4iIiJqSPCMiImpK8oyIiKgpyTMiIqKmJM+IiIiakjwjIiJqSvKMiIioKckzIiKipn7fiRqrn/lL5jN10dRB2xy9wdEdiiYiYmzLyDMiIqKmJM9hUN5FuzLb7S9pm1XYb6+kQ1Z2+4iIWDlJnsPA9q4ruen+wEonT6p5PJM8IyI6LMlzGEhaXL7uKWm6pLMl3SrpDEkq646XdLOk2ZK+JGlX4M3ACZJmStpC0pGSrpc0S9JPJK1Ttj1N0lclXSXpDknLpis7Hti9bP+v3Tj2iIjxKA8MDb8dgW2BPwJXAq+SdDPwFmBr25a0vu0HJZ0DnGv7bABJD9o+uZT/C3gP8LXS78bAbsDWwDnA2cCxwDG239hfIJKmAFMANth0gxE52IiI8Sgjz+F3ne17bC8FZlJdWn0YeBw4RdJbgb8MsO12ki6XNAd4J1USXubntpfavhl4bjuB2J5mu2G70TOxZyUPJyIiWiV5Dr8nmspLgDVsPwXsAvyE6j7n+QNsexrwL7ZfCnwaWGuAfjVcwUZERH25bNsBknqAdWz/WtI1wO1l1SPAuk1N1wXuk7Qm1cjz3iG6bt0+IiI6IMmzM9YFfiFpLapR47KHe84ETpZ0FHAA8B/AtcBdwByGToyzgackzQJOs33iQA03mrBRXoIQETFMZLvbMUQHNBoN9/X1dTuMiIgxRdIM243W+tzzjIiIqCnJMyIioqYkz4iIiJqSPCMiImpK8oyIiKgpyTMiIqKmJM+IiIia8pKEcWL+kvlMXTR10DZ5iUJERHsy8oyIiKgpybMDJK0v6QPdjiMiIoZHkmdnrA8keUZErCaSPDvjeGALSTMlnSDpo5KulzRb0qcBJPVKulXSKZLmSjpD0j6SrpR0m6RdSrvjJH1P0sWl/siuHllExDiU5NkZxwK/tz0Z+D9gK6r5PScDO0nao7TbEpgKbA9sDRwC7AYcA3yiqb/tgTcArwQ+Jen5/e1U0hRJfZL6Fi9cPNzHFBExbiV5dt5ry+dG4AaqJLlVWXen7Tm2lwI3ARe5mvZmDtDb1McvbD9meyFwCVUiXoHtabYbths9E3tG5mgiIsah/KlK5wn4vO1vP61S6gWeaKpa2rS8lKd/r1rnkcu8chERHZSRZ2c8wvKJrS8AjpDUAyBpE0kb1exvP0lrSdoQ2BO4ftgijYiIIWXk2QG2HygP/swFzgN+AFwtCWAx8E/AkhpdXgf8Ctgc+KztPw61wUYTNspLECIihkmSZ4fYPqSlqr/X/WzX1P6wpvK85nXA72xPGc74IiKifblsGxERUVNGnmOM7eO6HUNExHiXkWdERERNSZ4RERE1JXlGRETUlOQZERFRU5JnRERETUme48T8JfOZumgqUxf19+elERFRR5JnRERETUme/ZD0a0nr12jfW16913GSMtdYRESH5SUJ/bC9b7djiIiI0WtcjjwlfUzSUaV8oqSLS/k1kr4vaZ6kiWVEeYukkyXdJOlCSWuXtjtJmiXpauCDTX1vK+k6STMlzZa0VennVknfLXVnS1qnqZ9LJc2QdIGkjUv9FpLOL/WXS9q61L9Q0tWSrpf02Q6fuoiIYJwmT+AyYPdSbgA9ktYEdgMub2m7FfB129sCDwJvK/XfAY6y/cqW9u8DptqeXPq+p9S/BJhme3vgYeADZZ9fAw6wvRNwKvC50n4a8KFSfwzwjVI/Ffim7Z2BPw12kJKmSOqT1Ld4Ya7uRkQMl/GaPGcAO0lal2rC6aupEt3urJg877Q9s2m7XknrAevbvrTUf6+p/dXAJyT9G/AC24+V+rttX1nK36dK1C+hmi3l/yTNBD4JbFrm+twV+HGp/zawcdn2VcAP+9nvCmxPs92w3eiZ2DNY04iIqGFc3vO0/aSkecDhwFXAbGAvYAvglpbmTzSVlwBrAwI8QN8/kHQt8AbgAknvBe7op71LPze1jl4lPRt4sIxe+93NYMcXEREja7yOPKG6dHtM+Xo51eXWmbaHTEy2HwQekrRbqXrnsnWSXgTcYfurwDnA9mXV5pKWJcmDgSuA3wKTltVLWlPStrYfBu6U9PZSL0k7lG2vBA5q3W9ERHTOeE6el1NdCr3a9v3A46x4yXYwhwNfLw8MPdZUfyAwt1xu3Ro4vdTfArxb0mzgOVT3Lf8KHAB8QdIsYCbV5VqoEuN7Sv1NwH6l/mjgg5KuB9arEW9ERAwTtTHQilUkqRc41/Z23Yqh0Wi4r6+vW7uPiBiTJM2w3WitH88jz4iIiJUyLh8Y6jTb86ieqo2IiNVARp4RERE1JXlGRETUlOQZERFRU5JnRERETUmeERERNSV5RkRE1JTkOU7MXzK/2yFERKw2OpI8Ja0wH5ak90k6dIjtDpN00gDrPjHIdvMkzSnzbV4o6Xn1o16peN8s6dhS3l/SNm30+7R2kj4jaZ9VjTciIkZO10aetr9l+/ShWw5owORZ7GV7B6CvtW150XqtY28nXtvn2D6+LO4PDJk8W9vZ/pTt39SJLSIiOqtryVPScZKOKeWdJc2WdLWkEyTNbWr6fEnnS7pN0hdL++OBtSXNlHTGELu6DNhSUq+kWyR9A7gB2EzSRyVdX/b96abYDi11syR9r594p0v6iqSrJM2VtEupP0zSSZJ2Bd4MnFBi3ELSkWVfsyT9RNI6A7Q7TdIBpb/XSLqxjKJPlfTMUj9P0qcl3VDWbb2q34+IiGjfaLnn+R3gfWVeyyUt6yZTzVTyUuBASZvZPhZ4zPZk20NNy/VGYE4pvwQ43faOpbwVsEvZx06S9pC0LfDvwN5l5Hr0AP0+y/auwAeAU5tX2L6Kajqyj5YYfw/81PbOpc9bgPcM0A4ASWsBpwEH2n4p1asU39+0m4W2XwZ8k2pqtRVImiKpT1Lf4oUrXDmPiIiV1PXkKWl9YN2SSAB+0NLkItsP2X4cuBl4QZtdX1KmBXs28PlSd5fta0r5teVzI9VIdGuqZLo3cLbthQC2/zxA/z8s6y8Dnl2OYzDbSbpc0hyq6ca2HaL9S4A7bf+uLH8X2KNp/U/L1xlAb38d2J5mu2G70TOxZ4jdRUREu0bDi+E1xPonmspLaD/mvZYlQPhbkn60Zb+ft/3tpwUjHQW0M09ba5uhtjkN2N/2LEmHAXsO0b7d81LnnERExDDo+sjT9iLgEUmvKFUHtbnpk5LWXIVdXwAcIakHQNImkjYCLgLeIWnDUv+cAbY/sKzfDXjI9kMt6x8B1m1aXhe4r8T8zkHaLXMr0Ctpy7L8LuDSdg8uIiJGTqdGLOtIuqdp+X9a1r8HOFnSo8B0oDUR9WcaMFvSDW3c91yB7Qsl/T1wtSSAxcA/2b5J0ueASyUtobqse1g/XSySdBXVZeEj+ll/Zjmmo4ADgP8ArgXuoroHu+4A7ZbF97ikw4EfS1oDuB74Vt3jjIiI4Se7nSuUIxyE1GN7cSkfC2xse6AHdbpO0nTgGNt93Y6lXY1Gw319YybciIhRQdIM243W+tFyr+wNkj5OFc9d9D/Si4iIGBVGRfK0fRZwVrfjaJftPbsdQ0REdE/XHxiKiIgYa5I8IyIiakryjIiIqCnJMyIioqYkz4iIiJqSPCMiImpK8hwn5i+Z3+0QIiJWG0meq6DMETp36JZ/a988V+cpklaYLHvZnKDDGWdERAyvUfGShPHI9nu7HUNERKycjDxX3QRJJ0u6SdKFktaWNFnSNZJmS/qZpA1aN5I0XVKjlA+X9DtJlwKvamrzJknXSrpR0m8kPVfSMyTdJmlSafMMSbdLmtixI46IGOeSPFfdVsDXbW8LPAi8DTgd+Dfb21PNoPKfA20saWPg01RJ8x+A5ku5VwCvsL0j1ewrH7O9FPg+y6c12weY1Tx3aVPfUyT1SepbvHDxqh1lRET8TZLnqrvT9sxSngFsAaxve9ncm98F9hhk+5cD020vsP1Xnv6O302BCyTNAT4KbFvqTwUOLeUjgO/017HtabYbths9E3tqHlZERAwkyXPVPdFUXgKsvxJ9DDQv3NeAk2y/FPhnYC0A23cD90vamyr5nrcS+4yIiJWU5Dn8HqKaKHv3svwu4NJB2l8L7ClpQ0lrAm9vWrcecG8pv7tlu1OoLt/+yPaSVQ87IiLaladtR8a7gW9JWge4Azh8oIa275N0HHA1cB9wAzChrD4O+LGke4FrgBc2bXoO1eXafi/ZRkTEyJE90BXDGM3Kk7on2t59yMZAo9FwX1/fCEcVEbF6kTTDdqO1PiPPMUjSscD7Wf7EbUREdFDueY5Bto+3/QLbV3Q7loiI8SjJMyIioqYkz4iIiJqSPCMiImpK8oyIiKgpyTMiIqKmJM+IiIiakjwjIiJqGjJ5SnqepDMl/V7SzZJ+LenFknolzR2JoCR9uLzarmPKHJz7Ni0fJumkYeh3WOYCk7SnpHOHo6+IiFg1gyZPSQJ+RjVl1ha2twE+ATx3uAJQpTWODwMdS56S1gAmA/sO0TQiImLIkedewJO2v7WswvZM25c3N5I0QdIJkq6XNFvSP5f6HkkXSbpB0hxJ+5X6Xkm3SPoG1YvQN2vq6yjg+cAlki4pdQeX7edK+kJ/gUqaJ+kLkq4rny1L/ZskXSvpRkm/kfTcUn+cpGmSLqSavPozwIGSZko6sKnfdSXdWWY8QdKzy77WbNn/cyX9TNKs8tm1Zb3KOZpbjuXAUv+0EaWkkyQdVsqvk3SrpCuAt5a6Z0i6TdKkpuXbJU0c7BsZERHDZ6jkuR3VBM9DeQ/wkO2dgZ2BIyW9EHgceIvtl1El4i+X0SzAS4DTbe9o+65lHdn+KvBHYC/be0l6PvAFYG+q0eHOkvYfII6Hbe8CnAR8pdRdAbzC9o7AmcDHmtrvBOxn+xDgU8BZtifb/tuE1LYfAaYDbyhVBwE/sf1ky76/ClxqewfgZcBNLevfWuLfAdgHOEHSxgMcB5LWAk4G3gTsDjyvxLOUaiqyZe+13QeYZXthP31MkdQnqW/BggUD7SoiImoargeGXgscKmkm1fyUGwJbAQL+W9Js4DfAJiy/5HuX7Wva6HtnqsvGC2w/BZwB7DFA2x82fX1lKW8KXCBpDvBRYNum9ufYfqyNGE5h+bRih9P/NGB7A98EsL3E9kMt63cDfljW3U81x+fOg+xza+BO27e5mvrm+03rTgUOLeUjBogH29NsN2w3Jk2aNMiuIiKijqGS501Uo7OhCPhQGbVNtv1C2xdSjY4mATvZngzcD6xVtnm0zRg1dJO/cT/lrwEn2X4p8M9N+287BttXAr2SXg1MsL0yD0oNdBxP8fTvQ3N8/c4XZ/tu4H5JewMvB85biXgiImIlDZU8LwaeKenIZRWSdi5JpNkFwPub7gu+WNKzgPWA+baflLQX8II243oEWLeUrwVeLWmipAnAwVSjtv4c2PT16lJeD7i3lN/d5j77czrViHagyacvopombNk94Ge3rL+M6p7qhHK/cg/gOuAuYBtJz5S0HvCa0v5W4IWStijLB7f0dwrVaPRHtpcMEndERAyzQZNnuVz4FuAfyp+q3AQcR3VPstkpwM3ADeXPV75NNVfoGUBDUh/VKPTWNuOaBpwn6RLb9wEfBy4BZgE32P7FANs9U9K1wNHAv5a644AfS7ocWOG+YJNLqJLY0x4YanIGsAHLLw23OhrYq1wensHTLw9D9dTy7HIMFwMfs/2nMor8UVl3BnAjgO3HgSnAr8oDQ3e19HcO0MPAyTwiIkaIqvw49kmaBzT6e3BmmPo/gOrhoneNRP91SWoAJ9revZ32jUbDfX19IxxVRMTqRdIM243W+jW6EcxYI+lrwOsZJX8HKulYqkvE7xyqbUREDL/VJnna7h3Bvj80Un2vDNvHA8d3O46IiPEq77aNiIioKckzIiKipiTPiIiImpI8IyIiakryjIiIqCnJMyIioqYkz4iIiJqSPDtAkiV9r2l5DUkLls3jKenN5cUHA20/WdKoeEFDREQkeXbKo8B2ktYuy//A8pfVY/uc8uKDgUxmlLzdKCIikjw76TyWT6h9ME0vmJd0mKSTSvntkuZKmiXpMkl/B3yGakaWmZIOlHRbmZkFSc+QdLukiR0+noiIcSvJs3POBA6StBawPdVUa/35FPCPtncA3mz7r6XurDJX6llUU5Ete6/tPsCskXohfkRErCjJs0NszwZ6qUadvx6k6ZXAaWUO1QkDtDkVOLSUj2CAackkTZHUJ6lvwYIFKxV3RESsKMmzs84BvsTAc4Ji+33AJ4HNgJmSNuynzd3A/ZL2Bl5OdUm4v76m2W7YbkyaNGk44o+ICFajWVXGiFOBh2zPkbRnfw0kbWH7WuBaSW+iSqKPAOu2ND2F6vLt92wvGbmQIyKiVUaeHWT7HttTh2h2gqQ5kuYClwGzgEuAbZY9MFTanQP0MMAl24iIGDkZeXaA7Z5+6qYD00v5NOC0Un5rP138Gdi5pW4HqgeFbh2+SCMioh1JnmNQeaHC+1n+xG1ERHRQLtuOQbaPt/0C21d0O5aIiPEoyTMiIqKmJM+IiIiakjwjIiJqSvKMiIioKckzIiKipiTPiIiImpI8IyIiakryHOUkrS/pA03Le0o6t5sxRUSMd0meo9/6wAeGahQREZ2T5NkBknol3SrpFElzJZ0haR9JV0q6TdIuko6TdKqk6ZLukHRU2fx4YIvyUvgTSl2PpLNLn2dIUpcOLSJiXMq7bTtnS+DtwBTgeuAQYDfgzcAngJnA1sBeVNOP/VbSN4Fjge1sT4bqsi2wI7At8EeqybNfBeRVfRERHZKRZ+fcaXuO7aXATcBFtg3MAXpLm1/ZfsL2QmA+8NwB+rquTG+2lCrp9vbXSNIUSX2S+hYsWDCMhxIRMb4leXbOE03lpU3LS1l+BaC5zRIGvjLQVjvb02w3bDcmTZpUP+KIiOhXkufo9wjVZdyIiBglkjxHOdsPAFeWB41OGHKDiIgYcapuu8XqrtFouK+vr9thRESMKZJm2G601mfkGRERUVOSZ0RERE1JnhERETUleUZERNSU5BkREVFTkmdERERNSZ4RERE1JXlGRETUlOQZERFRU5JnRERETUmeERERNSV5rgYkTeh2DBER48lA80XGKCLps8BC21PL8ueA+4G3APcBk4FtuhZgRMQ4k5Hn2PC/wLsBJD0DOAi4F9gF+Hfb/SZOSVMk9UnqW7BgQceCjYhY3SV5jgG25wEPSNoReC1wI/AAcJ3tOwfZbprthu3GpEmTOhNsRMQ4kMu2Y8cpwGHA84BTS92jXYsmImIcy8hz7PgZ8DpgZ+CCLscSETGuZeQ5Rtj+q6RLgAdtL5HU7ZAiIsatJM8xojwo9Arg7QC2pwPTuxhSRMS4lcu2Y4CkbYDbgYts39bteCIixruMPMcA2zcDL+p2HBERUcnIMyIioibZ7nYM0QGSHgF+2+042jARWNjtINqQOIfXWIkTxk6siXN4vMD2Cn8on8u248dvbTe6HcRQJPUlzuGTOIffWIk1cY6sXLaNiIioKckzIiKipiTP8WNatwNoU+IcXolz+I2VWBPnCMoDQxERETVl5BkREVFTkmdERERNSZ6rEUmvk/RbSbdLOraf9ZL01bJ+tqSXdSPOEstQsW4t6WpJT0g6phsxljiGivOd5VzOlnSVpB1GaZz7lRhnlgnSdxuNcTa121nSEkkHdDK+pv0PdT73lPRQOZ8zJX2qG3GWWIY8pyXemZJuknRpp2MsMQx1Tj/adD7nlu//c7oRa1ts57MafIAJwO+pXuP3d8AsYJuWNvsC5wGiesn8taM41o2opl/7HHDMKI5zV2CDUn59N85pm3H2sPwZh+2BW0djnE3tLgZ+DRwwGuME9gTO7ca/y5WIdX3gZmDzsrzRaIyzpf2bgIu7fX4H+2TkufrYBbjd9h22/wqcCezX0mY/4HRXrgHWl7RxpwOljVhtz7d9PfBkF+Jbpp04r7K9qCxeA2za4RihvTgXu/xUAp4FdONJwXb+jQJ8CPgJML+TwTVpN87RoJ1YDwF+avsPUP3f6nCMUP+cHgz8sCORraQkz9XHJsDdTcv3lLq6bTphtMQxlLpxvodqZN9pbcUp6S2SbgV+BRzRodiaDRmnpE2AtwDf6mBcrdr9vr9S0ixJ50natjOhraCdWF8MbCBpuqQZkg7tWHTLtf1/SdI6wOuofoEatfJ6vtVHf7Njt44u2mnTCaMljqG0HaekvaiSZzfuJbYVp+2fAT+TtAfwWWCfkQ6sRTtxfgX4N3d3wvd24ryB6p2niyXtC/wc2GqkA+tHO7GuAewEvAZYG7ha0jW2fzfSwTWp83/+TcCVtv88gvGssiTP1cc9wGZNy5sCf1yJNp0wWuIYSltxStoeOAV4ve0HOhRbs1rn0/ZlkraQNNF2J1/I3U6cDeDMkjgnAvtKesr2zzsSYWXIOG0/3FT+taRvdOF8Qvv/7xfafhR4VNJlwA5AJ5NnnX+jBzHKL9kCeWBodflQ/SJ0B/BClt+Q37alzRt4+gND143WWJvaHkf3Hhhq55xuTjVR+a6j/Hu/JcsfGHoZcO+y5dEUZ0v70+jOA0PtnM/nNZ3PXYA/dPp81oj174GLStt1gLnAdqMtztJuPeDPwLM6fS7rfjLyXE3YfkrSvwAXUD3ZdqrtmyS9r6z/FtXTi/tS/bD/C3D4aI1V0vOAPuDZwFJJH6Z6Ou/hgfrtRpzAp4ANgW+U0dJT7vAMEW3G+TbgUElPAo8BB7r8tBplcXZdm3EeALxf0lNU5/OgTp/PdmO1fYuk84HZwFLgFNtzR1ucpelbgAtdjZJHtbyeLyIioqY8bRsREVFTkmdERERNSZ4RERE1JXlGRETUlOQZERFRU5JnRERETUmeERERNf1/GAmJ+ZKCN70AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a pd.Series of features importances\n",
    "importances = pd.Series(data=rf.feature_importances_,\n",
    "                        index= X_train.columns)\n",
    "\n",
    "# Sort importances\n",
    "importances_sorted = importances.sort_values()\n",
    "\n",
    "# Draw a horizontal barplot of importances_sorted\n",
    "importances_sorted.plot(kind = 'barh', color = 'lightgreen')\n",
    "plt.title('Features Importances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the AdaBoost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Import AdaBoostClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Instantiate dt\n",
    "dt = DecisionTreeClassifier(max_depth = 2, random_state=1)\n",
    "\n",
    "# Instantiate ada\n",
    "ada = AdaBoostClassifier(base_estimator=dt, n_estimators=180, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the AdaBoost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = liver.drop('Liver_disease', axis = 1)\n",
    "y = liver['Liver_disease']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit ada to the training set\n",
    "ada.fit(X_train, y_train)\n",
    "\n",
    "# Compute the probabilities of obtaining the positive class\n",
    "y_pred_proba = ada.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the AdaBoost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC score: 0.63\n"
     ]
    }
   ],
   "source": [
    "# Import roc_auc_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Evaluate test-set roc_auc_score\n",
    "ada_roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# Print roc_auc_score\n",
    "print('ROC AUC score: {:.2f}'.format(ada_roc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the GB regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import GradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor \n",
    "\n",
    "# Instantiate gb\n",
    "gb = GradientBoostingRegressor(max_depth = 4, \n",
    "            n_estimators = 200,\n",
    "            random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the GB regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = bike[['hr', 'holiday', 'workingday', 'temp', 'hum', 'windspeed', 'instant',\n",
    "       'mnth', 'yr', 'Clear to partly cloudy', 'Light Precipitation', 'Misty']]\n",
    "y = bike['cnt']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit gb to the training set\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = gb.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the GB regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set RMSE of gb: 43.113\n"
     ]
    }
   ],
   "source": [
    "# Import mean_squared_error as MSE\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Compute MSE\n",
    "mse_test = MSE(y_test, y_pred)\n",
    "\n",
    "# Compute RMSE\n",
    "rmse_test = mse_test ** 0.5\n",
    "\n",
    "# Print RMSE\n",
    "print('Test set RMSE of gb: {:.3f}'.format(rmse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression with SGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import GradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Instantiate sgbr\n",
    "sgbr = GradientBoostingRegressor(max_depth=4, \n",
    "            subsample=0.9,\n",
    "            max_features=0.75,\n",
    "            n_estimators=200,                                \n",
    "            random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the SGB regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit sgbr to the training set\n",
    "sgbr.fit(X_train, y_train)\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = sgbr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the SGB regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set RMSE of sgbr: 45.143\n"
     ]
    }
   ],
   "source": [
    "# Import mean_squared_error as MSE\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Compute test set MSE\n",
    "mse_test = MSE(y_test, y_pred)\n",
    "\n",
    "# Compute test set RMSE\n",
    "rmse_test = mse_test ** 0.5\n",
    "\n",
    "# Print rmse_test\n",
    "print('Test set RMSE of sgbr: {:.3f}'.format(rmse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the tree's hyperparameter grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define params_dt\n",
    "params_dt = {'max_depth': [2, 3, 4], 'min_samples_leaf': [0.12, 0.14, 0.16, 0.18]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search for the optimal tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Instantiate grid_dt\n",
    "grid_dt = GridSearchCV(estimator=dt,\n",
    "                       param_grid=params_dt,\n",
    "                       scoring='roc_auc',\n",
    "                       cv=5,\n",
    "                       n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the optimal tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=DecisionTreeClassifier(max_depth=2, random_state=1),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'max_depth': [2, 3, 4],\n",
       "                         'min_samples_leaf': [0.12, 0.14, 0.16, 0.18]},\n",
       "             scoring='roc_auc')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = liver.drop('Liver_disease', axis = 1)\n",
    "y = liver['Liver_disease']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)\n",
    "grid_dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set ROC AUC score: 0.731\n"
     ]
    }
   ],
   "source": [
    "# Import roc_auc_score from sklearn.metrics \n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Extract the best estimator\n",
    "best_model = grid_dt.best_estimator_\n",
    "\n",
    "# Predict the test set probabilities of the positive class\n",
    "y_pred_proba = best_model.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Compute test_roc_auc\n",
    "test_roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# Print test_roc_auc\n",
    "print('Test set ROC AUC score: {:.3f}'.format(test_roc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the hyperparameter grid of RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dictionary 'params_rf'\n",
    "params_rf = {'n_estimators': [100, 350, 500], 'max_features': ['log2', 'auto', 'sqrt'], 'min_samples_leaf': [2, 10, 30]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search for the optimal forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
    "           max_features='auto', max_leaf_nodes=None,\n",
    "#           min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "           min_impurity_decrease=0.0, \n",
    "           min_samples_leaf=1, min_samples_split=2,\n",
    "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=-1,\n",
    "           oob_score=False, random_state=2, verbose=0, warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import GridSearchCV\n",
    "from sklearn.model_selection import  GridSearchCV\n",
    "\n",
    "# Instantiate grid_rf\n",
    "grid_rf = GridSearchCV(estimator=rf,\n",
    "                       param_grid=params_rf,\n",
    "                       scoring='neg_mean_squared_error',\n",
    "                       cv=3,\n",
    "                       verbose=1,\n",
    "                       n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SEGES\\anaconda3\\envs\\Data_Science\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:396: FutureWarning: Criterion 'mse' was deprecated in v1.0 and will be removed in version 1.2. Use `criterion='squared_error'` which is equivalent.\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=RandomForestRegressor(criterion='mse', n_estimators=10,\n",
       "                                             n_jobs=-1, random_state=2),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'max_features': ['log2', 'auto', 'sqrt'],\n",
       "                         'min_samples_leaf': [2, 10, 30],\n",
       "                         'n_estimators': [100, 350, 500]},\n",
       "             scoring='neg_mean_squared_error', verbose=1)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = bike[['hr', 'holiday', 'workingday', 'temp', 'hum', 'windspeed', 'instant',\n",
    "       'mnth', 'yr', 'Clear to partly cloudy', 'Light Precipitation', 'Misty']]\n",
    "y = bike['cnt']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)\n",
    "grid_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the optimal forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE of best model: 51.755\n"
     ]
    }
   ],
   "source": [
    "# Import mean_squared_error from sklearn.metrics as MSE \n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Extract the best estimator\n",
    "best_model = grid_rf.best_estimator_\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Compute rmse_test\n",
    "rmse_test = MSE(y_test, y_pred)**0.5\n",
    "\n",
    "# Print rmse_test\n",
    "print('Test RMSE of best model: {:.3f}'.format(rmse_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
