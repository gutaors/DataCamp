{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# O que o caderno faz, em linhas gerais?\n",
    "### Roteiro\n",
    "\n",
    "#### corre o olho em tudo\n",
    "#### corre a tela para ver o markdown e não ver o código\n",
    "#### Lê o markdown\n",
    "#### mentaliza como seria o código\n",
    "#### confere\n",
    "#### que dados ele treina?\n",
    "#### o que ele joga pro dataset de submissão?\n",
    "#### Reescreva a célula sem olhar, mesmo que fique errado\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Kaggle competitions process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "como eu exploraria? le csv, faz shape e head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (15500, 5)\n",
      "       id        date  store  item  sales\n",
      "0  100000  2017-12-01      1     1     19\n",
      "1  100001  2017-12-02      1     1     16\n",
      "2  100002  2017-12-03      1     1     31\n",
      "3  100003  2017-12-04      1     1      7\n",
      "4  100004  2017-12-05      1     1     20\n"
     ]
    }
   ],
   "source": [
    "# Read train data\n",
    "train = pd.read_csv('demand_forecasting_train_1_month.csv')\n",
    "\n",
    "# Look at the shape of the data\n",
    "# isto nos dá dicas sobre tamanhgo do dataset, quais ferramentas podem resolver \n",
    "# e de quanto processamento que vamos precisar para resolver o problema\n",
    "\n",
    "print('Train shape:', train.shape)\n",
    "\n",
    "# Look at the head of the data\n",
    "print(train.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15500 entries, 0 to 15499\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      15500 non-null  int64 \n",
      " 1   date    15500 non-null  object\n",
      " 2   store   15500 non-null  int64 \n",
      " 3   item    15500 non-null  int64 \n",
      " 4   sales   15500 non-null  int64 \n",
      "dtypes: int64(4), object(1)\n",
      "memory usage: 605.6+ KB\n"
     ]
    }
   ],
   "source": [
    "#tem também o info que eu vou colocar por minha conta\n",
    "\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7    1550\n",
       "6    1550\n",
       "5    1550\n",
       "4    1550\n",
       "3    1550\n",
       "Name: store, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#quais as cinco lojas que tem mais vendas?\n",
    "train['store'].value_counts().head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#quantas lojas temos ao todo?\n",
    "train['store'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22     295\n",
       "31     288\n",
       "29     284\n",
       "24     281\n",
       "21     279\n",
       "      ... \n",
       "3        2\n",
       "118      2\n",
       "123      2\n",
       "128      1\n",
       "124      1\n",
       "Name: sales, Length: 123, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#quantas vendas temos?\n",
    "train['sales'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='sales', ylabel='count'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# um gráfico das vendas ( tudo aqui eu trouxe do dataset 911 lá do bootcamp udemy)\n",
    "import seaborn as sns\n",
    "sns.countplot(x='sales',data=train,palette='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "o que é importante observar no teste em relação ao número de colunas comparado ao train? \n",
    "uma coluna a menos pois não tem o y \n",
    "\n",
    "qual comando lista as colunas?\n",
    "\n",
    "a pergunta já dá o spoiler, columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train columns: ['id', 'date', 'store', 'item', 'sales']\n",
      "Test columns: ['id', 'date', 'store', 'item']\n"
     ]
    }
   ],
   "source": [
    "#o teste é simplesmente o treino sem a coluna y\n",
    "import pandas as pd\n",
    "\n",
    "# Read test data\n",
    "test = pd.read_csv('demand_forecasting_test.csv')\n",
    "\n",
    "# Print train and test columns\n",
    "# note que o test tem as mesmas colunas do train só tirando o sales que é o que vamos prever\n",
    "\n",
    "print('Train columns:', train.columns.tolist())\n",
    "print('Test columns:', test.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a simple model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "depois de ver os formatos dos dados o que eu faria pra dar uma explorada?\n",
    "\n",
    "um modelo simples para ver se depois estarei fazendo progresso nos próximos modelos\n",
    "\n",
    "qual comando eu uso?\n",
    "\n",
    "randomforestRegressor\n",
    "\n",
    "como sei que é regressor? \n",
    "aqui não mostra mas rodou um histograma e verificou se y é uma variável contínua\n",
    "\n",
    "quais parâmetros?\n",
    "x=train[['','']], y=train['']\n",
    "\n",
    "como roda?\n",
    "fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(n_estimators=10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#um modelo seco, sem nenhuma manipulação especial, rodamos um RandomForestRegressor no treino com 10 estimadores\n",
    "# escolhemos os campos loja e item que por acaso são numéricos\n",
    "# fazemos isto para termos uma linha base, assim a gente sabe se nosso trabalho está melhorando ou piorando o resultado\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Read train data\n",
    "train = pd.read_csv('demand_forecasting_train_1_month.csv')\n",
    "\n",
    "# Create Random Forest object\n",
    "rf = RandomForestRegressor(n_estimators = 10)\n",
    "\n",
    "# Train a model\n",
    "rf.fit(X=train[['store', 'item']], y=train['sales'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare a submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "quem está na submissão? \n",
    "eu acho que vendas(y), loja e item do dataset de test\n",
    "de onde obtenho? da execução do algoritmo\n",
    "\n",
    "Errei, o algoritmo só retorna as sales(vendas) e são só elas que eu escrevo. A loja e item já são sabidas pela linha que estou prevendo do arquivo original ('demand_forecasting_test.csv')\n",
    "o que estou escrevendo é o kaggle_submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read test and sample submission data\n",
    "#vou preparar a submissão baseada no que treinei acima, logo os campos tem que ser os mesmos do fit\n",
    "test = pd.read_csv('demand_forecasting_test.csv')\n",
    "\n",
    "# o formato do arquivo é assim\n",
    "#  id        date  store  item\n",
    "#   0  2018-01-01      1     1\n",
    "#   1  2018-01-02      1     1\n",
    "#   2  2018-01-03      1     1\n",
    "\n",
    "# Faz predições para o teste\n",
    "# ele ignora datas\n",
    "test['sales'] = rf.predict(test[['store', 'item']])\n",
    "\n",
    "# Write test predictions in the sample_submission format\n",
    "test[['sales']].to_csv('kaggle_submission.csv', index=False)\n",
    "#print (test[['sales']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train XGBoost models\n",
    "\n",
    "aqui começo do zero, a única coisa que aproveito de cima é a importação do csv\n",
    "\n",
    "como rodo xgboost, é bem simples, import sem from, o turbo é independente  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17188/3917570318.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFutureWarning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Create DMatrix on train data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Create DMatrix on train data\n",
    "dtrain = xgb.DMatrix(data=train[['store', 'item']],\n",
    "                     label=train['sales'])\n",
    "\n",
    "# Define xgboost parameters\n",
    "params = {'objective': 'reg:linear',\n",
    "          'max_depth':2,\n",
    "          'silent': 1}\n",
    "\n",
    "# Train xgboost model\n",
    "xg_depth_2 = xgb.train(params=params, dtrain=dtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Create DMatrix on train data\n",
    "dtrain = xgb.DMatrix(data=train[['store', 'item']],\n",
    "                     label=train['sales'])\n",
    "\n",
    "# Define xgboost parameters\n",
    "params = {'objective': 'reg:linear',\n",
    "          'max_depth': 8,\n",
    "          'silent': 1}\n",
    "\n",
    "# Train xgboost model\n",
    "xg_depth_8 = xgb.train(params=params, dtrain=dtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Create DMatrix on train data\n",
    "dtrain = xgb.DMatrix(data=train[['store', 'item']],\n",
    "                     label=train['sales'])\n",
    "\n",
    "# Define xgboost parameters\n",
    "params = {'objective': 'reg:linear',\n",
    "          'max_depth': 15,\n",
    "          'silent': 1}\n",
    "\n",
    "# Train xgboost model\n",
    "xg_depth_15 = xgb.train(params=params, dtrain=dtrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore overfitting XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Lê os dados\n",
    "dtrain = xgb.DMatrix(data=train[['store', 'item']])\n",
    "dtest = xgb.DMatrix(data=test[['store', 'item']])\n",
    "\n",
    "# For each of 3 trained models\n",
    "for model in [xg_depth_2, xg_depth_8, xg_depth_15]:\n",
    "    # Make predictions\n",
    "    train_pred = model.predict(dtrain)     \n",
    "    test_pred = model.predict(dtest)          \n",
    "    \n",
    "    # Compute metrics\n",
    "    mse_train = mean_squared_error(train['sales'], train_pred)                  \n",
    "    mse_test = mean_squared_error(test['sales'], test_pred)\n",
    "    print('MSE Train: {:.3f}. MSE Test: {:.3f}'.format(mse_train, mse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui não overfitou, mas lá no datacamp sim, então vamos ler a mensagem de lá:\n",
    "\n",
    "So, you see that the third model with depth 15 is already overfitting. It has considerably lower train error compared to the second model, however test error is higher. Be aware of overfitting and move on to the next chapter to know how to beat it!\n",
    "o erro de treino caiu no terceiro mas o erro de teste aumentou, segundo os resultados lá no datacamp. Veja os números abaixo:\n",
    "    MSE Train: 631.275. MSE Test: 558.522\n",
    "    MSE Train: 183.771. MSE Test: 337.337\n",
    "    MSE Train: 134.984. MSE Test: 355.534"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dive into the Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a competition metric\n",
    "o que faz a MSE?\n",
    "Compara previsões com o de verdade (true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_regression_true= np.array([0.7, 0.29, 0.23, 0.55, 0.72, 0.42, 0.98, 0.68, 0.48, 0.39, 0.34, 0.73, 0.44, 0.06, 0.4, 0.74, 0.18, 0.18, 0.53, 0.53, 0.63, 0.85, 0.72, 0.61, 0.72, 0.32, 0.36, 0.23, 0.29, 0.63, 0.09, 0.43, 0.43, 0.49, 0.43, 0.31, 0.43, 0.89, 0.94, 0.5, 0.62, 0.12, 0.32, 0.41, 0.87, 0.25, 0.48, 0.99, 0.52, 0.61, 0.12, 0.83, 0.6, 0.55, 0.34, 0.3, 0.42, 0.68, 0.88, 0.51, 0.67, 0.59, 0.62, 0.67, 0.84, 0.08, 0.76, 0.24, 0.19, 0.57, 0.1, 0.89, 0.63, 0.72, 0.02, 0.59, 0.56, 0.16, 0.15, 0.7, 0.32, 0.69, 0.55, 0.39, 0.93, 0.84, 0.36, 0.04, 0.3, 0.4, 0.7, 1.0, 0.36, 0.76, 0.59, 0.69, 0.15, 0.4, 0.24, 0.34])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_regression_pred = np.array([0.51, 0.67, 0.11, 0.13, 0.32, 0.66, 0.85, 0.55, 0.85, 0.38, 0.32, 0.35, 0.17, 0.83, 0.34, 0.55, 0.58, 0.52, 0.0, 0.99, 0.91, 0.21, 0.29, 0.52, 0.9, 0.98, 0.26, 0.56, 0.81, 0.39, 0.73, 0.16, 0.6, 0.87, 0.98, 0.08, 0.43, 0.2, 0.45, 0.55, 0.09, 0.3, 0.93, 0.57, 0.46, 0.75, 0.74, 0.05, 0.71, 0.84, 0.17, 0.78, 0.29, 0.31, 0.67, 0.11, 0.66, 0.89, 0.7, 0.44, 0.44, 0.77, 0.57, 0.08, 0.58, 0.81, 0.34, 0.93, 0.75, 0.57, 0.75, 0.08, 0.86, 0.82, 0.91, 0.13, 0.08, 0.14, 0.4, 0.42, 0.56, 0.12, 0.2, 0.81, 0.47, 0.81, 0.01, 0.55, 0.93, 0.58, 0.21, 0.72, 0.38, 0.67, 0.03, 0.64, 0.03, 0.74, 0.47, 0.12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Import MSE from sklearn\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Define your own MSE function\n",
    "def own_mse(y_true, y_pred):\n",
    "  \t# Find squared differences\n",
    "    squares = np.power(y_true - y_pred, 2)\n",
    "    # Find mean over all observations\n",
    "    err = np.mean(squares)\n",
    "    return err\n",
    "\n",
    "print('Sklearn MSE: {:.5f}. '.format(mean_squared_error(y_regression_true, y_regression_pred)))\n",
    "print('Your MSE: {:.5f}. '.format(own_mse(y_regression_true, y_regression_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_classification_true = np.array([1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_classification_pred = np.array([0.21, 0.44, 0.72, 0.41, 0.19, 0.97, 0.65, 0.87, 0.03, 0.27, 0.5, 0.07, 0.99, 0.24, 0.37, 0.21, 0.11, 0.23, 0.3, 0.63, 0.28, 0.36, 0.01, 0.37, 0.53, 0.16, 0.6, 0.29, 0.63, 0.03, 0.89, 0.02, 0.13, 0.78, 0.05, 0.71, 0.97, 0.87, 0.71, 0.96, 0.43, 0.87, 0.36, 0.93, 0.15, 0.94, 0.83, 0.85, 0.12, 0.6, 0.02, 0.72, 0.01, 0.08, 0.23, 0.88, 0.36, 0.54, 0.57, 0.23, 0.57, 0.66, 0.3, 0.42, 0.45, 0.93, 0.59, 0.95, 0.56, 0.5, 0.0, 0.48, 0.93, 0.2, 0.05, 0.41, 0.37, 0.86, 0.03, 0.92, 0.68, 0.9, 0.61, 0.81, 0.34, 0.35, 0.39, 0.75, 0.37, 0.24, 0.94, 0.91, 0.35, 0.63, 0.27, 0.21, 0.34, 0.33, 0.88, 0.82])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Import log_loss from sklearn\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# Define your own LogLoss function\n",
    "def own_logloss(y_true, prob_pred):\n",
    "   \t# Find loss for each observation\n",
    "     terms = y_true * np.log(prob_pred) + (1 - y_true) * np.log(1-prob_pred)\n",
    "     # Find mean over all observations\n",
    "     err = np.mean(terms) \n",
    "     return -err\n",
    "\n",
    "print('Sklearn LogLoss: {:.5f}'.format(log_loss(y_classification_true, y_classification_pred)))\n",
    "print('Your LogLoss: {:.5f}'.format(own_logloss(y_classification_true, y_classification_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA statistics\n",
    "### aqui temos os taxis sendo tratados, vamos olhar para os datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('taxi_train_chapter_4.csv')\n",
    "test = pd.read_csv('taxi_test_chapter_4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shapes of train and test data\n",
    "print('Train shape:', train.shape)\n",
    "print('Test shape:', test.shape)\n",
    "\n",
    "# Train head\n",
    "display(train.head())\n",
    "\n",
    "# Describe the target variable\n",
    "print(train.fare_amount.describe())\n",
    "\n",
    "# Train distribution of passengers within rides\n",
    "print(train.passenger_count.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(train.passenger_count.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA plots I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_distance(train):\n",
    "    \n",
    "    data = [train]\n",
    "    lat1, long1, lat2, long2 = 'pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude'\n",
    "    \n",
    "    for i in data:\n",
    "        R = 6371  #radius of earth in kilometers\n",
    "        #R = 3959 #radius of earth in miles\n",
    "        phi1 = np.radians(i[lat1])\n",
    "        phi2 = np.radians(i[lat2])\n",
    "    \n",
    "        delta_phi = np.radians(i[lat2]-i[lat1])\n",
    "        delta_lambda = np.radians(i[long2]-i[long1])\n",
    "    \n",
    "        #a = sin²((φB - φA)/2) + cos φA . cos φB . sin²((λB - λA)/2)\n",
    "        a = np.sin(delta_phi / 2.0) ** 2 + np.cos(phi1) * np.cos(phi2) * np.sin(delta_lambda / 2.0) ** 2\n",
    "    \n",
    "        #c = 2 * atan2( √a, √(1−a) )\n",
    "        c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "    \n",
    "        #d = R*c\n",
    "        d = (R * c) #in kilometers\n",
    "        \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the ride distance\n",
    "train['distance_km'] = haversine_distance(train)\n",
    "\n",
    "# Draw a scatterplot\n",
    "plt.scatter(train['fare_amount'], train['distance_km'], alpha=0.5)\n",
    "plt.xlabel('Fare amount')\n",
    "plt.ylabel('Distance, km')\n",
    "plt.title('Fare amount based on the distance')\n",
    "\n",
    "# Limit on the distance\n",
    "plt.ylim(0, 50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA plots II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create hour feature\n",
    "train['pickup_datetime'] = pd.to_datetime(train.pickup_datetime)\n",
    "train['hour'] = train.pickup_datetime.dt.hour\n",
    "\n",
    "# Find median fare_amount for each hour\n",
    "hour_price = train.groupby('hour', as_index=False)['fare_amount'].median()\n",
    "\n",
    "# Plot the line plot\n",
    "plt.plot(hour_price.hour, hour_price.fare_amount, marker='o')\n",
    "plt.xlabel('Hour of the day')\n",
    "plt.ylabel('Fare amount')\n",
    "plt.title('Fare amount based on day time')\n",
    "plt.xticks(range(24))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame([[10, 1.5, 3, '53a5b119ba8f7b61d4e010512e0dfc85', 40.7145, -73.9425, '5ba989232d0489da1b5f2c45f6688adc', 3000, 'medium'], [10000, 1.0, 2, 'c5c8a357cba207596b04d1afd1e4f130', 40.7947, -73.9667, '7533621a882f71e25173b27e3139d83d', 5465, 'low'], [100004, 1.0, 1, 'c3ba40552e2120b0acfc3cb5730bb2aa', 40.7388, -74.0018, 'd9039c43983f6e564b1482b273bd7b01', 2850, 'high'], [100007, 1.0, 1, '28d9ad350afeaab8027513a3e52ac8d5', 40.7539, -73.9677, '1067e078446a7897d2da493d2f741316', 3275, 'low'], [100013, 1.0, 4, '0', 40.8241, -73.9493, '98e13ad4b495b9613cef886d79a6291f', 3350, 'low'], [100014, 2.0, 4, '38a913e46c94a7f46ddf19b756a9640c', 40.7429, -74.0028, 'b209e2c4384a64cc307c26759ee0c651', 7995, 'medium'], [100016, 1.0, 2, '3ba49a93260ca5df92fde024cb4ca61f', 40.8012, -73.96600000000002, '01287194f20de51872e81f660def4784', 3600, 'low'], [100020, 2.0, 1, '0372927bcb6a0949613ef5bf893bbac7', 40.7427, -73.9957, 'e6472c7237327dd3903b3d6f6a94515a', 5645, 'low'], [100026, 1.0, 1, 'a7efbeb58190aa267b4a9121cd0c88c0', 40.8234, -73.9457, 'c1a6598437b7db560cde66e5a297a53f', 1725, 'medium'], [100027, 2.0, 4, '0', 40.7278, -73.9808, '23a01ea7717b38875f5b070282d1b9d2', 5800, 'low'], [100030, 1.0, 0, '0', 40.7769, -73.9467, 'e32475a6134d6d18279946b7b20a0f12', 1950, 'low'], [10004, 1.0, 1, '0', 40.8448, -73.9396, '2dee0868ac01057760055b856e66e942', 1675, 'low'], [100044, 1.0, 2, '67c9b420da4a365bc26a6cd0ef4a5320', 40.7488, -73.977, '537e06890f6a86dbb70c187db5be4d55', 3000, 'high'], [100048, 2.0, 2, '0', 40.7707, -73.9817, '3813434aabfbad659a45f86a740cd23f', 6895, 'low'], [10005, 1.0, 1, '0', 40.7584, -73.9648, '75f38d077366d7964c2f3bb80c3e7b1d', 3050, 'low'], [100051, 1.0, 0, 'bfb9405149bfff42a92980b594c28234', 40.7439, -73.9743, 'dbbb6b990661b1e507a387f019bcb1a0', 2350, 'medium'], [100052, 1.0, 2, '642cc2c920512ffe2a74c28122f8b47f', 40.7305, -73.983, '0411b25b134141100d8214ed6ec02f56', 3650, 'low'], [100053, 1.0, 1, '0', 40.8643, -73.928, 'ab15d581a04ed87b6f25d5aff8ddde30', 1695, 'low'], [100055, 1.0, 4, 'cc4c6ae9225df6d2395c4e16c235f7ab', 40.7999, -73.9638, '8f5a9c893f6d602f4953fcc0b8e6e9b4', 5000, 'low'], [100058, 1.0, 1, 'dc3cae15729b48fec3394f9295671991', 40.7328, -73.9799, '8f5a9c893f6d602f4953fcc0b8e6e9b4', 3973, 'low'], [100062, 1.0, 3, '205f95d4a78f1f3befda48b89edc9669', 40.7454, -73.9845, '3793e58c60343a3fd6846ca2d2ef3c7f', 4395, 'low'], [100063, 1.0, 2, 'ecea86b79afa539505aa4bad3ff449c6', 40.7427, -73.9794, 'e6472c7237327dd3903b3d6f6a94515a', 2999, 'medium'], [100065, 1.0, 1, '1a6cf9b71da65cdc0cfd5015a75317ac', 40.7447, -73.9741, '8f5a9c893f6d602f4953fcc0b8e6e9b4', 2595, 'low'], [100066, 1.0, 1, 'a6200d7448037cfee809b2451219f879', 40.7074, -74.0081, '6e5c10246156ae5bdcd9b487ca99d96a', 3695, 'low'], [10007, 2.0, 4, '05b871a1e1e0368391160bcefc608e00', 40.7391, -73.9936, '0cff4a2a147d81e51ae8a91289e129f7', 7400, 'medium'], [100071, 3.5, 4, '45b4143a9841eb63d7203c4f21a64ff7', 40.7584, -73.9653, 'ad3d8ddc52c7e0859b5c6c7f7949c3bd', 7500, 'low'], [100075, 1.0, 1, 'd68497ff1f5ac77f143e5f21817ed6fb', 40.7728, -73.9502, '4f9b376a972b863bbfbdcdc8d717fa4e', 2295, 'medium'], [100076, 1.0, 1, 'ec447453d3b8033e14a7b54ba1e13e02', 40.7709, -73.9917, '63e8c482071c22f43dd91b954e75cc2c', 3164, 'low'], [100079, 1.0, 1, 'f06ad2f6f1a821c8efc03dc195d369df', 40.8335, -73.9141, '12c0a30e296faa0dfd422fe918d7d4f4', 1350, 'high'], [100081, 2.0, 2, 'f115a9bff3a9c4e6eedc9114374b3d74', 40.7716, -73.9506, '797f306f24780e1a24e4f063d46a2d8d', 5600, 'low'], [100083, 1.0, 0, '10901f921370e636cef42c4bbed02483', 40.7897, -73.976, '92ff104c90a9c40d153ea970e19c27c1', 2750, 'medium'], [100084, 1.0, 2, '0aa1e10d1f77ce334a02ecd84558f439', 40.7902, -73.9678, 'bce75205499f22e47b0c1aa94e09131c', 3500, 'medium'], [100085, 1.0, 1, '7b932952de7f92e25ee7acc7eeb3b55c', 40.7179, -74.0148, '9e75980b063d7d41226e28ebc8e02824', 5165, 'low'], [100087, 1.0, 2, '46b5fd6e1ed6d002f5e6557f7e25bce1', 40.7301, -73.9942, '9e4ef9d8a24f407bf0b355b27f0d9965', 4400, 'low'], [10009, 1.0, 2, '3ae1e557d21e2a2e080e29aac7dd60c6', 40.844, -73.9404, '7c5e4fc025b70c6540d6b0e06716b9dd', 2300, 'low'], [100090, 1.0, 1, '0', 40.79, -73.9418, '42931f67f3661ec1b33d6b7e42754d9a', 1650, 'low'], [100096, 1.0, 0, '0', 40.8184, -73.9389, 'fb520ae87b8a3b3eb646e137f0c8ddc8', 1300, 'low'], [100098, 1.0, 0, '7d2a37e6633ae6e663fca13dfa55d9a9', 40.7649, -73.9763, '32bf3cd19652f17e3764c055b7a9178e', 1980, 'high'], [100099, 2.0, 2, '39661b8ce46a8d71461497819f67c279', 40.7471, -73.9867, 'ad3d8ddc52c7e0859b5c6c7f7949c3bd', 6500, 'low'], [10010, 1.0, 0, 'd0234abbc01a982d54e8d446acc03405', 40.753, -73.9959, '6a6c75bdb10af785bebd2a09a53fc4c2', 2396, 'medium'], [100100, 1.0, 3, 'be6b7c3fdf3f63a2756306f4af7788a6', 40.7231, -74.0044, '64249f81378907ae7cf65e8ccb4bd8dc', 3733, 'low'], [100102, 3.0, 4, 'cd5dbf58ff7014957be69643a96aaaf5', 40.7723, -73.9533, '964dc31a872efa33fee9af11f62e843c', 15000, 'low'], [100107, 1.0, 0, '0', 40.7753, -73.954, '537e06890f6a86dbb70c187db5be4d55', 2400, 'medium'], [100112, 1.0, 0, '7b48df0cc3a04a24b062efbc5cf5022e', 40.7739, -73.9511, 'cb87dadbca78fad02b388dc9e8f25a5b', 1850, 'medium'], [100113, 1.0, 1, '0', 40.8031, -73.957, 'baf2b718f413a41eba82a1354a82ca86', 2500, 'low'], [100115, 1.0, 1, '1bd24d2906d981ec60703de71921dd2e', 40.7975, -73.9626, '44500d5a13767adee85ea875fb2d3bd5', 2045, 'medium'], [100117, 1.0, 0, '379a891accd668964c9d2080cce45179', 40.7372, -73.9981, 'aa5f74da43dceaca9b34ed7f55a39f5d', 2650, 'low'], [100119, 1.0, 2, '7bc33ecc302c59a2f179803c3f90a03f', 40.7479, -74.0005, '6be05c0ba31bf6adce280cf734933a12', 3745, 'low'], [10012, 1.0, 1, 'cdf3e0d9b2c48161b2432786b3ed0cd0', 40.7802, -73.9504, 'ac888923cda3b9e4f0311267be25de9c', 1900, 'low'], [100124, 1.0, 2, 'a6fb47bf44e73e699b9acf3d8cde48f0', 40.7377, -73.9831, '046ed6c8a67bc942fcf4ca43e96f27d8', 5595, 'low']], columns = ['id', 'bathrooms', 'bedrooms', 'building_id', 'latitude', 'longitude', 'manager_id', 'price', 'interest_level'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('twosigma_train.csv')\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import KFold\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Create a KFold object\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=123)\n",
    "\n",
    "# Loop through each split\n",
    "fold = 0\n",
    "for train_index, test_index in kf.split(train):\n",
    "    # Obtain training and testing folds\n",
    "    cv_train, cv_test = train.iloc[train_index], train.iloc[test_index]\n",
    "    print('Fold: {}'.format(fold))\n",
    "    print('CV train shape: {}'.format(cv_train.shape))\n",
    "    print('Medium interest listings in CV train: {}\\n'.format(sum(cv_train.interest_level == 'medium')))\n",
    "    fold += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratified K-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import StratifiedKFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Create a StratifiedKFold object\n",
    "str_kf = StratifiedKFold(n_splits=3, shuffle = True, random_state=123)\n",
    "\n",
    "# Loop through each split\n",
    "fold = 0\n",
    "for train_index, test_index in str_kf.split(train, train.interest_level):\n",
    "    # Obtain training and testing folds\n",
    "    cv_train, cv_test = train.iloc[train_index], train.iloc[test_index]\n",
    "    print('Fold: {}'.format(fold))\n",
    "    print('CV train shape: {}'.format(cv_train.shape))\n",
    "    print('Medium interest listings in CV train: {}\\n'.format(sum(cv_train.interest_level == 'medium')))\n",
    "    fold += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time K-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read train data\n",
    "train = pd.read_csv('demand_forecasting_train_1_month.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# Create TimeSeriesSplit object\n",
    "time_kfold = TimeSeriesSplit(n_splits = 3)\n",
    "\n",
    "# Sort train data by date\n",
    "train = train.sort_values('date')\n",
    "\n",
    "# Iterate through each split\n",
    "fold = 0\n",
    "for train_index, test_index in time_kfold.split(train):\n",
    "    cv_train, cv_test = train.iloc[train_index], train.iloc[test_index]\n",
    "    \n",
    "    print('Fold :', fold)\n",
    "    print('Train date range: from {} to {}'.format(cv_train.date.min(), cv_train.date.max()))\n",
    "    print('Test date range: from {} to {}\\n'.format(cv_test.date.min(), cv_test.date.max()))\n",
    "    fold += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall validation score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fold_mse(train, kf):\n",
    "    mse_scores = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(train):\n",
    "        fold_train, fold_test = train.loc[train_index], train.loc[test_index]\n",
    "\n",
    "        # Fit the data and make predictions\n",
    "        # Create a Random Forest object\n",
    "        rf = RandomForestRegressor(n_estimators=10, random_state=123)\n",
    "\n",
    "        # Train a model\n",
    "        rf.fit(X=fold_train[['store', 'item']], y=fold_train['sales'])\n",
    "\n",
    "        # Get predictions for the test set\n",
    "        pred = rf.predict(fold_test[['store', 'item']])\n",
    "    \n",
    "        fold_score = round(mean_squared_error(fold_test['sales'], pred), 5)\n",
    "        mse_scores.append(fold_score)\n",
    "        \n",
    "    return mse_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import numpy as np\n",
    "\n",
    "# Sort train data by date\n",
    "train = train.sort_values('date')\n",
    "\n",
    "# Initialize 3-fold time cross-validation\n",
    "kf = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "# Get MSE scores for each cross-validation split\n",
    "mse_scores = get_fold_mse(train, kf)\n",
    "\n",
    "print('Mean validation MSE: {:.5f}'.format(np.mean(mse_scores)))\n",
    "print('MSE by fold: {}'.format(mse_scores))\n",
    "print('Overall validation MSE: {:.5f}'.format(np.mean(mse_scores) + np.std(mse_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('house_prices_train.csv')\n",
    "test = pd.read_csv('house_prices_test.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arithmetical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kfold_rmse(train):\n",
    "    mse_scores = []\n",
    "\n",
    "    for train_index, test_index in kf.split(train):\n",
    "        train = train.fillna(0)\n",
    "        feats = [x for x in train.columns if x not in ['Id', 'SalePrice', 'RoofStyle', 'CentralAir']]\n",
    "        \n",
    "        fold_train, fold_test = train.loc[train_index], train.loc[test_index]\n",
    "\n",
    "        # Fit the data and make predictions\n",
    "        # Create a Random Forest object\n",
    "        rf = RandomForestRegressor(n_estimators=10, min_samples_split=10, random_state=123)\n",
    "\n",
    "        # Train a model\n",
    "        rf.fit(X=fold_train[feats], y=fold_train['SalePrice'])\n",
    "\n",
    "        # Get predictions for the test set\n",
    "        pred = rf.predict(fold_test[feats])\n",
    "    \n",
    "        fold_score = mean_squared_error(fold_test['SalePrice'], pred)\n",
    "        mse_scores.append(np.sqrt(fold_score))\n",
    "        \n",
    "    return round(np.mean(mse_scores) + np.std(mse_scores), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the initial RMSE\n",
    "print('RMSE before feature engineering:', get_kfold_rmse(train))\n",
    "\n",
    "# Add total area of the house\n",
    "train['TotalArea'] = train['TotalBsmtSF'] + train['1stFlrSF'] + train['2ndFlrSF']\n",
    "print('RMSE with total area:', get_kfold_rmse(train))\n",
    "\n",
    "# Add garder area of the property\n",
    "train['GardenArea'] = train['LotArea'] - train['1stFlrSF']\n",
    "print('RMSE with garden area:', get_kfold_rmse(train))\n",
    "\n",
    "# Add total number of bathrooms\n",
    "train['TotalBath'] = train['FullBath'] + train['HalfBath']\n",
    "print('RMSE with number of bathrooms:', get_kfold_rmse(train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('taxi_train_chapter_4.csv')\n",
    "test = pd.read_csv('taxi_test_chapter_4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate train and test together\n",
    "taxi = pd.concat([train, test])\n",
    "\n",
    "# Convert pickup date to datetime object\n",
    "taxi['pickup_datetime'] = pd.to_datetime(taxi['pickup_datetime'])\n",
    "\n",
    "# Create day of week feature\n",
    "taxi['day_of_week'] = taxi['pickup_datetime'].dt.dayofweek\n",
    "\n",
    "# Create hour feature\n",
    "taxi['hour'] = taxi['pickup_datetime'].dt.hour\n",
    "\n",
    "# Split back into train and test\n",
    "new_train = taxi[taxi.id.isin(train.id)]\n",
    "new_test = taxi[taxi.id.isin(test.id)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('house_prices_train.csv')\n",
    "test = pd.read_csv('house_prices_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate train and test together\n",
    "houses = pd.concat([train, test])\n",
    "\n",
    "# Label encoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Create new features\n",
    "houses['RoofStyle_enc'] = le.fit_transform(houses['RoofStyle'])\n",
    "houses['CentralAir_enc'] = le.fit_transform(houses['CentralAir'])\n",
    "\n",
    "# Look at new features\n",
    "print(houses[['RoofStyle', 'RoofStyle_enc', 'CentralAir', 'CentralAir_enc']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate train and test together\n",
    "houses = pd.concat([train, test])\n",
    "\n",
    "# Label encode binary 'CentralAir' feature\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "houses['CentralAir_enc'] = le.fit_transform(houses['CentralAir'])\n",
    "\n",
    "# Create One-Hot encoded features\n",
    "ohe = pd.get_dummies(houses['RoofStyle'], prefix='RoofStyle')\n",
    "\n",
    "# Concatenate OHE features to houses\n",
    "houses = pd.concat([houses, ohe], axis=1)\n",
    "\n",
    "# Look at OHE features\n",
    "display(houses[[col for col in houses.columns if 'RoofStyle' in col]].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean target encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_mean_target_encoding(train, test, target, categorical, alpha=5):\n",
    "    # Calculate global mean on the train data\n",
    "    global_mean = train[target].mean()\n",
    "    \n",
    "    # Group by the categorical feature and calculate its properties\n",
    "    train_groups = train.groupby(categorical)\n",
    "    category_sum = train_groups[target].sum()\n",
    "    category_size = train_groups.size()\n",
    "    \n",
    "    # Calculate smoothed mean target statistics\n",
    "    train_statistics = (category_sum + global_mean * alpha) / (category_size + alpha)\n",
    "    \n",
    "    # Apply statistics to the test data and fill new categories\n",
    "    test_feature = test[categorical].map(train_statistics).fillna(global_mean)\n",
    "    return test_feature.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mean_target_encoding(train, target, categorical, alpha=5):\n",
    "    # Create 5-fold stratified cross-validation\n",
    "    skf = StratifiedKFold(n_splits=5, random_state=123, shuffle=True)\n",
    "    train_feature = pd.Series(index=train.index)\n",
    "    \n",
    "    # For each folds split\n",
    "    for train_index, test_index in skf.split(train, train[target]):\n",
    "        cv_train, cv_test = train.iloc[train_index], train.iloc[test_index]\n",
    "      \n",
    "        # Calculate out-of-fold statistics and apply to cv_test\n",
    "        cv_test_feature = test_mean_target_encoding(cv_train, cv_test, target, categorical, alpha)\n",
    "        \n",
    "        # Save new feature for this particular fold\n",
    "        train_feature.iloc[test_index] = cv_test_feature       \n",
    "    return train_feature.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_target_encoding(train, test, target, categorical, alpha=5):\n",
    "  \n",
    "    # Get test feature\n",
    "    test_feature = test_mean_target_encoding(train, test, target, categorical, alpha)\n",
    "    \n",
    "    # Get train feature\n",
    "    train_feature = train_mean_target_encoding(train, target, categorical, alpha)\n",
    "    \n",
    "    # Return new features to add to the model\n",
    "    return train_feature, test_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bryant_shots = pd.DataFrame([[2, 20000012, 34.0443, -118.4268, 10, 0, '2000-01', 15, '2PT Field Goal', 0.0], [3, 20000012, 33.9093, -118.3708, 7, 0, '2000-01', 16, '2PT Field Goal', 1.0], [4, 20000012, 33.8693, -118.1318, 6, 0, '2000-01', 22, '2PT Field Goal', 0.0], [5, 20000012, 34.0443, -118.2698, 6, 0, '2000-01', 0, '2PT Field Goal', 1.0], [6, 20000012, 34.0553, -118.4148, 9, 0, '2000-01', 14, '2PT Field Goal', 0.0], [7, 20000012, 34.0443, -118.2698, 8, 0, '2000-01', 0, '2PT Field Goal', 1.0], [9, 20000012, 33.9363, -118.3348, 6, 0, '2000-01', 12, '2PT Field Goal', 1.0], [10, 20000012, 33.9193, -118.3028, 3, 0, '2000-01', 12, '2PT Field Goal', 0.0], [11, 20000012, 33.8063, -118.3638, 1, 0, '2000-01', 25, '3PT Field Goal', 0.0], [12, 20000019, 33.9173, -118.1488, 11, 0, '2000-01', 17, '2PT Field Goal', 1.0], [13, 20000019, 33.9343, -118.3368, 7, 0, '2000-01', 12, '2PT Field Goal', 1.0], [14, 20000019, 34.0403, -118.3638, 2, 0, '2000-01', 9, '2PT Field Goal', 0.0], [15, 20000019, 33.9973, -118.2928, 1, 0, '2000-01', 5, '2PT Field Goal', 0.0], [16, 20000019, 33.8523, -118.2078, 0, 0, '2000-01', 20, '2PT Field Goal', 0.0], [18, 20000019, 33.8183, -118.3868, 8, 0, '2000-01', 25, '3PT Field Goal', 1.0], [19, 20000019, 33.9473, -118.4018, 11, 0, '2000-01', 16, '2PT Field Goal', 0.0], [21, 20000019, 33.9003, -118.2668, 10, 0, '2000-01', 14, '2PT Field Goal', 0.0], [22, 20000019, 33.9173, -118.1358, 9, 0, '2000-01', 18, '2PT Field Goal', 0.0], [23, 20000019, 33.9343, -118.2858, 7, 0, '2000-01', 11, '2PT Field Goal', 1.0], [24, 20000019, 33.8943, -118.3788, 5, 0, '2000-01', 18, '2PT Field Goal', 1.0], [25, 20000019, 33.9813, -118.3158, 5, 0, '2000-01', 7, '2PT Field Goal', 1.0], [26, 20000019, 34.0443, -118.2698, 2, 0, '2000-01', 0, '2PT Field Goal', 0.0], [27, 20000019, 33.8483, -118.3278, 2, 0, '2000-01', 20, '2PT Field Goal', 0.0], [28, 20000019, 33.8583, -118.4528, 0, 0, '2000-01', 26, '3PT Field Goal', 0.0], [29, 20000019, 33.8713, -118.1848, 8, 0, '2000-01', 19, '2PT Field Goal', 0.0], [30, 20000019, 33.9573, -118.2668, 6, 0, '2000-01', 8, '2PT Field Goal', 0.0], [31, 20000019, 34.0403, -118.1488, 1, 0, '2000-01', 12, '2PT Field Goal', 1.0], [32, 20000019, 34.0103, -118.1428, 0, 0, '2000-01', 13, '2PT Field Goal', 0.0], [39, 20000047, 33.8603, -118.1788, 3, 0, '2000-01', 20, '2PT Field Goal', 1.0], [40, 20000047, 33.7723, -118.2968, 0, 0, '2000-01', 27, '3PT Field Goal', 1.0]], columns = ['shot_id', 'game_id', 'lat', 'lon', 'minutes_remaining', 'playoffs', 'season', 'shot_distance', 'shot_type', 'shot_made_flag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Create 5-fold cross-validation\n",
    "skf = StratifiedKFold(n_splits=5, random_state=123, shuffle=True)\n",
    "\n",
    "# For each folds split\n",
    "for train_index, test_index in skf.split(bryant_shots, bryant_shots['shot_made_flag']):\n",
    "    cv_train, cv_test = bryant_shots.iloc[train_index], bryant_shots.iloc[test_index]\n",
    "    \n",
    "    # Create mean target encoded feature\n",
    "    cv_train['game_id_enc'], cv_test['game_id_enc'] = mean_target_encoding(train=cv_train,\n",
    "                                                                           test=cv_test,\n",
    "                                                                           target='shot_made_flag',\n",
    "                                                                           categorical='game_id',\n",
    "                                                                           alpha=5)\n",
    "    # Look at the encoding\n",
    "    display(cv_train[['game_id', 'shot_made_flag', 'game_id_enc']].sample(n=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beyond binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('house_prices_train.csv')\n",
    "test = pd.read_csv('house_prices_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mean target encoded feature\n",
    "train['RoofStyle_enc'], test['RoofStyle_enc'] = mean_target_encoding(train=train,\n",
    "                                                                     test=test,\n",
    "                                                                     target='SalePrice',\n",
    "                                                                     categorical='RoofStyle',\n",
    "                                                                     alpha=10)\n",
    "# Look at the encoding\n",
    "print(test[['RoofStyle', 'RoofStyle_enc']].drop_duplicates())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataframe\n",
    "twosigma = pd.read_csv('twosigma_train.csv')\n",
    "\n",
    "# Find the number of missing values in each column\n",
    "print(twosigma.isnull().sum())\n",
    "\n",
    "# Look at the columns with missing values\n",
    "display(twosigma[[\"building_id\", \"price\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SimpleImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Create mean imputer\n",
    "mean_imputer = SimpleImputer(strategy = 'mean')\n",
    "\n",
    "# Price imputation\n",
    "twosigma[['price']] = mean_imputer.fit_transform(twosigma[['price']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SimpleImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Create constant imputer\n",
    "constant_imputer = SimpleImputer(strategy = 'constant', fill_value = 'MISSING')\n",
    "\n",
    "# building_id imputation\n",
    "twosigma[['building_id']] = constant_imputer.fit_transform(twosigma[['building_id']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replicate validation score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('taxi_train_chapter_4.csv')\n",
    "train.shape\n",
    "validation_train = train[:10000]\n",
    "validation_test = train[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "# Calculate average fare_amount on the validation_train data\n",
    "naive_prediction = np.mean(validation_train['fare_amount'])\n",
    "\n",
    "# Assign naive prediction to all the holdout observations\n",
    "validation_test['pred'] = naive_prediction\n",
    "\n",
    "# Measure the local RMSE\n",
    "rmse = sqrt(mean_squared_error(validation_test['fare_amount'], validation_test['pred']))\n",
    "print('Validation RMSE for Baseline I model: {:.3f}'.format(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline based on the date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('taxi_train_chapter_4.csv', parse_dates = ['pickup_datetime'])\n",
    "test = pd.read_csv('taxi_test_chapter_4.csv', parse_dates = ['pickup_datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pickup hour from the pickup_datetime column\n",
    "train['hour'] = train['pickup_datetime'].dt.hour\n",
    "test['hour'] = test['pickup_datetime'].dt.hour\n",
    "\n",
    "# Calculate average fare_amount by pickup hour \n",
    "hour_groups = train.groupby('hour').fare_amount.mean()\n",
    "\n",
    "# Make predicitons on the test set\n",
    "test['fare_amount'] = test.hour.map(hour_groups)\n",
    "\n",
    "# Write predictions\n",
    "test[['id','fare_amount']].to_csv('hour_mean_sub.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline based on the gradient boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Select only numeric features\n",
    "features = ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude',\n",
    "            'dropoff_latitude', 'passenger_count', 'hour']\n",
    "\n",
    "# Train a Random Forest model\n",
    "rf = RandomForestRegressor()\n",
    "rf.fit(train[features], train.fare_amount)\n",
    "\n",
    "# Make predictions on the test data\n",
    "test['fare_amount'] = rf.predict(test[features])\n",
    "\n",
    "# Write predictions\n",
    "test[['id','fare_amount']].to_csv('rf_sub.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "def get_cv_score(train, params):\n",
    "    # Create KFold object\n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=123)\n",
    "\n",
    "    rmse_scores = []\n",
    "    \n",
    "    # Loop through each split\n",
    "    for train_index, test_index in kf.split(train):\n",
    "        cv_train, cv_test = train.iloc[train_index], train.iloc[test_index]\n",
    "    \n",
    "        # Train a Gradient Boosting model\n",
    "        gb = GradientBoostingRegressor(random_state=123, **params).fit(cv_train[features], cv_train.fare_amount)\n",
    "    \n",
    "        # Make predictions on the test data\n",
    "        pred = gb.predict(cv_test[features])\n",
    "    \n",
    "        fold_score = np.sqrt(mean_squared_error(cv_test['fare_amount'], pred))\n",
    "        rmse_scores.append(fold_score)\n",
    "    \n",
    "    return np.round(np.mean(rmse_scores) + np.std(rmse_scores), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possible max depth values\n",
    "max_depth_grid = [3, 6, 9, 12, 15]\n",
    "results = {}\n",
    "\n",
    "# For each value in the grid\n",
    "for max_depth_candidate in max_depth_grid:\n",
    "    # Specify parameters for the model\n",
    "    params = {'max_depth': max_depth_candidate}\n",
    "\n",
    "    # Calculate validation score for a particular hyperparameter\n",
    "    validation_score = get_cv_score(train, params)\n",
    "\n",
    "    # Save the results for each max depth value\n",
    "    results[max_depth_candidate] = validation_score   \n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "# Hyperparameter grids\n",
    "max_depth_grid = [3, 5, 7]\n",
    "subsample_grid = [0.8, 0.9, 1.0]\n",
    "results = {}\n",
    "\n",
    "# For each couple in the grid\n",
    "for max_depth_candidate, subsample_candidate in product(max_depth_grid, subsample_grid):\n",
    "    params = {'max_depth': max_depth_candidate,\n",
    "              'subsample': subsample_candidate}\n",
    "    validation_score = get_cv_score(train, params)\n",
    "    # Save the results for each couple\n",
    "    results[(max_depth_candidate, subsample_candidate)] = validation_score   \n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model blending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "\n",
    "# Train a Gradient Boosting model\n",
    "gb = GradientBoostingRegressor().fit(train[features], train.fare_amount)\n",
    "\n",
    "# Train a Random Forest model\n",
    "rf = RandomForestRegressor().fit(train[features], train.fare_amount)\n",
    "\n",
    "# Make predictions on the test data\n",
    "test['gb_pred'] = gb.predict(test[features])\n",
    "test['rf_pred'] = rf.predict(test[features])\n",
    "\n",
    "# Find mean of model predictions\n",
    "test['blend'] = (test['gb_pred'] +test['rf_pred']) / 2\n",
    "print(test[['gb_pred', 'rf_pred', 'blend']].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model stacking I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "\n",
    "# Split train data into two parts\n",
    "part_1, part_2 = train_test_split(train, test_size=0.5, random_state=123)\n",
    "\n",
    "# Train a Gradient Boosting model on Part 1\n",
    "gb = GradientBoostingRegressor().fit(part_1[features], part_1.fare_amount)\n",
    "\n",
    "# Train a Random Forest model on Part 1\n",
    "rf = RandomForestRegressor().fit(part_1[features], part_1.fare_amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the Part 2 data\n",
    "part_2['gb_pred'] = gb.predict(part_2[features])\n",
    "part_2['rf_pred'] = rf.predict(part_2[features])\n",
    "\n",
    "# Make predictions on the test data\n",
    "test['gb_pred'] = gb.predict(test[features])\n",
    "test['rf_pred'] = rf.predict(test[features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model stacking II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Create linear regression model without the intercept\n",
    "lr = LinearRegression(fit_intercept=False)\n",
    "\n",
    "# Train 2nd level model in the part_2 data\n",
    "lr.fit(part_2[['gb_pred', 'rf_pred']], part_2.fare_amount)\n",
    "\n",
    "# Make stacking predictions on the test data\n",
    "test['stacking'] = lr.predict(test[['gb_pred', 'rf_pred']])\n",
    "\n",
    "# Look at the model coefficients\n",
    "print(lr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Kaggle forum ideas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cv_score(train):\n",
    "    features = ['pickup_longitude', 'pickup_latitude',\n",
    "            'dropoff_longitude', 'dropoff_latitude',\n",
    "            'passenger_count', 'distance_km', 'hour', 'weird_feature']\n",
    "    \n",
    "    features = [x for x in features if x in train.columns]\n",
    "  \n",
    "    # Create KFold object\n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=123)\n",
    "\n",
    "    rmse_scores = []\n",
    "    \n",
    "    # Loop through each split\n",
    "    for train_index, test_index in kf.split(train):\n",
    "        cv_train, cv_test = train.iloc[train_index], train.iloc[test_index]\n",
    "    \n",
    "        # Train a Gradient Boosting model\n",
    "        gb = GradientBoostingRegressor(random_state=123).fit(cv_train[features], cv_train.fare_amount)\n",
    "    \n",
    "        # Make predictions on the test data\n",
    "        pred = gb.predict(cv_test[features])\n",
    "    \n",
    "        fold_score = np.sqrt(mean_squared_error(cv_test['fare_amount'], pred))\n",
    "        rmse_scores.append(fold_score)\n",
    "    \n",
    "    return np.round(np.mean(rmse_scores) + np.std(rmse_scores), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete passenger_count column\n",
    "new_train_1 = train.drop('passenger_count', axis=1)\n",
    "\n",
    "# Compare validation scores\n",
    "initial_score = get_cv_score(train)\n",
    "new_score = get_cv_score(new_train_1)\n",
    "\n",
    "print('Initial score is {} and the new score is {}'.format(initial_score, new_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create copy of the initial train DataFrame\n",
    "# new_train_2 = train.copy()\n",
    "\n",
    "# # Find sum of pickup latitude and ride distance\n",
    "# new_train_2['weird_feature'] = new_train_2.pickup_latitude + new_train_2.distance_km\n",
    "\n",
    "# # Compare validation scores\n",
    "# initial_score = get_cv_score(train)\n",
    "# new_score = get_cv_score(new_train_2)\n",
    "\n",
    "# print('Initial score is {} and the new score is {}'.format(initial_score, new_score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
