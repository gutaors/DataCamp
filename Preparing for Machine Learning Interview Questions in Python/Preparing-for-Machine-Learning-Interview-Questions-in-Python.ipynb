{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The hunt for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "loan_data = pd.read_csv('LoansTrainingSetReduced.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loan Status                         0\n",
      "Current Loan Amount                 0\n",
      "Term                                0\n",
      "Credit Score                    21338\n",
      "Years in current job                0\n",
      "Home Ownership                      0\n",
      "Annual Income                   21338\n",
      "Purpose                             0\n",
      "Monthly Debt                        0\n",
      "Years of Credit History             0\n",
      "Months since last delinquent    48506\n",
      "Number of Open Accounts             0\n",
      "Number of Credit Problems           0\n",
      "Current Credit Balance              0\n",
      "Maximum Open Credit                 0\n",
      "Bankruptcies                      192\n",
      "Tax Liens                           8\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print missing values\n",
    "#print(loan_data.isnull().sum())\n",
    "print(loan_data.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0\n"
     ]
    }
   ],
   "source": [
    "# Drop rows with missing values\n",
    "dropNArows = loan_data.dropna(axis = 0)\n",
    "\n",
    "# Print percentage of rows remaining\n",
    "print(dropNArows.shape[0]/loan_data.shape[0] * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0\n"
     ]
    }
   ],
   "source": [
    "# Drop columns with missing values\n",
    "dropNAcols = loan_data.dropna(axis = 1)\n",
    "\n",
    "# Print percentage of columns remaining\n",
    "print(dropNAcols.shape[1]/loan_data.shape[1] * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values with zero\n",
    "loan_data_filled = loan_data.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_object_dtype_isnan' from 'sklearn.utils.fixes' (C:\\Users\\94275793668\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-45bc095928db>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Import imputer module\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimpute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSimpleImputer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Subset numeric features: numeric_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mnumeric_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloan_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_dtypes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minclude\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\impute\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;34m\"\"\"Transformers for missing value imputation\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_base\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMissingIndicator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSimpleImputer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m __all__ = [\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\impute\\_base.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFLOAT_DTYPES\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfixes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_object_dtype_isnan\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mis_scalar_nan\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name '_object_dtype_isnan' from 'sklearn.utils.fixes' (C:\\Users\\94275793668\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py)"
     ]
    }
   ],
   "source": [
    "# Import imputer module\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Subset numeric features: numeric_cols\n",
    "numeric_cols = loan_data.select_dtypes(include=[np.number])\n",
    "\n",
    "# Impute with mean\n",
    "imp_mean = SimpleImputer(strategy='mean')\n",
    "loans_imp_mean = imp_mean.fit_transform(numeric_cols)\n",
    "\n",
    "# Convert returned array to DataFrame\n",
    "loans_imp_meanDF = pd.DataFrame(loans_imp_mean, columns=numeric_cols.columns)\n",
    "\n",
    "# Check the DataFrame's info\n",
    "print(loans_imp_meanDF.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterative imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicitly require this experimental feature\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "# Now you can import normally from sklearn.impute\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# Subset numeric features: numeric_cols\n",
    "numeric_cols = loan_data.select_dtypes(include=[np.number])\n",
    "\n",
    "# Iteratively impute\n",
    "imp_iter = IterativeImputer(max_iter=5, sample_posterior=True, random_state=123)\n",
    "loans_imp_iter = imp_iter.fit_transform(numeric_cols)\n",
    "\n",
    "# Convert returned array to DataFrame\n",
    "loans_imp_iterDF = pd.DataFrame(loans_imp_iter, columns=numeric_cols.columns)\n",
    "\n",
    "# Check the DataFrame's info\n",
    "print(loans_imp_iterDF.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data['Loan Status'] = np.random.binomial(1, 0.71, loan_data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/test distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "# Create `loan_data` subset: loan_data_subset\n",
    "loan_data_subset = loan_data[['Credit Score','Annual Income','Loan Status']]\n",
    "\n",
    "# Create train and test sets\n",
    "trainingSet, testSet = train_test_split(loan_data_subset, test_size=0.2, random_state=123)\n",
    "\n",
    "# Examine pairplots\n",
    "plt.figure()\n",
    "sns.pairplot(trainingSet, hue='Loan Status', palette='RdBu')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "sns.pairplot(testSet, hue='Loan Status', palette='RdBu')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log and power transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset loan_data\n",
    "cr_yrs = loan_data['Years of Credit History']\n",
    "\n",
    "# Histogram and kernel density estimate\n",
    "plt.figure()\n",
    "sns.distplot(cr_yrs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import boxcox\n",
    "# Subset loan_data\n",
    "cr_yrs = loan_data['Years of Credit History']\n",
    "\n",
    "# Box-Cox transformation\n",
    "cr_yrs_log = boxcox(cr_yrs, lmbda=0.0)\n",
    "\n",
    "# Histogram and kernel density estimate\n",
    "plt.figure()\n",
    "sns.distplot(cr_yrs_log)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset loan_data\n",
    "cr_yrs = loan_data['Years of Credit History']\n",
    "\n",
    "# Square root transform\n",
    "cr_yrs_sqrt = boxcox(cr_yrs, lmbda=0.5)\n",
    "\n",
    "# Histogram and kernel density estimate\n",
    "plt.figure()\n",
    "sns.distplot(cr_yrs_sqrt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate and multivariate boxplots\n",
    "fig, ax =plt.subplots(1,2)\n",
    "sns.boxplot(y=loan_data['Annual Income'], ax=ax[0])\n",
    "sns.boxplot(x='Loan Status', y='Annual Income', data=loan_data, ax=ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multivariate boxplot\n",
    "fig, ax =plt.subplots(1,2)\n",
    "sns.boxplot(y=loan_data['Monthly Debt'], ax=ax[0])\n",
    "sns.boxplot(x='Loan Status', y='Monthly Debt', data=loan_data, ax=ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate and multivariate boxplots\n",
    "fig, ax =plt.subplots(1,2)\n",
    "sns.boxplot(x=loan_data['Years of Credit History'], ax=ax[0])\n",
    "sns.boxplot(x='Loan Status', y='Years of Credit History', data=loan_data, ax=ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "categoric_cols = loan_data[[i for i in loan_data.columns if i not in numeric_cols.columns]]\n",
    "\n",
    "# Print: before dropping\n",
    "print(numeric_cols.mean())\n",
    "# print(numeric_cols.median())\n",
    "# print(numeric_cols.max())\n",
    "\n",
    "# Create index of rows to keep\n",
    "idx = (np.abs(stats.zscore(numeric_cols)) < 3).all(axis=1)\n",
    "\n",
    "# Concatenate numeric and categoric subsets\n",
    "ld_out_drop = pd.concat([numeric_cols.loc[idx], categoric_cols.loc[idx]], axis=1)\n",
    "\n",
    "# Print: after dropping\n",
    "print(ld_out_drop.mean())\n",
    "# print(ld_out_drop.median())\n",
    "# print(ld_out_drop.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mstats\n",
    "# Print: before winsorize\n",
    "print((loan_data['Monthly Debt']).mean())\n",
    "# print((loan_data['Monthly Debt']).median())\n",
    "# print((loan_data['Monthly Debt']).max())\n",
    "\n",
    "# Winsorize numeric columns\n",
    "debt_win =  mstats.winsorize(loan_data['Monthly Debt'], limits=[0.05, 0.05])\n",
    "\n",
    "# Convert to DataFrame, reassign column name\n",
    "debt_out = pd.DataFrame(debt_win, columns=['Monthly Debt'])\n",
    "\n",
    "# Print: after winsorize\n",
    "print(debt_out.mean())\n",
    "# print(debt_out.median())\n",
    "# print(debt_out.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print: before replace with median\n",
    "print((loan_data['Monthly Debt']).mean())\n",
    "# print((loan_data['Monthly Debt']).median())\n",
    "# print((loan_data['Monthly Debt']).max())\n",
    "\n",
    "# Find median\n",
    "median = loan_data.loc[loan_data['Monthly Debt'] < 2120, 'Monthly Debt'].median()\n",
    "loan_data['Monthly Debt'] = np.where(loan_data['Monthly Debt'] > 2120, median, loan_data['Monthly Debt'])\n",
    "\n",
    "# Print: after replace with median\n",
    "print((loan_data['Monthly Debt']).mean())\n",
    "# print((loan_data['Monthly Debt']).median())\n",
    "# print((loan_data['Monthly Debt']).max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Z-score standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# Subset features\n",
    "numeric_cols = loan_data.select_dtypes(include=[np.number])\n",
    "categoric_cols = loan_data.select_dtypes(include=[object])\n",
    "\n",
    "# Instantiate\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform, convert to DF\n",
    "numeric_cols_scaled = scaler.fit_transform(numeric_cols)\n",
    "numeric_cols_scaledDF = pd.DataFrame(numeric_cols_scaled, columns=numeric_cols.columns)\n",
    "\n",
    "# Concatenate categoric columns to scaled numeric columns\n",
    "final_DF = pd.concat([numeric_cols_scaledDF, categoric_cols], axis =1)\n",
    "final_DF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter and wrapper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = pd.read_csv('diabetes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create correlation matrix and print it\n",
    "cor = diabetes.corr()\n",
    "print(cor)\n",
    "\n",
    "# Correlation matrix heatmap\n",
    "plt.figure()\n",
    "sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\n",
    "plt.show()\n",
    "\n",
    "# Correlation with output variable\n",
    "cor_target = abs(cor[\"progression\"])\n",
    "cor_target\n",
    "# Selecting highly correlated features\n",
    "best_features = cor_target[cor_target> 0.5]\n",
    "print(best_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = diabetes.drop('progression', axis =1)\n",
    "y = diabetes['progression']\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "# Instantiate estimator and feature selector\n",
    "svr_mod = SVR(kernel=\"linear\")\n",
    "feat_selector = RFECV(svr_mod, cv=5)\n",
    "\n",
    "# Fit\n",
    "feat_selector = feat_selector.fit(X, y)\n",
    "\n",
    "# Print support and ranking\n",
    "print(feat_selector.support_)\n",
    "print(feat_selector.ranking_)\n",
    "print(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from sklearn.linear_model import LarsCV\n",
    "\n",
    "# Drop feature suggested not important in step 2\n",
    "X = X.drop('sex', axis=1)\n",
    "\n",
    "# Instantiate\n",
    "lars_mod = LarsCV(cv=5, normalize=False)\n",
    "\n",
    "# Fit\n",
    "feat_selector = lars_mod.fit(X,y)\n",
    "\n",
    "# Print r-squared score and estimated alpha\n",
    "print(lars_mod.score(X, y))\n",
    "print(lars_mod.alpha_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection through feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Instantiate\n",
    "rf_mod = RandomForestRegressor(max_depth=2, random_state=123, \n",
    "              n_estimators=100, oob_score=True)\n",
    "\n",
    "# Fit\n",
    "rf_mod.fit(X, y)\n",
    "\n",
    "# Print\n",
    "print(diabetes.columns)\n",
    "print(rf_mod.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "# Instantiate\n",
    "xt_mod = ExtraTreesRegressor(n_estimators=10)\n",
    "\n",
    "# Fit\n",
    "xt_mod.fit(X,y)\n",
    "\n",
    "# Print\n",
    "print(diabetes.columns)\n",
    "print(xt_mod.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from sklearn.linear_model import Lasso, LassoCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=123, test_size=0.3)\n",
    "\n",
    "# Instantiate cross-validated lasso, fit\n",
    "lasso_cv = LassoCV(alphas=None, cv=10, max_iter=10000)\n",
    "lasso_cv.fit(X_train, y_train) \n",
    "\n",
    "# Instantiate lasso, fit, predict and print MSE\n",
    "lasso = Lasso(alpha = lasso_cv.alpha_)\n",
    "lasso.fit(X_train, y_train)\n",
    "print(mean_squared_error(y_true=y_test, y_pred=lasso.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=123, test_size=0.3)\n",
    "\n",
    "# Instantiate cross-validated ridge, fit\n",
    "ridge_cv = RidgeCV(alphas=np.logspace(-6, 6, 13))\n",
    "ridge_cv.fit(X_train, y_train)\n",
    "\n",
    "# Instantiate ridge, fit, predict and print MSE\n",
    "ridge = Ridge(alpha = ridge_cv.alpha_)\n",
    "ridge.fit(X_train, y_train)\n",
    "print(mean_squared_error(y_true=y_test, y_pred=ridge.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression baseline classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Create X matrix and y array\n",
    "X = loan_data.drop(\"Loan Status\", axis=1)\n",
    "y = loan_data[\"Loan Status\"]\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123)\n",
    "\n",
    "# Instantiate\n",
    "logistic = LogisticRegression(solver = 'liblinear')\n",
    "\n",
    "# Fit\n",
    "logistic.fit(X_train, y_train)\n",
    "\n",
    "# Predict and print accuracy\n",
    "print(accuracy_score(y_true=y_test, y_pred=logistic.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dti_ratio variable\n",
    "# monthly_income = loan_data[\"Annual Income\"]/12\n",
    "# loan_data[\"dti_ratio\"] = loan_data[\"Monthly Debt\"]/monthly_income * 100\n",
    "# loan_data = loan_data.drop([\"Monthly Debt\",\"Annual Income\"], axis=1)\n",
    "\n",
    "# Replace target variable levels\n",
    "# loan_data[\"Loan Status\"] = loan_data[\"Loan Status\"].replace({'Fully Paid': 0, \n",
    "#                                             'Charged Off': 1})\n",
    "\n",
    "# One-hot encode categorical variables\n",
    "loan_data = pd.get_dummies(data=loan_data)\n",
    "\n",
    "# Print\n",
    "loan_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X matrix and y array\n",
    "X = loan_data.drop(\"Loan Status\", axis=1)\n",
    "y = loan_data[\"Loan Status\"]\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123)\n",
    "\n",
    "# Instantiate\n",
    "logistic_dti = LogisticRegression(solver='liblinear')\n",
    "\n",
    "# Fit\n",
    "logistic_dti.fit(X_train, y_train)\n",
    "\n",
    "# Predict and print accuracy\n",
    "print(accuracy_score(y_true=y_test, y_pred=logistic_dti.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrap aggregation (bagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier\n",
    "# Instantiate bootstrap aggregation model\n",
    "bagged_model = BaggingClassifier(n_estimators=50, random_state=123)\n",
    "\n",
    "# Fit\n",
    "bagged_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "bagged_pred = bagged_model.predict(X_test)\n",
    "\n",
    "# Print accuracy score\n",
    "print(accuracy_score(y_test, bagged_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boosting model\n",
    "boosted_model = AdaBoostClassifier(n_estimators=50, random_state=123)\n",
    "\n",
    "# Fit\n",
    "boosted_fit = boosted_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "boosted_pred = boosted_model.predict(X_test)\n",
    "\n",
    "# Print model accuracy\n",
    "print(accuracy_score(y_test, boosted_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier \n",
    "# Instantiate\n",
    "xgb = XGBClassifier(random_state=123, learning_rate=0.1, n_estimators=10, max_depth=3)\n",
    "\n",
    "# Fit\n",
    "xgb_fit = xgb.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "xgb_pred = xgb.predict(X_test)\n",
    "\n",
    "# Print accuracy score\n",
    "print('Final prediction score: [%.8f]' % accuracy_score(y_test, xgb_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal component analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import module\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Feature matrix and target array\n",
    "X = diabetes.drop('progression', axis=1)\n",
    "y = diabetes['progression']\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=3)\n",
    "\n",
    "# Fit and transform\n",
    "principalComponents = pca.fit_transform(X)\n",
    "\n",
    "# Print ratio of variance explained\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular value decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import module\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Feature matrix and target array\n",
    "X = diabetes.drop('progression', axis=1)\n",
    "y = diabetes['progression']\n",
    "\n",
    "# SVD\n",
    "svd = TruncatedSVD(n_components=3)\n",
    "\n",
    "# Fit and transform\n",
    "principalComponents = svd.fit_transform(X)\n",
    "\n",
    "# Print ratio of variance explained\n",
    "print(svd.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization separation of classes with PCA I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature matrix and target array\n",
    "X = loan_data.drop('Loan Status', axis=1)\n",
    "y = loan_data['Loan Status']\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=3)\n",
    "\n",
    "# Fit and transform\n",
    "principalComponents = pca.fit_transform(X)\n",
    "principalComponents = pd.DataFrame(principalComponents, columns = ['principal component 1', 'principal component 2', 'principal component 3'])\n",
    "loan_data_PCA = pd.concat([principalComponents, y], axis =1)\n",
    "loan_data_PCA.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [0, 1]\n",
    "colors = ['r', 'b']\n",
    "\n",
    "\n",
    "# For loop to create plot\n",
    "for target, color in zip(targets,colors):\n",
    "    indicesToKeep = loan_data_PCA['Loan Status'] == target\n",
    "    plt.scatter(loan_data_PCA.loc[indicesToKeep, 'principal component 1']\n",
    "               , loan_data_PCA.loc[indicesToKeep, 'principal component 2']\n",
    "               , c = color\n",
    "               , s = 50)\n",
    "\n",
    "# Legend    \n",
    "plt.legend(targets)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization PCs with a scree plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove target variable\n",
    "X = loan_data.drop('Loan Status', axis=1)\n",
    "\n",
    "# Instantiate\n",
    "pca = PCA(n_components=10)\n",
    "\n",
    "# Fit and transform\n",
    "principalComponents = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List principal components names\n",
    "principal_components = ['PC1','PC2','PC3','PC4','PC5','PC6','PC7','PC8','PC9','PC10']\n",
    "\n",
    "# Create a DataFrame\n",
    "pca_df = pd.DataFrame({'Variance Explained': pca.explained_variance_ratio_,\n",
    "             'PC':principal_components})\n",
    "\n",
    "# Plot DataFrame\n",
    "sns.barplot(x='PC',y='Variance Explained', \n",
    "           data=pca_df, color=\"c\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate, fit and transform\n",
    "pca2 = PCA()\n",
    "principalComponents2 = pca2.fit_transform(X)\n",
    "# Assign variance explained\n",
    "var = pca2.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cumulative variance\n",
    "cumulative_var = np.cumsum(var)*100\n",
    "plt.plot(cumulative_var,'k-o',markerfacecolor='None',markeredgecolor='k')\n",
    "plt.title('Principal Component Analysis',fontsize=12)\n",
    "plt.xlabel(\"Principal Component\",fontsize=12)\n",
    "plt.ylabel(\"Cumulative Proportion of Variance Explained\",fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import module\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Create feature matrix\n",
    "X = diabetes.drop(\"progression\", axis=1)\n",
    "\n",
    "# Instantiate\n",
    "kmeans = KMeans(n_clusters=2, random_state=123)\n",
    "\n",
    "# Fit\n",
    "fit = kmeans.fit(X)\n",
    "\n",
    "# Print inertia\n",
    "print(\"Sum of squared distances for 2 clusters is\", kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate\n",
    "kmeans = KMeans(n_clusters=5, random_state=123)\n",
    "\n",
    "# Fit\n",
    "fit = kmeans.fit(X)\n",
    "\n",
    "# Print inertia\n",
    "print(\"Sum of squared distances for 5 clusters is\", kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Instantiate\n",
    "kmeans = KMeans(n_clusters=10, random_state=123)\n",
    "\n",
    "# Fit\n",
    "fit = kmeans.fit(X)\n",
    "\n",
    "# Print inertia\n",
    "print(\"Sum of squared distances for 10 clusters is\", kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate\n",
    "kmeans = KMeans(n_clusters=20, random_state=123)\n",
    "\n",
    "# Fit\n",
    "fit = kmeans.fit(X)\n",
    "\n",
    "# Print inertia\n",
    "print(\"Sum of squared distances for 20 clusters is\", kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical agglomerative clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import hierarchical clustering libraries\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Create dendrogram\n",
    "dendrogram = sch.dendrogram(sch.linkage(X, method='ward'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create clusters and fit\n",
    "hc = AgglomerativeClustering(affinity = 'euclidean', linkage = 'ward')\n",
    "hc.fit(X)\n",
    "\n",
    "# Print number of clusters\n",
    "print(hc.n_clusters_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silhouette method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Feature matrix\n",
    "X = diabetes.drop(\"progression\", axis=1)\n",
    "\n",
    "# For loop\n",
    "for n_clusters in range(2, 9):\n",
    "    kmeans = KMeans(n_clusters=n_clusters)\n",
    "    # Fit and predict your k-Means object\n",
    "    preds = kmeans.fit_predict(X)\n",
    "    score = silhouette_score(X, preds, metric='euclidean')\n",
    "    print (\"For n_clusters = {}, silhouette score is {})\".format(n_clusters, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elbow method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty list\n",
    "sum_of_squared_distances = []\n",
    "\n",
    "# Create for loop\n",
    "for k in range(1,15):\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    kmeans = kmeans.fit(X)\n",
    "    sum_of_squared_distances.append(kmeans.inertia_)\n",
    "\n",
    "# Plot\n",
    "plt.plot(range(1,15), sum_of_squared_distances, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Sum of squared distances')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create X matrix and y array\n",
    "X = loan_data.drop(\"Loan Status\", axis=1)\n",
    "y = loan_data[\"Loan Status\"]\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=123)\n",
    "\n",
    "# Instantiate, Fit, Predict\n",
    "loans_clf = DecisionTreeClassifier() \n",
    "loans_clf.fit(X_train, y_train)\n",
    "y_pred = loans_clf.predict(X_test)\n",
    "\n",
    "# Evaluation metric\n",
    "print(\"Decision Tree Accuracy: {}\".format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create the hyperparameter grid\n",
    "param_grid = {\"criterion\": [\"gini\"], \"min_samples_split\": [2, 10, 20], \n",
    "              \"max_depth\": [None, 2, 5, 10]}\n",
    "\n",
    "# Instantiate classifier and GridSearchCV, fit\n",
    "loans_clf = DecisionTreeClassifier()\n",
    "dtree_cv = GridSearchCV(loans_clf, param_grid=param_grid, cv=5)\n",
    "fit = dtree_cv.fit(X_train, y_train)\n",
    "\n",
    "# Print the optimal parameters and best score\n",
    "print(\"Tuned Decision Tree Parameter: {}\".format(dtree_cv.best_params_))\n",
    "print(\"Tuned Decision Tree Accuracy: {}\".format(dtree_cv.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A forest of decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=123)\n",
    "\n",
    "# Instantiate, Fit, Predict\n",
    "loans_rf = RandomForestClassifier(n_estimators=10) \n",
    "loans_rf.fit(X_train, y_train)\n",
    "y_pred = loans_rf.predict(X_test)\n",
    "\n",
    "# Evaluation metric\n",
    "print(\"Random Forest Accuracy: {}\".format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create the hyperparameter grid\n",
    "param_grid = {\"criterion\": [\"gini\"], \"min_samples_split\": [2, 10, 20], \n",
    "              \"max_depth\": [None, 2, 5, 10],\"max_features\": [10, 20, 30]}\n",
    "\n",
    "# Instantiate classifier and GridSearchCV, fit\n",
    "loans_rf = RandomForestClassifier(n_estimators=10)\n",
    "rf_cv = GridSearchCV(loans_rf, param_grid=param_grid, cv=5)\n",
    "fit = rf_cv.fit(X_train, y_train)\n",
    "\n",
    "# Print the optimal parameters and best score\n",
    "print(\"Tuned Random Forest Parameter: {}\".format(rf_cv.best_params_))\n",
    "print(\"Tuned Random Forest Accuracy: {}\".format(rf_cv.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imbalanced class metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Instantiate, fit, predict\n",
    "lr = LogisticRegression(solver='liblinear')\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Confusion matrix:\\n {}\".format(confusion_matrix(y_test, y_pred)))\n",
    "print(\"Accuracy: {}\".format(accuracy_score(y_test, y_pred)))\n",
    "print(\"Precision: {}\".format(precision_score(y_test, y_pred)))\n",
    "print(\"Recall: {}\".format(recall_score(y_test, y_pred)))\n",
    "print(\"F1: {}\".format(f1_score(y_test ,y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deny = loan_data[loan_data['Loan Status']==0]\n",
    "approve = loan_data[loan_data['Loan Status']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "# Upsample minority and combine with majority\n",
    "loans_upsampled = resample(deny, replace=True, n_samples=len(approve), random_state=123)\n",
    "upsampled = pd.concat([approve, loans_upsampled])\n",
    "\n",
    "# Downsample majority and combine with minority\n",
    "loans_downsampled = resample(approve, replace = False,  n_samples = len(deny), random_state = 123)\n",
    "downsampled = pd.concat([loans_downsampled, deny])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upsampled feature matrix and target array\n",
    "X_train_up = upsampled.drop('Loan Status', axis=1)\n",
    "y_train_up = upsampled['Loan Status']\n",
    "\n",
    "# Instantiate, fit, predict\n",
    "loan_lr_up = LogisticRegression(solver='liblinear')\n",
    "loan_lr_up.fit(X_train_up, y_train_up)\n",
    "upsampled_y_pred = loan_lr_up.predict(X_test)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Confusion matrix:\\n {}\".format(confusion_matrix(y_test, upsampled_y_pred)))\n",
    "print(\"Accuracy: {}\".format(accuracy_score(y_test, upsampled_y_pred)))\n",
    "print(\"Precision: {}\".format(precision_score(y_test, upsampled_y_pred)))\n",
    "print(\"Recall: {}\".format(recall_score(y_test, upsampled_y_pred)))\n",
    "print(\"F1: {}\".format(f1_score(y_test,upsampled_y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsampled feature matrix and target array\n",
    "X_train_down = downsampled.drop('Loan Status', axis=1)\n",
    "y_train_down = downsampled['Loan Status']\n",
    "\n",
    "# Instantiate, fit, predict\n",
    "loan_lr_down = LogisticRegression(solver='liblinear')\n",
    "loan_lr_down.fit(X_train_down, y_train_down)\n",
    "downsampled_y_pred = loan_lr_down.predict(X_test)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Confusion matrix:\\n {}\".format(confusion_matrix(y_test, downsampled_y_pred)))\n",
    "print(\"Accuracy: {}\".format(accuracy_score(y_test, downsampled_y_pred)))\n",
    "print(\"Precision: {}\".format(precision_score(y_test, downsampled_y_pred)))\n",
    "print(\"Recall: {}\".format(recall_score(y_test, downsampled_y_pred)))\n",
    "print(\"F1: {}\".format(f1_score(y_test, downsampled_y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multicollinearity techniques - feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature matrix and target array\n",
    "X = diabetes.drop('progression', axis=1)\n",
    "y = diabetes['progression']\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "# Instantiate, fit, predict\n",
    "lin_mod = LinearRegression()\n",
    "lin_mod.fit(X_train, y_train)\n",
    "y_pred = lin_mod.predict(X_test)\n",
    "\n",
    "# Coefficient estimates\n",
    "print('Coefficients: \\n', lin_mod.coef_)\n",
    "\n",
    "# Mean squared error\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(y_test, y_pred))\n",
    "\n",
    "# Explained variance score\n",
    "print('R_squared score: %.2f' % r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "diab_corr = diabetes.corr()\n",
    "\n",
    "# Generate correlation heatmap\n",
    "ax = sns.heatmap(diab_corr, center=0, cmap=sns.diverging_palette(20,220, n=256), square=True)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
    "plt.show()\n",
    "\n",
    "# Print correlations\n",
    "print(diab_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering\n",
    "diabetes['s1_s2'] = diabetes['s1']*diabetes['s2']\n",
    "diabetes = diabetes.drop(['s1','s2'], axis=1)\n",
    "\n",
    "# Print variable names\n",
    "print(diabetes.columns)\n",
    "\n",
    "# Train/test split\n",
    "X2 = diabetes.drop('progression', axis=1)\n",
    "y2 = diabetes['progression']\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=0.3, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate, fit, predict\n",
    "lin_mod2 = LinearRegression()\n",
    "lin_mod2.fit(X_train2, y_train2)\n",
    "y_pred2 = lin_mod2.predict(X_test2)\n",
    "\n",
    "# Coefficient estimates\n",
    "print('Coefficients: \\n', lin_mod2.coef_)\n",
    "\n",
    "# Mean squared error\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(y_test2, y_pred2))\n",
    "\n",
    "# Explained variance score\n",
    "print('R_squared score: %.2f' % r2_score(y_test2, y_pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multicollinearity techniques - PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Instantiate\n",
    "pca = PCA()\n",
    "\n",
    "# Fit on train\n",
    "pca.fit(X_train)\n",
    "\n",
    "# Transform train and test\n",
    "X_trainPCA = pca.transform(X_train)\n",
    "X_testPCA = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate, fit, predict\n",
    "LinRegr = LinearRegression()\n",
    "LinRegr.fit(X_trainPCA, y_train)\n",
    "predictions = LinRegr.predict(X_testPCA)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', LinRegr.coef_)\n",
    "\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\" % mean_squared_error(y_test, predictions))\n",
    "\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % r2_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "X_trainPCA = pd.DataFrame(X_trainPCA)\n",
    "diab_corrPCA = X_trainPCA.corr()\n",
    "\n",
    "# Generate correlation heatmap\n",
    "ax = sns.heatmap(diab_corrPCA, center=0, cmap=sns.diverging_palette(20,220, n=256), square=True)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
    "plt.show()\n",
    "\n",
    "# Print correlations\n",
    "print(diab_corrPCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X matrix and y array\n",
    "X = loan_data.drop(\"Loan Status\", axis=1)\n",
    "y = loan_data[\"Loan Status\"]\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=123)\n",
    "\n",
    "# Instantiate, fit, predict\n",
    "rf_model = RandomForestClassifier(n_estimators=50, random_state=123, oob_score = True)\n",
    "rf_fit = rf_model.fit(X_train, y_train)\n",
    "rf_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Random Forest Accuracy: {}\".format(accuracy_score(y_test, rf_pred)))\n",
    "print(\"Confusion matrix:\\n {}\".format(confusion_matrix(y_test, rf_pred)))\n",
    "print(\"Precision: {}\".format(precision_score(y_test, rf_pred)))\n",
    "print(\"Recall: {}\".format(recall_score(y_test, rf_pred)))\n",
    "print(\"F1: {}\".format(f1_score(y_test, rf_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Instantiate, fit, predict\n",
    "gb_model = GradientBoostingClassifier(n_estimators=50, learning_rate=0.01,random_state=123)\n",
    "gb_fit = gb_model.fit(X_train, y_train)\n",
    "gb_pred = gb_model.predict(X_test)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Gradient Boosting Accuracy: {}\".format(accuracy_score(y_test, gb_pred)))\n",
    "print(\"Confusion matrix:\\n {}\".format(confusion_matrix(y_test, gb_pred)))\n",
    "print(\"Precision: {}\".format(precision_score(y_test, gb_pred)))\n",
    "print(\"Recall: {}\".format(recall_score(y_test, gb_pred)))\n",
    "print(\"F1: {}\".format(f1_score(y_test, gb_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
