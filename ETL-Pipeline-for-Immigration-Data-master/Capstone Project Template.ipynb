{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Pipeline for Immigration Data\n",
    "\n",
    "## Project Summary\n",
    "The goal of the project is to develop an ETL pipeline for the immigration data. I plan to extract the data from three different data sources, transform the data using pyspark and pandas, and save the data in the parquet files. \n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Project Scope\n",
    "\n",
    "I want to examine the origin country and the destination city of the immigrats. I will use the i94 data together with the city demographic data and the country gapminder data. The end solution will be a fact table with the immigration data and two dimention tables with the details of the destination city and the origin country. I'll use both pandas and spark as the tool. \n",
    "\n",
    "#### Describe and Gather Data \n",
    "The city demographic data comes from OpenSoft. It contains the demographic information of US cities. The web address of the source is https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/. \n",
    "\n",
    "The country data comes from gapminder.org. It contains the fertility, life, population, child mortality, gdp and region of the countries. The web address of the source is https://www.gapminder.org/data/. \n",
    "\n",
    "The immigration data comes from the US National Tourism and Trade Office. The web address of the source is https://travel.trade.gov/research/reports/i94/historical/2016.html. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601.0</td>\n",
       "      <td>41862.0</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>30908.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129.0</td>\n",
       "      <td>49500.0</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147.0</td>\n",
       "      <td>32935.0</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040.0</td>\n",
       "      <td>46799.0</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819.0</td>\n",
       "      <td>8229.0</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>4759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127.0</td>\n",
       "      <td>87105.0</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821.0</td>\n",
       "      <td>33878.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040.0</td>\n",
       "      <td>143873.0</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829.0</td>\n",
       "      <td>86253.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City          State  Median Age  Male Population  \\\n",
       "0     Silver Spring       Maryland        33.8          40601.0   \n",
       "1            Quincy  Massachusetts        41.0          44129.0   \n",
       "2            Hoover        Alabama        38.5          38040.0   \n",
       "3  Rancho Cucamonga     California        34.5          88127.0   \n",
       "4            Newark     New Jersey        34.6         138040.0   \n",
       "\n",
       "   Female Population  Total Population  Number of Veterans  Foreign-born  \\\n",
       "0            41862.0             82463              1562.0       30908.0   \n",
       "1            49500.0             93629              4147.0       32935.0   \n",
       "2            46799.0             84839              4819.0        8229.0   \n",
       "3            87105.0            175232              5821.0       33878.0   \n",
       "4           143873.0            281913              5829.0       86253.0   \n",
       "\n",
       "   Average Household Size State Code                       Race  Count  \n",
       "0                    2.60         MD         Hispanic or Latino  25924  \n",
       "1                    2.39         MA                      White  58723  \n",
       "2                    2.58         AL                      Asian   4759  \n",
       "3                    3.18         CA  Black or African-American  24437  \n",
       "4                    2.73         NJ                      White  76402  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the csv file\n",
    "city = pd.read_csv('us-cities-demographics.csv', sep = ';')\n",
    "city.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Fertility</th>\n",
       "      <th>Life</th>\n",
       "      <th>Population</th>\n",
       "      <th>Child_Mortality</th>\n",
       "      <th>Gdp</th>\n",
       "      <th>Region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>4.900</td>\n",
       "      <td>60.947</td>\n",
       "      <td>34499915.0</td>\n",
       "      <td>96.7</td>\n",
       "      <td>1884.0</td>\n",
       "      <td>South Asia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Albania</td>\n",
       "      <td>1.771</td>\n",
       "      <td>77.392</td>\n",
       "      <td>3238316.0</td>\n",
       "      <td>14.9</td>\n",
       "      <td>9961.0</td>\n",
       "      <td>Europe &amp; Central Asia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>2.795</td>\n",
       "      <td>71.000</td>\n",
       "      <td>36983924.0</td>\n",
       "      <td>25.2</td>\n",
       "      <td>12893.0</td>\n",
       "      <td>Middle East &amp; North Africa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>Angola</td>\n",
       "      <td>5.863</td>\n",
       "      <td>51.899</td>\n",
       "      <td>20714494.0</td>\n",
       "      <td>167.1</td>\n",
       "      <td>7488.0</td>\n",
       "      <td>Sub-Saharan Africa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>Antigua and Barbuda</td>\n",
       "      <td>2.089</td>\n",
       "      <td>75.954</td>\n",
       "      <td>91404.0</td>\n",
       "      <td>8.7</td>\n",
       "      <td>20353.0</td>\n",
       "      <td>America</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Country  Fertility    Life  Population  Child_Mortality  \\\n",
       "49           Afghanistan      4.900  60.947  34499915.0             96.7   \n",
       "99               Albania      1.771  77.392   3238316.0             14.9   \n",
       "149              Algeria      2.795  71.000  36983924.0             25.2   \n",
       "199               Angola      5.863  51.899  20714494.0            167.1   \n",
       "249  Antigua and Barbuda      2.089  75.954     91404.0              8.7   \n",
       "\n",
       "         Gdp                      Region  \n",
       "49    1884.0                  South Asia  \n",
       "99    9961.0       Europe & Central Asia  \n",
       "149  12893.0  Middle East & North Africa  \n",
       "199   7488.0          Sub-Saharan Africa  \n",
       "249  20353.0                     America  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the csv file\n",
    "country = pd.read_csv('gapminder.csv')\n",
    "country.columns = [c.title() for c in country.columns] \n",
    "country = country[country.Year == 2013].drop('Year', axis =1)\n",
    "country.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2027561</th>\n",
       "      <td>4084316.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>HHW</td>\n",
       "      <td>20566.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>HI</td>\n",
       "      <td>20573.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1955.0</td>\n",
       "      <td>07202016</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>JL</td>\n",
       "      <td>5.658267e+10</td>\n",
       "      <td>00782</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2171295</th>\n",
       "      <td>4422636.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>MCA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>TX</td>\n",
       "      <td>20568.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>10222016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>*GA</td>\n",
       "      <td>9.436200e+10</td>\n",
       "      <td>XBLNG</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589494</th>\n",
       "      <td>1195600.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>OGG</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>FL</td>\n",
       "      <td>20571.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1940.0</td>\n",
       "      <td>07052016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LH</td>\n",
       "      <td>5.578047e+10</td>\n",
       "      <td>00464</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2631158</th>\n",
       "      <td>5291768.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20572.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>20581.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>10272016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>QR</td>\n",
       "      <td>9.478970e+10</td>\n",
       "      <td>00739</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3032257</th>\n",
       "      <td>985523.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>CHM</td>\n",
       "      <td>20550.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NY</td>\n",
       "      <td>20553.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>07042016</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.232257e+10</td>\n",
       "      <td>LAND</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  i94mode  \\\n",
       "2027561  4084316.0  2016.0     4.0   209.0   209.0     HHW  20566.0      1.0   \n",
       "2171295  4422636.0  2016.0     4.0   582.0   582.0     MCA  20567.0      1.0   \n",
       "589494   1195600.0  2016.0     4.0   148.0   112.0     OGG  20551.0      1.0   \n",
       "2631158  5291768.0  2016.0     4.0   297.0   297.0     LOS  20572.0      1.0   \n",
       "3032257   985523.0  2016.0     4.0   111.0   111.0     CHM  20550.0      3.0   \n",
       "\n",
       "        i94addr  depdate   ...     entdepu  matflag  biryear   dtaddto gender  \\\n",
       "2027561      HI  20573.0   ...         NaN        M   1955.0  07202016      F   \n",
       "2171295      TX  20568.0   ...         NaN        M   1990.0  10222016      M   \n",
       "589494       FL  20571.0   ...         NaN        M   1940.0  07052016      M   \n",
       "2631158      CA  20581.0   ...         NaN        M   1991.0  10272016      M   \n",
       "3032257      NY  20553.0   ...         NaN        M   1997.0  07042016      F   \n",
       "\n",
       "        insnum airline        admnum  fltno visatype  \n",
       "2027561    NaN      JL  5.658267e+10  00782       WT  \n",
       "2171295    NaN     *GA  9.436200e+10  XBLNG       B2  \n",
       "589494     NaN      LH  5.578047e+10  00464       WT  \n",
       "2631158    NaN      QR  9.478970e+10  00739       B2  \n",
       "3032257    NaN     NaN  4.232257e+10   LAND       WT  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the csv file\n",
    "immigration_data_sample = pd.read_csv('immigration_data_sample.csv', index_col = 0)\n",
    "immigration_data_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "'Unsupported class file major version 56'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o102.parquet.\n: java.lang.IllegalArgumentException: Unsupported class file major version 56\n\tat org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:166)\n\tat org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:148)\n\tat org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:136)\n\tat org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:237)\n\tat org.apache.spark.util.ClosureCleaner$.getClassReader(ClosureCleaner.scala:49)\n\tat org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:517)\n\tat org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:500)\n\tat scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)\n\tat scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:134)\n\tat scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:134)\n\tat scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:236)\n\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)\n\tat scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:134)\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)\n\tat org.apache.spark.util.FieldAccessFinder$$anon$3.visitMethodInsn(ClosureCleaner.scala:500)\n\tat org.apache.xbean.asm6.ClassReader.readCode(ClassReader.java:2175)\n\tat org.apache.xbean.asm6.ClassReader.readMethod(ClassReader.java:1238)\n\tat org.apache.xbean.asm6.ClassReader.accept(ClassReader.java:631)\n\tat org.apache.xbean.asm6.ClassReader.accept(ClassReader.java:355)\n\tat org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(ClosureCleaner.scala:307)\n\tat org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(ClosureCleaner.scala:306)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:306)\n\tat org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:162)\n\tat org.apache.spark.SparkContext.clean(SparkContext.scala:2326)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2100)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.mergeSchemasInParallel(ParquetFileFormat.scala:633)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.inferSchema(ParquetFileFormat.scala:241)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$6.apply(DataSource.scala:180)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$6.apply(DataSource.scala:180)\n\tat scala.Option.orElse(Option.scala:289)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:179)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:373)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:645)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:567)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:835)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-fc94c3faf4ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0menableHiveSupport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mimmigration_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sas_data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, *paths)\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'string'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'year'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'month'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'day'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \"\"\"\n\u001b[0;32m--> 316\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: 'Unsupported class file major version 56'"
     ]
    }
   ],
   "source": [
    "# create a spark session and read in the sas data\n",
    "spark = SparkSession.builder\\\n",
    "        .config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    "        .enableHiveSupport()\\\n",
    "        .getOrCreate()\n",
    "immigration_data = spark.read.parquet(\"sas_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "immigration_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "immigration_data.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_data_quality(df):\n",
    "    print('The percentage of missing values in each column is...')\n",
    "    print(df.isnull().mean().sort_values(ascending = False))\n",
    "    print('\\n')\n",
    "    print('The number of duplicated rows is...')\n",
    "    print(df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assess_data_quality(city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assess_data_quality(country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assess_data_quality(immigration_data_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the data type of the immigration_data\n",
    "immigration_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Findings:\n",
    "- The city dataframe has few missing values. \n",
    "- The Gdp and Child Mortality columns of the country dataframe have missing values, but less than 10%.\n",
    "- The immigration_data_sample dataframe, assessed in place of the immigration_data, has a few columns ('entdepu', 'occup', 'insnum', 'visapost') with more than 50% of the missing values, which need to be removed.  \n",
    "- The immigration_data has data type issues. 'i94yr','i94mon','i94bir','biryear', and 'admnum' columns should be integers not doubles. 'i94res', 'i94visa', and 'i94mode' columns should be first converted to integers and then to strings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_data_type(immigration_data):\n",
    "    '''\n",
    "    The function updates the data types of the columns\n",
    "    '''\n",
    "    # Select the columns of interest\n",
    "    immigration_table = immigration_data.select(['i94yr','i94mon','i94res','i94port','i94addr',\n",
    "                                                 'i94visa','i94mode','i94bir','biryear','gender', \n",
    "                                                 'visatype', 'airline', 'fltno','admnum'])\n",
    "    # Change the data type from double to int\n",
    "    for col_name in ['i94yr','i94mon','i94bir','biryear', 'admnum', 'i94res', 'i94visa','i94mode']:\n",
    "        immigration_table = immigration_table.withColumn(col_name, col(col_name).cast('int'))\n",
    "    \n",
    "    # Change the data ype from double to string\n",
    "    for col_name in ['i94res', 'i94visa','i94mode']:\n",
    "        immigration_table = immigration_table.withColumn(col_name, col(col_name).cast('string'))   \n",
    "    \n",
    "    return immigration_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_destination_city(city, immigration_table):\n",
    "    '''\n",
    "    The function uses city and immigration_table as the source and \n",
    "    outputs the destination city demographic dataframe.\n",
    "    '''\n",
    "    # move the race counts of the same city to one row\n",
    "    race_count = city[['City', 'State', 'Race', 'Count']]\n",
    "    race_count_pivot = pd.pivot_table(race_count, index = ['City', 'State'], \n",
    "                                      columns = 'Race', values = 'Count').reset_index()\n",
    "    city = city.drop(['Race', 'Count'], axis =1)\n",
    "    city = pd.merge(city, race_count_pivot, on = ['City', 'State'])\n",
    "    city = city.drop_duplicates()    \n",
    "\n",
    "    # create a dictionary with the i94port and the city/state code\n",
    "    destination_airport = {}\n",
    "    with open('destination_airport.txt') as f:\n",
    "        for line in f:\n",
    "            key = line.split('=')[0].replace(\"'\",\"\").strip()\n",
    "            value = line.split('=')[1].replace(\"'\",\"\").strip()\n",
    "            destination_airport[key] = value\n",
    "\n",
    "    # pull i94port information from immigration_table\n",
    "    i94port = [i.i94port for i in immigration_table.select('i94port').distinct().collect()]\n",
    "\n",
    "    # find the city and state code and create a data frame \n",
    "    city_and_state_code = [destination_airport[i] for i in i94port]\n",
    "    destination = pd.DataFrame({'city_and_state_code': city_and_state_code})\n",
    "    destination = destination.city_and_state_code.str.split(',', expand = True)\n",
    "    destination.columns = ['City', 'State Code', 'other']\n",
    "    destination['City'] = destination['City'].str.title().str.strip()\n",
    "    destination['State Code'] = destination['State Code'].str.strip()\n",
    "    destination = destination.drop('other', axis = 1)\n",
    "\n",
    "    # add i94port to the dataframe and drop duplicates\n",
    "    destination['i94port'] = i94port\n",
    "    destination = destination.drop_duplicates()\n",
    "\n",
    "    # merge destination and city datafram\n",
    "    merge = pd.merge(destination, city, on = ['City', 'State Code'])\n",
    "    merge.columns = ['_'.join(c.split(' ')) for c in merge.columns]\n",
    "    \n",
    "    return merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_origin_country(country, immigration_table):\n",
    "    \n",
    "    # create a dictionary with the i94res and corresponding country name\n",
    "    origin_country = {}\n",
    "    with open('origin_country.txt') as f:\n",
    "        for line in f:\n",
    "            key = line.split('=')[0].replace(\"'\",\"\").strip()\n",
    "            value = line.split('=')[1].replace(\"'\",\"\").strip()\n",
    "            origin_country[key] = value\n",
    "\n",
    "    # select distinct i94res and the year from the fact table       \n",
    "    origin = pd.DataFrame(immigration_table.select('i94res', 'i94yr').distinct().collect())\n",
    "    origin.columns = ['i94res', 'Year']\n",
    "\n",
    "    # get the country name using the origin_country dictionary\n",
    "    origin['Country'] = origin['i94res'].map(origin_country)\n",
    "    origin['Country'] = origin['Country'].str.title()\n",
    "\n",
    "    merge = pd.merge(origin, country, on = ['Country'])\n",
    "    \n",
    "    return merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "\n",
    "The **fact table** will contain information from the I94 immigration data. It will be saved to parquet files partitioned by the origin country (i94res) and the destination city (i94port). \n",
    "- i94yr - 4 digit year\n",
    "- i94mon - Numeric month\n",
    "- *i94res* - 3 digit code of origin country\n",
    "- *i94port* - 3 character code of destination city\n",
    "- i94addr - state code of destination city\n",
    "- i94visa - reason for immigration (1 = Business, 2 = Pleasure, 3 = Student)\n",
    "- i94mode - 1 digit travel code (1 = 'Air', 2 = 'Sea', 3 = 'Land', 9 = 'Not reported')\n",
    "- i94bir - Age of Respondent in Years\n",
    "- biryear - year of birth\n",
    "- gender - 1 character code of gender\n",
    "- visatype - Class of admission legally admitting the non-immigrant to temporarily stay in U.S.\n",
    "- airline -  Airline used to arrive in U.S. \n",
    "- fltno - Flight number of Airline used to arrive in U.S.\n",
    "- admnum - Admission Number\n",
    "\n",
    "The **first dimension table** will contain destination city's demographic information from the city dataframe and the immigration_data dataframe. It will be saved to parquet files partitioned by the destination city (i94port). \n",
    "\n",
    "- City - Name of the city\n",
    "- State_Code - 2 character state code\n",
    "- *i94port* - 3 character code of destination city\n",
    "- State - State name\n",
    "- Median_Age - Median age of the city\n",
    "- Male_Population - The number of male population\n",
    "- Female_Population - The number of female population\n",
    "- Total_Population - The number of total population\n",
    "- Number_of_Veterans - The number of vaterans\n",
    "- Foreign-born - The number of foreign-born population\n",
    "- Average_Household_Size - The average household size\n",
    "- American_Indian_and_Alaska_Native - The number of american indian and alaska native population\n",
    "- Asian - The number of asian population\n",
    "- Black_or_African-American - The number of african american population\n",
    "- Hispanic_or_Latino - The number of hispanic or latino population\n",
    "- White - The number of white population\n",
    " \n",
    "The **second dimension table** will contain origin country's information from the country dataframe and the immigration_data dataframe. It will be saved to parquet files partitioned by the origin country (i94res). \n",
    "\n",
    "- *i94res* - 3 digit code of origin country\n",
    "- Country - Country name\n",
    "- Year - 4 digit year\n",
    "- Fertility - Births per woman\n",
    "- Life - The average number of years a newborn would live if the current mortality rates were to stay the same\n",
    "- Population - The total population of the country\n",
    "- Child_Mortality - Death of children under 5 years of age per 1000 live births\n",
    "- Gdp - GDP per capita PPP$ inflation adjusted\n",
    "- Region - The region of the country in the world\n",
    "  \n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "The steps necessary to pipeline the data into the chosen data model:\n",
    "\n",
    "- Clean immigration_data and create the fact table immigration_table partitioned by i94port and i94res. \n",
    "- Clean the city demographic data, create the first dimention table destination_table and write to parquet file partitioned by i94port.\n",
    "- Clean the country gapminder data, create the second dimention table origin_table and write to parquet file partitioned by i94res."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the fact table using the immigration_data as the data source, and pyspark as the tool\n",
    "immigration_table = change_data_type(immigration_data)\n",
    "immigration_table.write.mode(\"append\").partitionBy(\"i94port\", \"i94res\").parquet(\"results/immigration_table.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the first dimention table using the city and immigration_data as the data source, and pyspark/pandas as the tool  \n",
    "destination_table = transform_destination_city(city, immigration_data)\n",
    "destination_table = spark.createDataFrame(destination_table)\n",
    "destination_table.write.mode(\"append\").partitionBy(\"i94port\").parquet(\"results/destination_table.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_table = transform_origin_country(country, immigration_table)\n",
    "origin_table = spark.createDataFrame(origin_table)\n",
    "origin_table.write.mode(\"append\").partitionBy(\"i94res\").parquet(\"results/origin_table.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "I'll perform two data quality checks to ensure the pipeline ran as expected. The first one is a check on the number of data rows, and the second one is a check on the number of data columns. Both checks are to ensure the completeness of data. \n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_rows_check(df, description):\n",
    "    '''\n",
    "    It takes the Spark dataframe and its description and print the outcome of the rows check\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    result = df.count()\n",
    "    if result <= 0:\n",
    "        print(\"The check on the rows failed for {} with zero records\".format(description))\n",
    "    else:\n",
    "        print(\"The check on the rows passed for {} with {} records\".format(description, result))\n",
    "\n",
    "# Perform data quality check\n",
    "num_rows_check(immigration_table, \"immigration table\")\n",
    "num_rows_check(destination_table, \"destination table\")\n",
    "num_rows_check(origin_table, \"origin table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_columns_check(df, description):\n",
    "    '''\n",
    "    It takes the Spark dataframe and its description and print the outcome of the columns check\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    result = len(df.columns)\n",
    "    if ((description == 'immigration table') & (result == 14)) | \\\n",
    "        ((description == 'destination table') & (result == 16)) | \\\n",
    "        ((description == 'origin table') & (result == 9)):\n",
    "        print(\"The check on the columns passed for {} with {} records\".format(description, result))\n",
    "        \n",
    "    else:\n",
    "        print(\"The check on the columns failed for {} with zero records\".format(description))\n",
    "\n",
    "# Perform data quality check\n",
    "num_columns_check(immigration_table, \"immigration table\")\n",
    "num_columns_check(destination_table, \"destination table\")\n",
    "num_columns_check(origin_table, \"origin table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "\n",
    "The **fact table** contains information from the I94 immigration data. The source of the data is the US National Tourism and Trade Office.\n",
    "\n",
    "- i94yr - 4 digit year\n",
    "- i94mon - Numeric month\n",
    "- *i94res* - 3 digit code of origin country\n",
    "- *i94port* - 3 character code of destination city\n",
    "- i94addr - state code of destination city\n",
    "- i94visa - reason for immigration (1 = Business, 2 = Pleasure, 3 = Student)\n",
    "- i94mode - 1 digit travel code (1 = 'Air', 2 = 'Sea', 3 = 'Land', 9 = 'Not reported')\n",
    "- i94bir - Age of Respondent in Years\n",
    "- biryear - year of birth\n",
    "- gender - 1 character code of gender\n",
    "- visatype - Class of admission legally admitting the non-immigrant to temporarily stay in U.S.\n",
    "- airline -  Airline used to arrive in U.S. \n",
    "- fltno - Flight number of Airline used to arrive in U.S.\n",
    "- admnum - Admission Number\n",
    "\n",
    "The **first dimension table** contains destination city's demographic information from the city dataframe and the immigration_data dataframe. The source of the data is OpenSoft and the US National Tourism and Trade Office.\n",
    "\n",
    "- City - Name of the city\n",
    "- State_Code - 2 character state code\n",
    "- *i94port* - 3 character code of destination city\n",
    "- State - State name\n",
    "- Median_Age - Median age of the city\n",
    "- Male_Population - The number of male population\n",
    "- Female_Population - The number of female population\n",
    "- Total_Population - The number of total population\n",
    "- Number_of_Veterans - The number of vaterans\n",
    "- Foreign-born - The number of foreign-born population\n",
    "- Average_Household_Size - The average household size\n",
    "- American_Indian_and_Alaska_Native - The number of american indian and alaska native population\n",
    "- Asian - The number of asian population\n",
    "- Black_or_African-American - The number of african american population\n",
    "- Hispanic_or_Latino - The number of hispanic or latino population\n",
    "- White - The number of white population\n",
    " \n",
    "The **second dimension table** contains origin country's information from the country dataframe and the immigration_data dataframe. The source of the data is gapminder.org and the US National Tourism and Trade Office.\n",
    "\n",
    "- *i94res* - 3 digit code of origin country\n",
    "- Country - Country name\n",
    "- Year - 4 digit year\n",
    "- Fertility - Births per woman\n",
    "- Life - The average number of years a newborn would live if the current mortality rates were to stay the same\n",
    "- Population - The total population of the country\n",
    "- Child_Mortality - Death of children under 5 years of age per 1000 live births\n",
    "- Gdp - GDP per capita PPP$ inflation adjusted\n",
    "- Region - The region of the country in the world"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "\n",
    "I chose Spark because it can handle several petabytes of data at a time, and also has developer libraries and APIs which support multiple programming languages including python. Spark's power lies in its ability to combine very different techniques and processes together into a coherent process. \n",
    "\n",
    "The data can be updated annualy to compare the year to year trend. It can also be updated quaterly or monthly if there are business needs to monitor these trends. \n",
    "\n",
    " * If the data was increased by 100x, we can move the data to S3, and use multiple clusters on AWS to process the data. Another option is to use incremental updates using Uber's Hudi. \n",
    " \n",
    " * If the data populates a dashboard that must be updated on a daily basis by 7am every day, we can use Airflow, because it has the scheduler tool. The Airflow scheduler monitors all tasks and DAGs, and triggers the task instances whose dependencies have been met. In the scheduler, we can set the SLA to meet the 7am goal. \n",
    "\n",
    " * If the database needed to be accessed by 100+ people, we can move the data warehouse to Redshift. There needs to be cost-benefit analysis though. We could also publish the parquet files to HDFS and give people read access.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
