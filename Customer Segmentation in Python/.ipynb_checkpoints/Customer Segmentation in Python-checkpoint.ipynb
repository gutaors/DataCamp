{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign daily acquisition cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that will parse the date\n",
    "def get_day(x): \n",
    "    return dt.datetime(x.year,x.month, x.day) \n",
    "\n",
    "# Create InvoiceDay column\n",
    "online['InvoiceDay'] = online['InvoiceDate'].apply(get_day) \n",
    "\n",
    "# Group by CustomerID and select the InvoiceDay value\n",
    "grouping = online.groupby('CustomerID')['InvoiceDay'] \n",
    "\n",
    "# Assign a minimum InvoiceDay value to the dataset\n",
    "online['CohortDay'] = grouping.transform('min')\n",
    "\n",
    "# View the top 5 rows\n",
    "print(online.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_int(df, column):\n",
    "    \n",
    "    year = df[column].dt.year\n",
    "    month = df[column].dt.month\n",
    "    day = df[column].dt.day\n",
    "    \n",
    "    return year, month, day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate time offset in days - part 1\n",
    "\n",
    "# Get the integers for date parts from the `InvoiceDay` column\n",
    "invoice_year, invoice_month, invoice_day = get_date_int(online, 'InvoiceDay')\n",
    "\n",
    "# Get the integers for date parts from the `CohortDay` column\n",
    "cohort_year, cohort_month, cohort_day = get_date_int(online, 'CohortDay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate time offset in days - part 2\n",
    "# Now, we have six different data sets with year, month and day values \n",
    "# for Invoice and Cohort dates - invoice_year, cohort_year, invoice_month, \n",
    "# cohort_month, invoice_day, and cohort_day.\n",
    "\n",
    "# Calculate difference in years\n",
    "years_diff = invoice_year - cohort_year\n",
    "\n",
    "# Calculate difference in months\n",
    "months_diff = invoice_month - cohort_month\n",
    "\n",
    "# Calculate difference in days\n",
    "days_diff = invoice_day - cohort_day\n",
    "\n",
    "# Extract the difference in days from all previous values\n",
    "online['CohortIndex'] = years_diff * 365 + months_diff * 30 + days_diff + 1\n",
    "print(online.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate retention rate from scratch\n",
    "\n",
    "grouping = online.groupby(['CohortMonth', 'CohortIndex'])\n",
    "\n",
    "# Count the number of unique values per customer ID\n",
    "cohort_data = grouping['CustomerID'].apply(pd.Series.nunique).reset_index()\n",
    "\n",
    "# Create a pivot \n",
    "cohort_counts = cohort_data.pivot(index='CohortMonth', columns='CohortIndex', values='CustomerID')\n",
    "\n",
    "# Select the first column and store it to cohort_sizes\n",
    "cohort_sizes = cohort_counts.iloc[:,0]\n",
    "\n",
    "# Divide the cohort count by cohort sizes along the rows\n",
    "retention = cohort_counts.divide(cohort_sizes, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate average price\n",
    "\n",
    "# Create a groupby object and pass the monthly cohort and cohort index as a list\n",
    "grouping = online.groupby(['CohortMonth', 'CohortIndex']) \n",
    "\n",
    "# Calculate the average of the unit price column\n",
    "cohort_data = grouping['UnitPrice'].mean()\n",
    "\n",
    "# Reset the index of cohort_data\n",
    "cohort_data = cohort_data.reset_index()\n",
    "\n",
    "# Create a pivot \n",
    "average_quantity = cohort_data.pivot(index='CohortMonth', columns='CohortIndex', values='UnitPrice')\n",
    "print(average_quantity.round(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize average quantity metric\n",
    "\n",
    "# Import seaborn package as sns\n",
    "import seaborn as sns\n",
    "\n",
    "# Initialize an 8 by 6 inches plot figure\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Add a title\n",
    "plt.title('Average Spend by Monthly Cohorts')\n",
    "\n",
    "# Create the heatmap\n",
    "sns.heatmap(average_quantity, annot=True, cmap='Blues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODULE 2\n",
    "### RFM Segmentation\n",
    "\n",
    "# https://www.putler.com/rfm-analysis/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Spend quartiles (q=4)\n",
    "\n",
    "# Create a spend quartile with 4 groups - a range between 1 and 5\n",
    "spend_quartile = pd.qcut(data['Spend'], q=4, labels=range(1,5))\n",
    "\n",
    "# Assign the quartile values to the Spend_Quartile column in data\n",
    "data['Spend_Quartile'] = spend_quartile\n",
    "\n",
    "# Print data with sorted Spend values\n",
    "print(data.sort_values('Spend'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate Recency deciles (q=10)\n",
    "\n",
    "# Store labels from 4 to 1 in a decreasing order\n",
    "r_labels = list(range(4, 0, -1))\n",
    "\n",
    "# Create a spend quartile with 4 groups and pass the previously created labels \n",
    "recency_quartiles = pd.qcut(data['Recency_Days'], q=4, labels=r_labels)\n",
    "\n",
    "# Assign the quartile values to the Recency_Quartile column in `data`\n",
    "data['Recency_Quartile'] = recency_quartiles \n",
    "\n",
    "# Print `data` with sorted Recency_Days values\n",
    "print(data.sort_values('Recency_Days'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate RFM values\n",
    "\n",
    "# Calculate Recency, Frequency and Monetary value for each customer \n",
    "datamart = online.groupby(['CustomerID']).agg({\n",
    "    'InvoiceDate': lambda x: (snapshot_date - x.max()).days,\n",
    "    'InvoiceNo': 'count',\n",
    "    'TotalSum': 'sum'})\n",
    "\n",
    "# Rename the columns \n",
    "datamart.rename(columns={'InvoiceDate': 'Recency',\n",
    "                         'InvoiceNo': 'Frequency',\n",
    "                         'TotalSum': 'MonetaryValue'}, inplace=True)\n",
    "\n",
    "# Print top 5 rows\n",
    "print(datamart.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate 3 groups for Recency and Frequency\n",
    "\n",
    "# Create labels for Recency and Frequency\n",
    "r_labels = range(3, 0, -1); f_labels = range(1, 4)\n",
    "\n",
    "# Assign these labels to three equal percentile groups \n",
    "r_groups = pd.qcut(datamart['Recency'], q=3, labels=r_labels)\n",
    "\n",
    "# Assign these labels to three equal percentile groups \n",
    "f_groups = pd.qcut(datamart['Frequency'], q=3, labels=f_labels)\n",
    "\n",
    "# Create new columns R and F \n",
    "datamart = datamart.assign(R=r_groups.values, F=f_groups.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate RFM Score\n",
    "\n",
    "# Create labels for MonetaryValue\n",
    "m_labels = range(1, 4)\n",
    "\n",
    "# Assign these labels to three equal percentile groups \n",
    "m_groups = pd.qcut(datamart['MonetaryValue'], q=3, labels=m_labels)\n",
    "\n",
    "# Create new column M\n",
    "datamart = datamart.assign(M=m_groups)\n",
    "\n",
    "# Calculate RFM_Score\n",
    "datamart['RFM_Score'] = datamart[['R','F','M']].sum(axis=1)\n",
    "print(datamart['RFM_Score'].head())\n",
    "\n",
    "\n",
    "<script.py> output:\n",
    "    CustomerID\n",
    "    12747    9.0\n",
    "    12748    9.0\n",
    "    12749    9.0\n",
    "    12820    9.0\n",
    "    12822    6.0\n",
    "    Name: RFM_Score, dtype: float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamart[datamart.RFM_Score == 9.0]['MonetaryValue'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating custom segments\n",
    "\n",
    "# Define rfm_level function\n",
    "def rfm_level(df):\n",
    "    if df['RFM_Score'] >= 10:\n",
    "        return 'Top'\n",
    "    elif ((df['RFM_Score'] >= 6) and (df['RFM_Score'] < 10)):\n",
    "        return 'Middle'\n",
    "    else:\n",
    "        return 'Low'\n",
    "\n",
    "# Create a new variable RFM_Level\n",
    "datamart['RFM_Level'] = datamart.apply(rfm_level, axis=1)\n",
    "\n",
    "# Print the header with top 5 rows to the console\n",
    "print(datamart.head())\n",
    "\n",
    "<script.py> output:\n",
    "                Recency  Frequency  MonetaryValue  R  F  M  RFM_Segment  RFM_Score RFM_Level\n",
    "    CustomerID                                                                              \n",
    "    12747             3         25         948.70  4  4  4          444       12.0       Top\n",
    "    12748             1        888        7046.16  4  4  4          444       12.0       Top\n",
    "    12749             4         37         813.45  4  4  4          444       12.0       Top\n",
    "    12820             4         17         268.02  4  3  3          433       10.0       Top\n",
    "    12822            71          9         146.15  2  2  3          223        7.0    Middle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average values for each RFM_Level, and return a size of each segment \n",
    "rfm_level_agg = datamart.groupby('RFM_Level').agg({\n",
    "    'Recency': 'mean',\n",
    "    'Frequency': 'mean',\n",
    "\n",
    "  \t# Return the size of each segment\n",
    "    'MonetaryValue': ['mean', 'count']\n",
    "}).round(1)\n",
    "\n",
    "# Print the aggregated dataset\n",
    "print(rfm_level_agg)\n",
    "\n",
    "\n",
    "<script.py> output:\n",
    "              Recency Frequency MonetaryValue      \n",
    "                 mean      mean          mean count\n",
    "    RFM_Level                                      \n",
    "    Low         180.8       3.2          52.7  1075\n",
    "    Middle       73.9      10.7         202.9  1547\n",
    "    Top          20.3      47.1         959.7  1021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MODULE 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate statistics of variables\n",
    "\n",
    "# Print the average values of the variables in the dataset\n",
    "print(data[['var1','var2','var3']].mean())\n",
    "\n",
    "# Print the standard deviation of the variables in the dataset\n",
    "print(data[['var1','var2','var3']].std())\n",
    "\n",
    "# Get the key statistics of the dataset\n",
    "print(data[['var1','var2','var3']].keys())\n",
    "\n",
    "<script.py> output:\n",
    "    var1    251.85000\n",
    "    var2      1.92559\n",
    "    var3     12.55028\n",
    "    dtype: float64\n",
    "    var1    90.993104\n",
    "    var2     2.583730\n",
    "    var3    34.516362\n",
    "    dtype: float64\n",
    "    Index(['var1', 'var2', 'var3'], dtype='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Detect skewed variables\n",
    "\n",
    "# Plot distribution of var1\n",
    "plt.subplot(3, 1, 1); sns.distplot(data['var1'])\n",
    "\n",
    "# Plot distribution of var2\n",
    "plt.subplot(3, 1, 2); sns.distplot(data['var2'])\n",
    "\n",
    "# Plot distribution of var3\n",
    "plt.subplot(3, 1, 3); sns.distplot(data['var3'])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Manage skewness\n",
    "\n",
    "# Apply log transformation to var2\n",
    "data['var2_log'] = np.log(data['var2'])\n",
    "\n",
    "# Apply log transformation to var3\n",
    "data['var3_log'] = np.log(data['var3'])\n",
    "\n",
    "# Create a subplot of the distribution of var2_log\n",
    "plt.subplot(2, 1, 1); sns.distplot(data['var2_log'])\n",
    "\n",
    "# Create a subplot of the distribution of var3_log\n",
    "plt.subplot(2, 1, 2); sns.distplot(data['var3_log'])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Center and scale manually\n",
    "\n",
    "# Center the data by subtracting average values from each entry\n",
    "data_centered = data - data.mean()\n",
    "\n",
    "# Scale the data by dividing each entry by standard deviation\n",
    "data_scaled = data / data.std()\n",
    "\n",
    "# Normalize the data by applying both centering and scaling\n",
    "data_normalized = (data - data.mean()) / data.std()\n",
    "\n",
    "# Print summary statistics to make sure average is zero and standard deviation is one\n",
    "print(data_normalized.describe().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Center and scale with StandardScaler()\n",
    "\n",
    "# Initialize a scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler\n",
    "scaler.fit(data)\n",
    "\n",
    "# Scale and center the data\n",
    "data_normalized = scaler.transform(data)\n",
    "\n",
    "# Create a pandas DataFrame\n",
    "data_normalized = pd.DataFrame(data_normalized, index=data.index, columns=data.columns)\n",
    "\n",
    "# Print summary statistics\n",
    "print(data_normalized.describe().round(2))\n",
    "\n",
    "<script.py> output:\n",
    "             var1    var2    var3\n",
    "    count  100.00  100.00  100.00\n",
    "    mean     0.00    0.00    0.00\n",
    "    std      1.01    1.01    1.01\n",
    "    min     -1.67   -0.73   -0.37\n",
    "    25%     -0.88   -0.51   -0.36\n",
    "    50%     -0.02   -0.29   -0.33\n",
    "    75%      0.97    0.11   -0.20\n",
    "    max      1.60    5.21    6.30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize RFM distributions\n",
    "\n",
    "# Plot recency distribution\n",
    "plt.subplot(3, 1, 1); sns.distplot(datamart_rfm['Recency'])\n",
    "\n",
    "# Plot frequency distribution\n",
    "plt.subplot(3, 1, 2); sns.distplot(datamart_rfm['Frequency'])\n",
    "\n",
    "# Plot monetary value distribution\n",
    "plt.subplot(3, 1, 3); sns.distplot(datamart_rfm['MonetaryValue'])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pre-process RFM data\n",
    "\n",
    "# Unskew the data\n",
    "datamart_log = np.log(datamart_rfm)\n",
    "\n",
    "# Initialize a standard scaler and fit it\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(datamart_log)\n",
    "\n",
    "# Scale and center the data\n",
    "datamart_normalized = scaler.transform(datamart_log)\n",
    "\n",
    "# Create a pandas DataFrame\n",
    "datamart_normalized = pd.DataFrame(data=datamart_normalized,\n",
    "                                   index=datamart_log.index,\n",
    "                                   columns=datamart_log.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize the normalized variables\n",
    "\n",
    "# Plot recency distribution\n",
    "plt.subplot(3, 1, 1); sns.distplot(datamart_normalized['Recency'])\n",
    "\n",
    "# Plot frequency distribution\n",
    "plt.subplot(3, 1, 2); sns.distplot(datamart_normalized['Frequency'])\n",
    "\n",
    "# Plot monetary value distribution\n",
    "plt.subplot(3, 1, 3); sns.distplot(datamart_normalized['MonetaryValue'])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODULE 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run KMeans\n",
    "\n",
    "# Import KMeans \n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Initialize KMeans\n",
    "kmeans = KMeans(n_clusters=3, random_state=1) \n",
    "\n",
    "# Fit k-means clustering on the normalized data set\n",
    "kmeans.fit(datamart_normalized)\n",
    "\n",
    "# Extract cluster labels\n",
    "cluster_labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign labels to raw data\n",
    "\n",
    "# Create a DataFrame by adding a new cluster label column\n",
    "datamart_rfm_k3 = datamart_rfm.assign(Cluster=cluster_labels)\n",
    "\n",
    "# Group the data by cluster\n",
    "grouped = datamart_rfm_k3.groupby(['Cluster'])\n",
    "\n",
    "# Calculate average RFM values and segment sizes per cluster value\n",
    "grouped.agg({\n",
    "    'Recency': 'mean',\n",
    "    'Frequency': 'mean',\n",
    "    'MonetaryValue': ['mean', 'count']\n",
    "  }).round(1)\n",
    "\n",
    "\n",
    "Out[1]: \n",
    "        Recency Frequency MonetaryValue      \n",
    "           mean      mean          mean count\n",
    "Cluster                                      \n",
    "0         166.7       3.0          53.0  1155\n",
    "1          77.3      12.2         213.7  1577\n",
    "2          16.5      49.9        1045.2   911"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate sum of squared errors\n",
    "\n",
    "# Fit KMeans and calculate SSE for each k\n",
    "for k in range(1, 21):\n",
    "  \n",
    "    # Initialize KMeans with k clusters\n",
    "    kmeans = KMeans(n_clusters=k, random_state=1)\n",
    "    \n",
    "    # Fit KMeans on the normalized dataset\n",
    "    kmeans.fit(data_normalized)\n",
    "    \n",
    "    # Assign sum of squared distances to k element of dictionary\n",
    "    sse[k] = kmeans.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot sum of squared errors\n",
    "\n",
    "# Add the plot title \"The Elbow Method\"\n",
    "plt.title('The Elbow Method')\n",
    "\n",
    "# Add X-axis label \"k\"\n",
    "plt.xlabel('k')\n",
    "\n",
    "# Add Y-axis label \"SSE\"\n",
    "plt.ylabel('sse')\n",
    "\n",
    "# Plot SSE values for each key in the dictionary\n",
    "sns.pointplot(x=list(sse.keys()), y=list(sse.values()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare data for the snake plot\n",
    "\n",
    "# Melt the normalized dataset and reset the index\n",
    "datamart_melt = pd.melt(\n",
    "  \t\t\t\t\tdatamart_normalized.reset_index(), \n",
    "                        \n",
    "# Assign CustomerID and Cluster as ID variables\n",
    "                    id_vars=['CustomerID', 'Cluster'],\n",
    "\n",
    "# Assign RFM values as value variables\n",
    "                    value_vars=['Recency', 'Frequency', 'MonetaryValue'], \n",
    "                        \n",
    "# Name the variable and value\n",
    "                    var_name='Metric', value_name='Value'\n",
    "\t\t\t\t\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize snake plot\n",
    "\n",
    "# Add the plot title\n",
    "plt.title('Snake plot of normalized variables')\n",
    "\n",
    "# Add the x axis label\n",
    "plt.xlabel('Metric')\n",
    "\n",
    "# Add the y axis label\n",
    "plt.ylabel('Value')\n",
    "\n",
    "# Plot a line for each value of the cluster variable\n",
    "sns.lineplot(data=datamart_melt, x='Metric', y='Value', hue='Cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate relative importance of each attribute\n",
    "\n",
    "# Calculate average RFM values for each cluster\n",
    "cluster_avg = datamart_rfm_k3.groupby(['Cluster']).mean() \n",
    "\n",
    "# Calculate average RFM values for the total customer population\n",
    "population_avg = datamart_rfm.mean()\n",
    "\n",
    "# Calculate relative importance of cluster's attribute value compared to population\n",
    "relative_imp = cluster_avg / population_avg - 1\n",
    "\n",
    "# Print relative importance scores rounded to 2 decimals\n",
    "print(relative_imp.round(2))\n",
    "\n",
    "\n",
    "         Recency  Frequency  MonetaryValue\n",
    "Cluster                                   \n",
    "0           0.84      -0.84          -0.86\n",
    "1          -0.15      -0.35          -0.42\n",
    "2          -0.82       1.67           1.82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot relative importance heatmap\n",
    "\n",
    "# Initialize a plot with a figure size of 8 by 2 inches \n",
    "plt.figure(figsize=(8, 2))\n",
    "\n",
    "# Add the plot title\n",
    "plt.title('Relative importance of attributes')\n",
    "\n",
    "# Plot the heatmap\n",
    "sns.heatmap(data=relative_imp, annot=True, fmt='.2f', cmap='RdYlGn')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pre-process data\n",
    "\n",
    "# Import StandardScaler \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Apply log transformation\n",
    "datamart_rfmt_log = np.log(datamart_rfmt)\n",
    "\n",
    "# Initialize StandardScaler and fit it \n",
    "scaler = StandardScaler(); scaler.fit(datamart_rfmt_log)\n",
    "\n",
    "# Transform and store the scaled data as datamart_rfmt_normalized\n",
    "datamart_rfmt_normalized = scaler.transform(datamart_rfmt_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate and plot sum of squared errors\n",
    "\n",
    "# Fit KMeans and calculate SSE for each k between 1 and 10\n",
    "for k in range(1, 11):\n",
    "  \n",
    "    # Initialize KMeans with k clusters and fit it \n",
    "    kmeans = KMeans(n_clusters=k, random_state=1).fit(datamart_rfmt_normalized)\n",
    "    \n",
    "    # Assign sum of squared distances to k element of the sse dictionary\n",
    "    sse[k] = kmeans.inertia_   \n",
    "\n",
    "# Add the plot title, x and y axis labels\n",
    "plt.title('The Elbow Method'); plt.xlabel('k'); plt.ylabel('SSE')\n",
    "\n",
    "# Plot SSE values for each k stored as keys in the dictionary\n",
    "sns.pointplot(x=list(sse.keys()), y=list(sse.values()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build 4-cluster solution\n",
    "# Import KMeans \n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Initialize KMeans\n",
    "kmeans = KMeans(n_clusters=4, random_state=1) \n",
    "\n",
    "# Fit k-means clustering on the normalized data set\n",
    "kmeans.fit(datamart_rfmt_normalized)\n",
    "\n",
    "# Extract cluster labels\n",
    "cluster_labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Analyze the segments\n",
    "\n",
    "# Create a new DataFrame by adding a cluster label column to datamart_rfmt\n",
    "datamart_rfmt_k4 = datamart_rfmt.assign(Cluster=cluster_labels)\n",
    "\n",
    "# Group by cluster\n",
    "grouped = datamart_rfmt_k4.groupby(['Cluster'])\n",
    "\n",
    "# Calculate average RFMT values and segment sizes for each cluster\n",
    "grouped.agg({\n",
    "    'Recency': 'mean',\n",
    "    'Frequency': 'mean',\n",
    "    'MonetaryValue': 'mean',\n",
    "    'Tenure': ['mean', 'count']\n",
    "  }).round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
