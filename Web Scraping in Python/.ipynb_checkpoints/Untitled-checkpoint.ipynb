{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Web Scraping in Python"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The ability to build tools capable of retrieving and parsing information stored across the internet has been and continues to be valuable in many veins of data science. In this course, you will learn to navigate and parse html code, and build tools to crawl websites automatically. Although our scraping will be conducted using the versatile Python library scrapy, many of the techniques you learn in this course can be applied to other popular Python libraries as well, including BeautifulSoup and Selenium. Upon the completion of this course, you will have a strong mental model of html structure, will be able to build tools to parse html code and access desired information, and create a simple scrapy spiders to crawl the web at scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MODULE 1\n",
    "\n",
    "## Introduction to HTML"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Learn the structure of HTML. We begin by explaining why web scraping can be a valuable addition to your data science toolbox and then delving into some basics of HTML. We end the chapter by giving a brief introduction on XPath notation, which is used to navigate the elements within HTML code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## From Tree to HTML\n",
    "\n",
    "html = '''\n",
    "<html>\n",
    "  <head>\n",
    "    <title>Intro HTML</title>\n",
    "  </head>\n",
    "  <body>\n",
    "    <p>Hello World!</p>\n",
    "    <p>Enjoy Datacamp!</p>\n",
    "  </body>\n",
    "</html>\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Keep it Classy\n",
    "\n",
    "# HTML code string\n",
    "html = '''\n",
    "<html>\n",
    "  <body>\n",
    "    <div class=\"class1\" id=\"div1\">\n",
    "      <p class=\"class2\">Visit DataCamp!</p>\n",
    "    </div>\n",
    "    <div class='you-are-classy'>\n",
    "      <p class=\"class2\">Keep up the good work!</p>\n",
    "    </div>\n",
    "  </body>\n",
    "</html>\n",
    "'''\n",
    "# Print out the class of the second div element\n",
    "whats_my_class( html )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Where am I?\n",
    "\n",
    "# Fill in the blank\n",
    "xpath = '/html/body/div[2]/p'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## It's Time to P\n",
    "\n",
    "# Fill in the blank\n",
    "xpath = '//p'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A classy span\n",
    "\n",
    "# Fill in the blank\n",
    "xpath = '//span[@class=\"span-class\"]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MODULE 2\n",
    "\n",
    "## XPaths and Selectors"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Leverage XPath syntax to explore scrapy selectors. Both of these concepts will move you towards being able to scrape an HTML document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Body Appendages\n",
    "\n",
    "# Create an XPath string to direct to children of body element\n",
    "xpath = '//body/*'\n",
    "\n",
    "# Print out the number of elements selected\n",
    "how_many_elements( xpath )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Choose DataCamp!\n",
    "\n",
    "# Create an XPath string to the desired paragraph element\n",
    "xpath = '/html/body/div[1]/div/p'\n",
    "\n",
    "# Print out the element text\n",
    "print_element_text( xpath )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Where it's @\n",
    "\n",
    "# Create an Xpath string to select desired p element\n",
    "xpath = '//*[@id=\"div3\"]/p'\n",
    "\n",
    "# Print out selection text\n",
    "print_element_text( xpath )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check your Class\n",
    "\n",
    "# Create an XPath string to select p element by class\n",
    "xpath = '//p[@class=\"class-1 class-2\"]'\n",
    "\n",
    "# Print out select text\n",
    "print_element_text( xpath )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyper(link) Active\n",
    "\n",
    "# Create an xpath to the href attribute\n",
    "xpath = '//p[@id=\"p2\"]/a/@href'\n",
    "\n",
    "# Print out the selection(s); there should be only one\n",
    "print_attribute( xpath )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Secret Links\n",
    "\n",
    "# Create an xpath to the href attributes\n",
    "xpath = '//a[contains(@class,\"course-block\")]/@href'\n",
    "\n",
    "# Print out how many elements are selected\n",
    "how_many_elements( xpath )\n",
    "# Preview the selected elements\n",
    "preview( xpath )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## XPath Chaining\n",
    "\n",
    "# Chain together xpath methods to select desired p element\n",
    "sel.xpath( '//div' ).xpath( './span/p[3]' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Divvy Up This Exercise\n",
    "\n",
    "from scrapy import Selector\n",
    "\n",
    "# Create a Selector selecting html as the HTML document\n",
    "sel = Selector( text = html )\n",
    "\n",
    "# Create a SelectorList of all div elements in the HTML document\n",
    "divs = sel.xpath( '//div' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Requesting a Selector\n",
    "\n",
    "# Import a scrapy Selector\n",
    "from scrapy import Selector\n",
    "\n",
    "# Import requests\n",
    "import requests\n",
    "\n",
    "# Create the string html containing the HTML source\n",
    "html = requests.get(url).content\n",
    "\n",
    "# Create the Selector object sel from html\n",
    "sel = Selector(text=html)\n",
    "\n",
    "# Print out the number of elements in the HTML document\n",
    "print(\"There are 1020 elements in the HTML document.\")\n",
    "print(\"You have found: \", len(sel.xpath('//*')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MODULE 3\n",
    "\n",
    "## CSS Locators, Chaining, and Responses"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Learn CSS Locator syntax and begin playing with the idea of chaining together CSS Locators with XPath. We also introduce Response objects, which behave like Selectors but give us extra tools to mobilize our scraping efforts across multiple websites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The (X)Path to CSS Locators\n",
    "\n",
    "# Create the XPath string equivalent to the CSS Locator \n",
    "xpath = '/html/body/span[1]//a'\n",
    "\n",
    "# Create the CSS Locator string equivalent to the XPath\n",
    "css_locator = 'html > body > span:nth-of-type(1) a'\n",
    "\n",
    "\n",
    "# part 2\n",
    "# Create the XPath string equivalent to the CSS Locator \n",
    "xpath = '//div[@id=\"uid\"]/span//h4'\n",
    "\n",
    "# Create the CSS Locator string equivalent to the XPath\n",
    "css_locator = 'div#uid > span h4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## from scrapy import Selector\n",
    "\n",
    "# Create a selector from the html (of a secret website)\n",
    "sel = Selector( text= html )\n",
    "\n",
    "\n",
    "from scrapy import Selector\n",
    "\n",
    "# Create a selector from the html (of a secret website)\n",
    "sel = Selector( text = html )\n",
    "\n",
    "# Fill in the blank\n",
    "css_locator = \"div.course-block > a\"\n",
    "\n",
    "# Print the number of selected elements.\n",
    "how_many_elements( css_locator )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The CSS Wildcard\n",
    "\n",
    "# Create the CSS Locator to all children of the element whose id is uid\n",
    "css_locator = '#uid > *'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## You've been `href`ed\n",
    "\n",
    "from scrapy import Selector\n",
    "\n",
    "# Create a selector object from a secret website\n",
    "sel = Selector( text=html )\n",
    "\n",
    "# Select all hyperlinks of div elements belonging to class \"course-block\"\n",
    "course_as = sel.css( 'div.course-block > a' )\n",
    "\n",
    "# Selecting all href attributes chaining with css\n",
    "hrefs_from_css = course_as.css( '::attr(href)' )\n",
    "\n",
    "# Selecting all href attributes chaining with xpath\n",
    "hrefs_from_xpath = course_as.xpath( './@href' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Top Level Text\n",
    "\n",
    "# Create an XPath string to the desired text.\n",
    "xpath = '//p[@id=\"p3\"]/text()'\n",
    "\n",
    "# Create a CSS Locator string to the desired text.\n",
    "css_locator = 'p#p3::text'\n",
    "\n",
    "# Print the text from our selections\n",
    "print_results( xpath, css_locator )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## All Level Text\n",
    "\n",
    "# Create an XPath string to the desired text.\n",
    "xpath = \"//p[@id='p3'] //text()\"\n",
    "\n",
    "# Create a CSS Locator string to the desired text.\n",
    "css_locator = \"p#p3 ::text\"\n",
    "\n",
    "# Print the text from our selections\n",
    "print_results( xpath, css_locator )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reveal By Response\n",
    "\n",
    "# Get the URL to the website loaded in response\n",
    "this_url = response.url\n",
    "\n",
    "# Get the title of the website loaded in response\n",
    "this_title = response.xpath('/html/head/title/text()').extract_first()\n",
    "\n",
    "# Print out our findings\n",
    "print_url_title( this_url, this_title )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Responding with Selectors\n",
    "\n",
    "# Create a CSS Locator string to the desired hyperlink elements\n",
    "css_locator = 'a.course-block__link'\n",
    "\n",
    "# Select the hyperlink elements from response and sel\n",
    "response_as = response.css(css_locator)\n",
    "sel_as = sel.css(css_locator)\n",
    "\n",
    "# Examine similarity\n",
    "nr = len( response_as )\n",
    "ns = len( sel_as )\n",
    "for i in range( min(nr, ns, 2) ):\n",
    "  print( \"Element %d from response: %s\" % (i+1, response_as[i]) )\n",
    "  print( \"Element %d from sel: %s\" % (i+1, sel_as[i]) )\n",
    "  print( \"\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Selecting from a Selection\n",
    "\n",
    "# Select all desired div elements\n",
    "divs = response.css(\"div.course-block\")\n",
    "\n",
    "# Take the first div element\n",
    "first_div = divs[0]\n",
    "\n",
    "# Extract the text from the (only) h4 element in first_div\n",
    "h4_text = first_div.css('h4 ::text').extract_first()\n",
    "\n",
    "# Print out the text\n",
    "print( \"The text from the h4 element is:\", h4_text )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Titular\n",
    "\n",
    "# Create a SelectorList of the course titles\n",
    "crs_title_els = response.css('h4 ::text')\n",
    "\n",
    "# Extract the course titles \n",
    "crs_titles = crs_title_els.extract()\n",
    "\n",
    "# Print out the course titles \n",
    "for el in crs_titles:\n",
    "  print( \">>\", el )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scraping with Children\n",
    "\n",
    "# Calculate the number of children of the mystery element\n",
    "how_many_kids = len( mystery.xpath( './*' ) )\n",
    "\n",
    "# Print out the number\n",
    "print( \"The number of elements you selected was:\", how_many_kids )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODULE 4\n",
    "\n",
    "### Spiders"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Learn to create web crawlers with scrapy. These scrapy spiders will crawl the web through multiple pages, following links to scrape each of those pages automatically according to the procedures we've learned in the previous chapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inheriting the Spider\n",
    "\n",
    "# Import scrapy library\n",
    "import scrapy\n",
    "\n",
    "# Create the spider class\n",
    "class YourSpider(scrapy.Spider):\n",
    "  name = \"your_spider\"\n",
    "  # start_requests method\n",
    "  def start_requests(self):\n",
    "    pass\n",
    "  # parse method\n",
    "  def parse(self, response):\n",
    "    pass\n",
    "  \n",
    "# Inspect Your Class\n",
    "inspect_class(YourSpider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hurl the URLs\n",
    "\n",
    "# Import scrapy library\n",
    "import scrapy\n",
    "\n",
    "# Create the spider class\n",
    "class YourSpider( scrapy.Spider ):\n",
    "  name = \"your_spider\"\n",
    "  # start_requests method\n",
    "  def start_requests( self ):\n",
    "    urls = [\"https://www.datacamp.com\", \"https://scrapy.org\"]\n",
    "    for url in urls:\n",
    "      yield url\n",
    "  # parse method\n",
    "  def parse( self, response ):\n",
    "    pass\n",
    "  \n",
    "# Inspect Your Class\n",
    "inspect_class( YourSpider )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Self Referencing is Classy\n",
    "\n",
    "# Import scrapy library\n",
    "import scrapy\n",
    "\n",
    "# Create the spider class\n",
    "class YourSpider( scrapy.Spider ):\n",
    "  name = \"your_spider\"\n",
    "  # start_requests method\n",
    "  def start_requests( self ):\n",
    "    self.print_msg( \"Hello World!\" )\n",
    "  # parse method\n",
    "  def parse( self, response ):\n",
    "    pass\n",
    "  # print_msg method\n",
    "  def print_msg( self, msg ):\n",
    "    print( \"Calling start_requests in YourSpider prints out:\", msg )\n",
    "  \n",
    "# Inspect Your Class\n",
    "inspect_class( YourSpider )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Starting with Start Requests\n",
    "\n",
    "# Import scrapy library\n",
    "import scrapy\n",
    "\n",
    "# Create the spider class\n",
    "class YourSpider( scrapy.Spider ):\n",
    "  name = \"your_spider\"\n",
    "  # start_requests method\n",
    "  def start_requests( self ):\n",
    "    yield scrapy.Request( url= \"https://www.datacamp.com\", callback=self.parse )\n",
    "  # parse method\n",
    "  def parse( self, response ):\n",
    "    pass\n",
    "  \n",
    "# Inspect Your Class\n",
    "inspect_class( YourSpider )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pen Names\n",
    "\n",
    "# Import the scrapy library\n",
    "import scrapy\n",
    "\n",
    "# Create the Spider class\n",
    "class DCspider( scrapy.Spider ):\n",
    "  name = 'dcspider'\n",
    "  # start_requests method\n",
    "  def start_requests( self ):\n",
    "    yield scrapy.Request( url = url_short, callback = self.parse )\n",
    "  # parse method\n",
    "  def parse( self, response ):\n",
    "    # Create an extracted list of course author names\n",
    "    author_names = response.css('p.course-block__author-name::text').extract()\n",
    "    # Here we will just return the list of Authors\n",
    "    return author_names\n",
    "  \n",
    "# Inspect the spider\n",
    "inspect_spider( DCspider )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Crawler Time\n",
    "\n",
    "# Import the scrapy library\n",
    "import scrapy\n",
    "\n",
    "# Create the Spider class\n",
    "class DCdescr( scrapy.Spider ):\n",
    "  name = 'dcdescr'\n",
    "  # start_requests method\n",
    "  def start_requests( self ):\n",
    "    yield scrapy.Request( url = url_short, callback = self.parse )\n",
    "  \n",
    "  # First parse method\n",
    "  def parse( self, response ):\n",
    "    links = response.css( 'div.course-block > a::attr(href)' ).extract()\n",
    "    # Follow each of the extracted links\n",
    "    for link in links:\n",
    "      yield response.follow(url= link, callback = self.parse_descr)\n",
    "      \n",
    "  # Second parsing method\n",
    "  def parse_descr( self, response ):\n",
    "    # Extract course description\n",
    "    course_descr = response.css( 'p.course__description::text' ).extract_first()\n",
    "    # For now, just yield the course description\n",
    "    yield course_descr\n",
    "\n",
    "\n",
    "# Inspect the spider\n",
    "inspect_spider( DCdescr )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Import scrapy\n",
    "import scrapy\n",
    "\n",
    "# Import the CrawlerProcess: for running the spider\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# Create the Spider class\n",
    "class DC_Chapter_Spider(scrapy.Spider):\n",
    "  name = \"dc_chapter_spider\"\n",
    "  # start_requests method\n",
    "  def start_requests(self):\n",
    "    yield scrapy.Request(url = url_short,\n",
    "                         callback = self.parse_front)\n",
    "  # First parsing method\n",
    "  def parse_front(self, response):\n",
    "    course_blocks = response.css('div.course-block')\n",
    "    course_links = course_blocks.xpath('./a/@href')\n",
    "    links_to_follow = course_links.extract()\n",
    "    for url in links_to_follow:\n",
    "      yield response.follow(url = url,\n",
    "                            callback = self.parse_pages)\n",
    "  # Second parsing method\n",
    "  def parse_pages(self, response):\n",
    "    crs_title = response.xpath('//h1[contains(@class,\"title\")]/text()')\n",
    "    crs_title_ext = crs_title.extract_first().strip()\n",
    "    ch_titles = response.css('h4.chapter__title::text')\n",
    "    ch_titles_ext = [t.strip() for t in ch_titles.extract()]\n",
    "    dc_dict[ crs_title_ext ] = ch_titles_ext\n",
    "\n",
    "# Initialize the dictionary **outside** of the Spider class\n",
    "dc_dict = dict()\n",
    "\n",
    "# Run the Spider\n",
    "process = CrawlerProcess()\n",
    "process.crawl(DC_Chapter_Spider)\n",
    "process.start()\n",
    "\n",
    "# Print a preview of courses\n",
    "previewCourses(dc_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DataCamp Descriptions\n",
    "\n",
    "# Import scrapy\n",
    "import scrapy\n",
    "\n",
    "# Import the CrawlerProcess: for running the spider\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# Create the Spider class\n",
    "class DC_Description_Spider(scrapy.Spider):\n",
    "  name = \"dc_chapter_spider\"\n",
    "  # start_requests method\n",
    "  def start_requests(self):\n",
    "    yield scrapy.Request(url = url_short,\n",
    "                         callback = self.parse_front)\n",
    "  # First parsing method\n",
    "  def parse_front(self, response):\n",
    "    course_blocks = response.css('div.course-block')\n",
    "    course_links = course_blocks.xpath('./a/@href')\n",
    "    links_to_follow = course_links.extract()\n",
    "    for url in links_to_follow:\n",
    "      yield response.follow(url = url,\n",
    "                            callback = self.parse_pages)\n",
    "  # Second parsing method\n",
    "  def parse_pages(self, response):\n",
    "    # Create a SelectorList of the course titles text\n",
    "    crs_title = response.xpath('//h1[contains(@class,\"title\")]/text()')\n",
    "    # Extract the text and strip it clean\n",
    "    crs_title_ext = crs_title.extract_first().strip()\n",
    "    # Create a SelectorList of course descriptions text\n",
    "    crs_descr = response.css( 'p.course__description::text' )\n",
    "    # Extract the text and strip it clean\n",
    "    crs_descr_ext = crs_descr.extract_first().strip()\n",
    "    # Fill in the dictionary\n",
    "    dc_dict[crs_title_ext] = crs_descr_ext\n",
    "\n",
    "# Initialize the dictionary **outside** of the Spider class\n",
    "dc_dict = dict()\n",
    "\n",
    "# Run the Spider\n",
    "process = CrawlerProcess()\n",
    "process.crawl(DC_Description_Spider)\n",
    "process.start()\n",
    "\n",
    "# Print a preview of courses\n",
    "previewCourses(dc_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Capstone Crawler\n",
    "\n",
    "# parse method\n",
    "def parse(self, response):\n",
    "  # Extracted course titles\n",
    "  crs_titles = response.xpath('//h4[contains(@class,\"block__title\")]/text()').extract()\n",
    "  # Extracted course descriptions\n",
    "  crs_descrs = response.xpath('//p[contains(@class,\"block__description\")]/text()').extract()\n",
    "  # Fill in the dictionary\n",
    "  for crs_title, crs_descr in zip(crs_titles, crs_descrs):\n",
    "    dc_dict[crs_title] = crs_descr\n",
    "    \n",
    "    \n",
    "\n",
    "## Part 2\n",
    "\n",
    "# Import scrapy\n",
    "import scrapy\n",
    "\n",
    "# Import the CrawlerProcess\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# Create the Spider class\n",
    "class YourSpider(scrapy.Spider):\n",
    "  name = 'yourspider'\n",
    "  # start_requests method\n",
    "  def start_requests( self ):\n",
    "    yield scrapy.Request(url = url_short, callback = self.parse)\n",
    "      \n",
    "  def parse(self, response):\n",
    "    # My version of the parser you wrote in the previous part\n",
    "    crs_titles = response.xpath('//h4[contains(@class,\"block__title\")]/text()').extract()\n",
    "    crs_descrs = response.xpath('//p[contains(@class,\"block__description\")]/text()').extract()\n",
    "    for crs_title, crs_descr in zip( crs_titles, crs_descrs ):\n",
    "      dc_dict[crs_title] = crs_descr\n",
    "    \n",
    "# Initialize the dictionary **outside** of the Spider class\n",
    "dc_dict = dict()\n",
    "\n",
    "# Run the Spider\n",
    "process = CrawlerProcess()\n",
    "process.crawl(YourSpider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
