{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1 - Exploring and Preparing Loan Data\n",
    "\n",
    "In this first chapter, we will discuss the concept of credit risk and define how it is calculated. Using cross tables and plots, we will explore a real-world data set. Before applying machine learning, we will process this data by finding and resolving problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Explore the credit data\n",
    "\n",
    "# Look at the distribution of loan amounts with a histogram\n",
    "n, bins, patches = plt.hist(x=cr_loan['loan_amnt'], bins='auto', color='blue',alpha=0.7, rwidth=0.85)\n",
    "plt.xlabel(\"Loan Amount\")\n",
    "plt.show()\n",
    "\n",
    "_________________________________________________________\n",
    "\n",
    "print(\"There are 32 000 rows of data so the scatter plot may take a little while to plot.\")\n",
    "\n",
    "# Plot a scatter plot of income against age\n",
    "plt.scatter(cr_loan['person_income'], cr_loan['person_age'],c='blue', alpha=0.5)\n",
    "\n",
    "plt.xlabel('Personal Income')\n",
    "plt.ylabel('Persone Age')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Crosstab and pivot tables\n",
    "\n",
    "# Create a cross table of the loan intent and loan status\n",
    "print(pd.crosstab(cr_loan['loan_intent'], cr_loan['loan_status'], margins = True))\n",
    "___________________________________________________________________\n",
    "\n",
    "# Create a cross table of home ownership, loan status, and grade\n",
    "print(pd.crosstab(cr_loan['person_home_ownership'], [cr_loan['loan_status'], cr_loan['loan_grade']]))\n",
    "___________________________________________________________________\n",
    "\n",
    "# Create a cross table of home ownership, loan status, and average percent income\n",
    "print(pd.crosstab(cr_loan['person_home_ownership'], cr_loan['loan_status'],\n",
    "              values=cr_loan['loan_percent_income'], aggfunc='mean'))\n",
    "___________________________________________________________________\n",
    "\n",
    "# Create a box plot of percentage income by loan status\n",
    "cr_loan.boxplot(column = ['loan_percent_income'], by = 'loan_status')\n",
    "plt.title('Average Percent Income by Loan Status')\n",
    "plt.suptitle('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Finding outliers with cross tables\n",
    "\n",
    "# Create the cross table for loan status, home ownership, and the max employment length\n",
    "print(pd.crosstab(cr_loan['loan_status'],cr_loan['person_home_ownership'],\n",
    "                  values=cr_loan['person_emp_length'], aggfunc='max'))\n",
    "___________________________________________________________________\n",
    "\n",
    "# Create an array of indices where employment length is greater than 60\n",
    "indices = cr_loan[cr_loan['person_emp_length'] > 60].index\n",
    "\n",
    "____________________________________________________________________\n",
    "\n",
    "# Drop the records from the data based on the indices and create a new dataframe\n",
    "cr_loan_new = cr_loan.drop(indices)\n",
    "____________________________________________________________________\n",
    "\n",
    "# Create the cross table from earlier and include minimum employment length\n",
    "print(pd.crosstab(cr_loan_new['loan_status'], cr_loan_new['person_home_ownership'],\n",
    "            values=cr_loan_new['person_emp_length'], aggfunc=['min','max']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualizing credit outliers\n",
    "\n",
    "# Create the scatter plot for age and amount\n",
    "plt.scatter(cr_loan['person_age'],\n",
    "            cr_loan['loan_amnt'], c='blue', alpha=0.5)\n",
    "plt.xlabel(\"Person Age\")\n",
    "plt.ylabel(\"Loan Amount\")\n",
    "plt.show()\n",
    "\n",
    "_________________________________________________\n",
    "\n",
    "# Use Pandas to drop the record from the data frame and create a new one\n",
    "cr_loan_new = cr_loan.drop(cr_loan[cr_loan['person_age'] > 100].index)\n",
    "\n",
    "# Create a scatter plot of age and interest rate\n",
    "colors = [\"blue\",\"red\"]\n",
    "plt.scatter(cr_loan_new['person_age'], cr_loan_new['loan_int_rate'],\n",
    "            c = cr_loan_new['loan_status'],\n",
    "            cmap = matplotlib.colors.ListedColormap(colors),\n",
    "            alpha=0.5)\n",
    "plt.xlabel(\"Person Age\")\n",
    "plt.ylabel(\"Loan Interest Rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Replacing missing credit data\n",
    "\n",
    "# Print a null value column array\n",
    "print(cr_loan.columns[cr_loan.isnull().any()])\n",
    "\n",
    "# Print the top five rows with nulls for employment length\n",
    "print(cr_loan[cr_loan['person_emp_length'].isnull()].head())\n",
    "\n",
    "# Impute the null values with the median value for all employment lengths\n",
    "cr_loan['person_emp_length'].fillna((cr_loan['person_emp_length'].median()), inplace=True)\n",
    "\n",
    "# Create a histogram of employment length\n",
    "n, bins, patches = plt.hist(cr_loan['person_emp_length'], bins='auto', color='blue')\n",
    "plt.xlabel(\"Person Employment Length\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Removing missing data\n",
    "\n",
    "# Print the number of nulls\n",
    "print(cr_loan['loan_int_rate'].isnull().sum())\n",
    "\n",
    "# Store the array on indices\n",
    "indices = cr_loan[cr_loan['loan_int_rate'].isnull()].index\n",
    "\n",
    "# Save the new data without missing data\n",
    "cr_loan_clean = cr_loan.drop(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Missing data intuition\n",
    "\n",
    "cr_loan['person_home_ownership'].value_counts(dropna=False)\n",
    "\n",
    "# Replace the data with the value Other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2 - Logistic Regression for Defaults\n",
    "\n",
    "With the loan data fully prepared, we will discuss the logistic regression model which is a standard in risk modeling. We will understand the components of this model as well as how to score its performance. Once we've created predictions, we can explore the financial impact of utilizing this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Logistic regression basics\n",
    "\n",
    "# Create the X and y data sets\n",
    "X = cr_loan_clean[['loan_int_rate']]\n",
    "y = cr_loan_clean[['loan_status']]\n",
    "\n",
    "# Create and fit a logistic regression model\n",
    "clf_logistic_single = LogisticRegression()\n",
    "clf_logistic_single.fit(X, np.ravel(y))\n",
    "\n",
    "# Print the parameters of the model\n",
    "print(clf_logistic_single.get_params())\n",
    "\n",
    "# Print the intercept of the model\n",
    "print(clf_logistic_single.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Multivariate logistic regression\n",
    "\n",
    "# Create X data for the model\n",
    "X_multi = cr_loan_clean[['loan_int_rate', 'person_emp_length']]\n",
    "\n",
    "# Create a set of y data for training\n",
    "y = cr_loan_clean[['loan_status']]\n",
    "\n",
    "# Create and train a new logistic regression\n",
    "clf_logistic_multi = LogisticRegression(solver='lbfgs').fit(X_multi, np.ravel(y))\n",
    "\n",
    "# Print the intercept of the model\n",
    "print(clf_logistic_multi.intercept_)\n",
    "\n",
    "\n",
    "# <script.py> output:\n",
    "#     [-4.21645549]\n",
    "\n",
    "# O novo modelo clf_logistic_multi possui um valor .intercept_ \n",
    "# mais pr처ximo de zero. Isso significa que as probabilidades de \n",
    "# log de um n찾o padr찾o est찾o se aproximando de zero\n",
    "__________________________________________\n",
    "\n",
    "# This means the log odds of a non-default is approaching zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating training and test sets\n",
    "\n",
    "# Create the X and y data sets\n",
    "X = cr_loan_clean[['loan_int_rate','person_emp_length', 'person_income']]\n",
    "y = cr_loan_clean[['loan_status']]\n",
    "\n",
    "# Use test_train_split to create the training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4, random_state=123)\n",
    "\n",
    "# Create and fit the logistic regression model\n",
    "clf_logistic = LogisticRegression(solver='lbfgs').fit(X_train, np.ravel(y_train))\n",
    "\n",
    "# Print the models coefficients\n",
    "print(clf_logistic.coef_)\n",
    "\n",
    "# <script.py> output:\n",
    "#     [[ 1.28517496e-09 -2.27622202e-09 -2.17211991e-05]]\n",
    "\n",
    "# Do you see that three columns were used for training and there \n",
    "# are three values in .coef_? This tells you how important each \n",
    "# column, or feature, was for predicting. The more positive \n",
    "# the value, the more it predicts defaults. \n",
    "# Look at the value for loan_int_rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Changing coefficients\n",
    "\n",
    "# Print the first five rows of each training set\n",
    "print(X1_train.head())\n",
    "print(X2_train.head())\n",
    "\n",
    "# Create and train a model on the first training data\n",
    "clf_logistic1 = LogisticRegression(solver='lbfgs').fit(X1_train, np.ravel(y_train))\n",
    "\n",
    "# Create and train a model on the second training data\n",
    "clf_logistic2 = LogisticRegression(solver='lbfgs').fit(X2_train, np.ravel(y_train))\n",
    "\n",
    "# Print the coefficients of each model\n",
    "print(clf_logistic1.coef_)\n",
    "print(clf_logistic2.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## One-hot encoding credit data\n",
    "\n",
    "# Create two data sets for numeric and non-numeric data\n",
    "cred_num = cr_loan_clean.select_dtypes(exclude=['object'])\n",
    "cred_str = cr_loan_clean.select_dtypes(include=['object'])\n",
    "\n",
    "# One-hot encode the non-numeric columns\n",
    "cred_str_onehot = pd.get_dummies(cred_str)\n",
    "\n",
    "# Union the one-hot encoded columns to the numeric ones\n",
    "cr_loan_prep = pd.concat([cred_num, cred_str_onehot], axis=1)\n",
    "\n",
    "# Print the columns in the new data set\n",
    "print(cr_loan_prep.columns)\n",
    "\n",
    "# If you've ever seen a credit scorecard, the column_name_value \n",
    "# format should look familiar. If you haven't seen one, look up\n",
    "# some pictures during your next break!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predicting probability of default\n",
    "\n",
    "# Train the logistic regression model on the training data\n",
    "clf_logistic = LogisticRegression(solver='lbfgs').fit(X_train, np.ravel(y_train))\n",
    "\n",
    "# Create predictions of probability for loan status using test data\n",
    "preds = clf_logistic.predict_proba(X_test)\n",
    "\n",
    "# Create dataframes of first five predictions, and first five true labels\n",
    "preds_df = pd.DataFrame(preds[:,1][0:5], columns = ['prob_default'])\n",
    "true_df = y_test.head()\n",
    "\n",
    "# Concatenate and print the two data frames for comparison\n",
    "print(pd.concat([true_df.reset_index(drop = True), preds_df], axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Default classification reporting\n",
    "\n",
    "# Create a dataframe for the probabilities of default\n",
    "preds_df = pd.DataFrame(preds[:,1], columns = ['prob_default'])\n",
    "\n",
    "# Reassign loan status based on the threshold\n",
    "preds_df['loan_status'] = preds_df['prob_default'].apply(lambda x: 1 if x > .5 else 0)\n",
    "\n",
    "# Print the row counts for each loan status\n",
    "print(preds_df['loan_status'].value_counts())\n",
    "\n",
    "# Print the classification report\n",
    "target_names = ['Non-Default', 'Default']\n",
    "print(classification_report(y_test, preds_df['loan_status'], target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Selecting report metrics\n",
    "\n",
    "# Print the classification report\n",
    "target_names = ['Non-Default', 'Default']\n",
    "print(classification_report(y_test, preds_df['loan_status'], target_names=target_names))\n",
    "_________________________________________________\n",
    "\n",
    "# Print all the non-average values from the report\n",
    "print(precision_recall_fscore_support(y_test, preds_df['loan_status']))\n",
    "\n",
    "_________________________________________________\n",
    "\n",
    "# Print the first two numbers from the report\n",
    "print(precision_recall_fscore_support(y_test, preds_df['loan_status'])[:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visually scoring credit models\n",
    "\n",
    "# Create predictions and store them in a variable\n",
    "preds = clf_logistic.predict_proba(X_test)\n",
    "\n",
    "# Print the accuracy score the model\n",
    "print(clf_logistic.score(X_test, y_test))\n",
    "\n",
    "# Plot the ROC curve of the probabilities of default\n",
    "prob_default = preds[:, 1]\n",
    "fallout, sensitivity, thresholds = roc_curve(y_test, prob_default)\n",
    "plt.plot(fallout, sensitivity, color = 'darkorange')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.show()\n",
    "\n",
    "# Compute the AUC and store it in a variable\n",
    "auc = roc_auc_score(y_test, prob_default)\n",
    "\n",
    "## ROC AUC PLOT\n",
    "\n",
    "# what the ROC chart shows us is the tradeoff between\n",
    "# all values of our false positive rate (fallout) and true positive rate (sensitivity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Thresholds and confusion matrices\n",
    "\n",
    "# Set the threshold for defaults to 0.5\n",
    "preds_df['loan_status'] = preds_df['prob_default'].apply(lambda x: 1 if x > 0.5 else 0)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(confusion_matrix(y_test, preds_df['loan_status']))\n",
    "\n",
    "# <script.py> output:\n",
    "#     [[9023  175]\n",
    "#      [2152  434]]\n",
    "_______________________________________________________\n",
    "\n",
    "# Set the threshold for defaults to 0.4\n",
    "preds_df['loan_status'] = preds_df['prob_default'].apply(lambda x: 1 if x > .4 else 0)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(confusion_matrix(y_test, preds_df['loan_status']))\n",
    "\n",
    "# <script.py> output:\n",
    "#     [[8476  722]\n",
    "#      [1386 1200]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## How thresholds affect performance\n",
    "\n",
    "# Reassign the values of loan status based on the new threshold\n",
    "preds_df['loan_status'] = preds_df['prob_default'].apply(lambda x: 1 if x > .4 else 0)\n",
    "\n",
    "# Store the number of loan defaults from the prediction data\n",
    "num_defaults = preds_df['loan_status'].value_counts()[1]\n",
    "\n",
    "# Store the default recall from the classification report\n",
    "default_recall = precision_recall_fscore_support(y_test, preds_df['loan_status'])[1][1]\n",
    "\n",
    "# Calculate the estimated impact of the new default recall rate\n",
    "print(avg_loan_amnt * num_defaults * (1 - default_recall))\n",
    "\n",
    "____________________________________________________________\n",
    "\n",
    "# Loan Amount     Defaults Predicted / Not Predicted        Estimated Loss on Defaults\n",
    "#     $50                      .04 / .96                    (50000 x .96) x 50 = $2,400,000\n",
    "____________________________________________________________\n",
    "\n",
    "# By our estimates, this loss would be around $9.8 million. \n",
    "# That seems like a lot! Try rerunning this code with threshold \n",
    "# values of 0.3 and 0.5. Do you see the estimated losses changing? \n",
    "# How do we find a good threshold value based on these metrics alone?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Threshold selection\n",
    "\n",
    "plt.plot(thresh, def_recalls)\n",
    "plt.plot(thresh, nondef_recalls)\n",
    "plt.plot(thresh, accs)\n",
    "plt.xlabel(\"Probability Threshold\")\n",
    "plt.xticks(ticks)\n",
    "plt.legend([\"Default Recall\",\"Non-default Recall\",\"Model Accuracy\"])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "3 - Gradient Boosted Trees Using XGBoos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a model\n",
    "import xgboost as xgb\n",
    "clf_gbt = xgb.XGBClassifier().fit(X_train, np.ravel(y_train))\n",
    "\n",
    "# Predict with a model\n",
    "gbt_preds = clf_gbt.predict_proba(X_test)\n",
    "\n",
    "# Create dataframes of first five predictions, and first five true labels\n",
    "preds_df = pd.DataFrame(gbt_preds[:,1][0:5], columns = ['prob_default'])\n",
    "true_df = y_test.head()\n",
    "\n",
    "# Concatenate and print the two data frames for comparison\n",
    "print(pd.concat([true_df.reset_index(drop = True), preds_df], axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gradient boosted portfolio performance"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "   gbt_prob_default  lr_prob_default  lgd  loan_amnt\n",
    "0          0.940435         0.445779  0.2      15000\n",
    "1          0.922014         0.223447  0.2      11200\n",
    "2          0.021707         0.288558  0.2      15000\n",
    "3          0.026483         0.169358  0.2      10800\n",
    "4          0.064803         0.114182  0.2       3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Print the first five rows of the portfolio data frame\n",
    "print(portfolio.head())\n",
    "\n",
    "# Create expected loss columns for each model using the formula\n",
    "portfolio['gbt_expected_loss'] = portfolio['gbt_prob_default'] * portfolio['lgd'] * portfolio['loan_amnt']\n",
    "portfolio['lr_expected_loss'] = portfolio['lr_prob_default'] * portfolio['lgd'] * portfolio['loan_amnt']\n",
    "\n",
    "# Print the sum of the expected loss for lr\n",
    "print('LR expected loss: ', np.sum(portfolio['lr_expected_loss']))\n",
    "\n",
    "# Print the sum of the expected loss for gbt\n",
    "print('GBT expected loss: ', np.sum(portfolio['gbt_expected_loss']))\n",
    "\n",
    "\n",
    "#    LR expected loss:  5596776.980\n",
    "#    GBT expected loss:  5447712.942"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "A data frame called portfolio has been created to combine the probabilities of default for both models, the loss given default (assume 20% for now), and the loan_amnt which will be assumed to be the exposure at default\n",
    "\n",
    "Formula\n",
    "Expected Loss = PD(prob_default) * LGD(loss given default) * Loan_amount (or mean amount) "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "LGB = Loss given default or LGD is the share of an asset that is lost if a borrower defaults. It is a common parameter in risk models and also a parameter used in the calculation of economic capital, expected loss or regulatory capital under Basel II for a banking institution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Assessing gradient boosted trees\n",
    "\n",
    "# Predict the labels for loan status\n",
    "gbt_preds = clf_gbt.predict(X_test)\n",
    "\n",
    "# Check the values created by the predict method\n",
    "print(gbt_preds)\n",
    "\n",
    "# Print the classification report of the model\n",
    "target_names = ['Non-Default', 'Default']\n",
    "print(classification_report(y_test, gbt_preds, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Column importance and default prediction\n",
    "\n",
    "# Create and train the model on the training data\n",
    "clf_gbt = xgb.XGBClassifier().fit(X_train, np.ravel(y_train))\n",
    "\n",
    "# Print the column importances from the model\n",
    "print(clf_gbt.get_booster().get_score(importance_type = 'weight'))\n",
    "\n",
    "\n",
    "# <script.py> output:\n",
    "#     {'loan_percent_income': 121, 'loan_int_rate': 183, 'person_income': 278, \n",
    "#      'person_home_ownership_MORTGAGE': 39, 'loan_amnt': 47, 'loan_grade_F': 6}\n",
    "_____________________________________________________________\n",
    "\n",
    "# So, the importance for loan_grade_F is only 6 in this case. \n",
    "# This could be because there are so few of the F-grade loans. \n",
    "# While the F-grade loans don't add much to predictions here, \n",
    "# they might affect the importance of other training columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualizing column importance\n",
    "\n",
    "# Train a model on the X data with 2 columns\n",
    "clf_gbt2 = xgb.XGBClassifier().fit(X2_train, np.ravel(y_train))\n",
    "\n",
    "# Plot the column importance for this model\n",
    "xgb.plot_importance(clf_gbt2, importance_type = 'weight')\n",
    "plt.show()\n",
    "________________________________________________________\n",
    "\n",
    "# Train a model on the X data with 3 columns\n",
    "clf_gbt3 = xgb.XGBClassifier().fit(X3_train,np.ravel(y_train))\n",
    "\n",
    "# Plot the column importance for this model\n",
    "xgb.plot_importance(clf_gbt3, importance_type = 'weight')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Column selection and model performance\n",
    "\n",
    "# Predict the loan_status using each model\n",
    "gbt_preds = gbt.predict(X_test)\n",
    "gbt2_preds = gbt2.predict(X2_test)\n",
    "\n",
    "# Print the classification report of the first model\n",
    "target_names = ['Non-Default', 'Default']\n",
    "print(classification_report(y_test, gbt_preds, target_names=target_names))\n",
    "\n",
    "# Print the classification report of the second model\n",
    "print(classification_report(y_test, gbt2_preds, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cross validating credit models\n",
    "\n",
    "# Set the values for number of folds and stopping iterations\n",
    "n_folds = 5\n",
    "early_stopping = 10\n",
    "\n",
    "# Create the DTrain matrix for XGBoost\n",
    "DTrain = xgb.DMatrix(X_train, label = y_train)\n",
    "\n",
    "# Create the data frame of cross validations\n",
    "cv_df = xgb.cv(params, DTrain, num_boost_round = 5, nfold=n_folds,\n",
    "            early_stopping_rounds=early_stopping)\n",
    "\n",
    "# Print the cross validations data frame\n",
    "print(cv_df)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<script.py> output:\n",
    "       train-auc-mean  train-auc-std  test-auc-mean  test-auc-std\n",
    "    0        0.898182       0.001318       0.892519      0.004650\n",
    "    1        0.909256       0.002052       0.902780      0.005053\n",
    "    2        0.913621       0.002205       0.906834      0.004423\n",
    "    3        0.918600       0.001092       0.910779      0.005221\n",
    "    4        0.922251       0.001818       0.914193      0.004422"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Limits to cross-validation testing\n",
    "\n",
    "# Print the first five rows of the CV results data frame\n",
    "print(cv_results_big.head())\n",
    "\n",
    "# Calculate the mean of the test AUC scores\n",
    "print(np.mean(cv_results_big['test-auc-mean']).round(2))\n",
    "\n",
    "# Plot the test AUC scores for each iteration\n",
    "plt.plot(cv_results_big['test-auc-mean'])\n",
    "plt.title('Test AUC Score Over 600 Iterations')\n",
    "plt.xlabel('Iteration Number')\n",
    "plt.ylabel('Test AUC Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cross-validation scoring\n",
    "\n",
    "# Create a gradient boosted tree model using two hyperparameters\n",
    "gbt = xgb.XGBClassifier(learning_rate = .1, max_depth = 7)\n",
    "\n",
    "# Calculate the cross validation scores for 4 folds\n",
    "cv_scores = cross_val_score(gbt, X_train, np.ravel(y_train), cv = 4)\n",
    "\n",
    "# Print the cross validation scores\n",
    "print(cv_scores)\n",
    "\n",
    "# Print the average accuracy and standard deviation of the scores\n",
    "print(\"Average accuracy: %0.2f (+/- %0.2f)\" % (cv_scores.mean(),\n",
    "                                               cv_scores.std() * 2))\n",
    "\n",
    "# <script.py> output:\n",
    "#     [0.94095023 0.93369541 0.93186962 0.92462653]\n",
    "#     Average accuracy: 0.93 (+/- 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Undersampling training data\n",
    "\n",
    "# Create data sets for defaults and non-defaults\n",
    "nondefaults = X_y_train[X_y_train['loan_status'] == 0]\n",
    "defaults = X_y_train[X_y_train['loan_status'] == 1]\n",
    "\n",
    "# Undersample the non-defaults\n",
    "nondefaults_under = nondefaults.sample(len(defaults))\n",
    "\n",
    "# Concatenate the undersampled nondefaults with defaults\n",
    "X_y_train_under = pd.concat([nondefaults_under.reset_index(drop = True),\n",
    "                             defaults.reset_index(drop = True)], axis = 0)\n",
    "\n",
    "# Print the value counts for loan status\n",
    "print(X_y_train_under['loan_status'].value_counts())\n",
    "\n",
    "\n",
    "# <script.py> output:\n",
    "#     1    3877\n",
    "#     0    3877\n",
    "#     Name: loan_status, dtype: int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Undersampled tree performance\n",
    "\n",
    "# Check the classification reports\n",
    "target_names = ['Non-Default', 'Default']\n",
    "print(classification_report(y_test, gbt_preds, target_names=target_names))\n",
    "print(classification_report(y_test, gbt2_preds, target_names=target_names))\n",
    "\n",
    "# <script.py> output:\n",
    "#                   precision    recall  f1-score   support\n",
    "#     \n",
    "#      Non-Default       0.93      0.99      0.96      9198\n",
    "#          Default       0.95      0.73      0.83      2586\n",
    "#     \n",
    "#        micro avg       0.93      0.93      0.93     11784\n",
    "#        macro avg       0.94      0.86      0.89     11784\n",
    "#     weighted avg       0.93      0.93      0.93     11784\n",
    "    \n",
    "#                   precision    recall  f1-score   support\n",
    "#     \n",
    "#      Non-Default       0.95      0.91      0.93      9198\n",
    "#          Default       0.72      0.84      0.77      2586\n",
    "    \n",
    "#        micro avg       0.89      0.89      0.89     11784\n",
    "#        macro avg       0.83      0.87      0.85     11784\n",
    "#     weighted avg       0.90      0.89      0.89     11784\n",
    "______________________________________________________\n",
    "\n",
    "# Print the confusion matrix for both old and new models\n",
    "print(confusion_matrix(y_test, gbt_preds))\n",
    "print(confusion_matrix(y_test, gbt2_preds))\n",
    "\n",
    "# <script.py> output:\n",
    "#     0.8613405315086655\n",
    "#     0.870884117348218\n",
    "______________________________________________________\n",
    "\n",
    "# Print and compare the AUC scores of the old and new models\n",
    "print(roc_auc_score(y_test, gbt_preds))\n",
    "print(roc_auc_score(y_test, gbt2_preds))\n",
    "\n",
    "# <script.py> output:\n",
    "#     0.8613405315086655\n",
    "#     0.870884117348218"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Evaluation and Implementation\n",
    "\n",
    "# Print the logistic regression classification report\n",
    "target_names = ['Non-Default', 'Default']\n",
    "print(classification_report(y_test, preds_df_lr['loan_status'], target_names=target_names))\n",
    "\n",
    "# Print the gradient boosted tree classification report\n",
    "print(classification_report(y_test, preds_df_gbt['loan_status'], target_names=target_names))\n",
    "\n",
    "# Print the default F-1 scores for the logistic regression\n",
    "print(precision_recall_fscore_support(y_test, preds_df_lr['loan_status'], average = 'macro')[2])\n",
    "\n",
    "# Print the default F-1 scores for the gradient boosted tree\n",
    "print(precision_recall_fscore_support(y_test, preds_df_gbt['loan_status'], average = 'macro')[2])\n",
    "\n",
    "________________________________________________________________\n",
    "# <script.py> output:\n",
    "#                   precision    recall  f1-score   support\n",
    "#     \n",
    "#      Non-Default       0.86      0.92      0.89      9198\n",
    "#          Default       0.62      0.46      0.53      2586\n",
    "    \n",
    "#        micro avg       0.82      0.82      0.82     11784\n",
    "#        macro avg       0.74      0.69      0.71     11784\n",
    "#     weighted avg       0.81      0.82      0.81     11784\n",
    "    \n",
    "#                   precision    recall  f1-score   support\n",
    "    \n",
    "#      Non-Default       0.93      0.99      0.96      9198\n",
    "#          Default       0.94      0.73      0.82      2586\n",
    "    \n",
    "#        micro avg       0.93      0.93      0.93     11784\n",
    "#        macro avg       0.93      0.86      0.89     11784\n",
    "#     weighted avg       0.93      0.93      0.93     11784\n",
    "    \n",
    "#     0.7108943782814463\n",
    "#    0.8909014142736051"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "4 - Model Evaluation and Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Comparing with ROCs\n",
    "\n",
    "# ROC chart components\n",
    "fallout_lr, sensitivity_lr, thresholds_lr = roc_curve(y_test, clf_logistic_preds)\n",
    "fallout_gbt, sensitivity_gbt, thresholds_gbt = roc_curve(y_test, clf_logistic_preds)\n",
    "\n",
    "# ROC Chart with both\n",
    "plt.plot(fallout_lr, sensitivity_lr, color = 'blue', label='%s' % 'Logistic Regression')\n",
    "plt.plot(fallout_gbt, sensitivity_gbt, color = 'green', label='%s' % 'GBT')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', label='%s' % 'Random Prediction')\n",
    "plt.title(\"ROC Chart for LR and GBT on the Probability of Default\")\n",
    "plt.xlabel('Fall-out')\n",
    "plt.ylabel('Sensitivity')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "________________________________________________________________\n",
    "\n",
    "# Print the logistic regression AUC with formatting\n",
    "print(\"Logistic Regression AUC Score: %0.2f\" % roc_auc_score(y_test, clf_logistic_preds))\n",
    "\n",
    "# Print the gradient boosted tree AUC with formatting\n",
    "print(\"Gradient Boosted Tree AUC Score: %0.2f\" % roc_auc_score(y_test, clf_gbt_preds))\n",
    "\n",
    "# <script.py> output:\n",
    "#     Logistic Regression AUC Score: 0.76\n",
    "#     Gradient Boosted Tree AUC Score: 0.94"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calibration curves\n",
    "\n",
    "# Add the calibration curve for the gradient boosted tree\n",
    "plt.plot([0, 1], [0, 1], 'k:', label='Perfectly calibrated')    \n",
    "plt.plot(mean_pred_val_lr, frac_of_pos_lr,\n",
    "         's-', label='%s' % 'Logistic Regression')\n",
    "plt.plot(mean_pred_val_gbt, frac_of_pos_gbt,\n",
    "         's-', label='%s' % \"Gradient Boosted tree\")\n",
    "plt.ylabel('Fraction of positives')\n",
    "plt.xlabel('Average Predicted Probability')\n",
    "plt.legend()\n",
    "plt.title('Calibration Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Acceptance rates\n",
    "\n",
    "# Check the statistics of the probabilities of default\n",
    "print(test_pred_df['prob_default'].describe())\n",
    "\n",
    "# Calculate the threshold for a 85% acceptance rate\n",
    "threshold_85 = np.quantile(test_pred_df['prob_default'], .85)\n",
    "\n",
    "# Apply acceptance rate threshold\n",
    "test_pred_df['pred_loan_status'] = test_pred_df['prob_default'].apply(lambda x: 1 if x > threshold_85 else 0)\n",
    "\n",
    "# Print the counts of loan status after the threshold\n",
    "print(test_pred_df['pred_loan_status'].count())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<script.py> output:\n",
    "    count    11784.000000\n",
    "    mean         0.216866\n",
    "    std          0.333038\n",
    "    min          0.000354\n",
    "    25%          0.022246\n",
    "    50%          0.065633\n",
    "    75%          0.177804\n",
    "    max          0.999557\n",
    "    Name: prob_default, dtype: float64\n",
    "    0    10016\n",
    "    1     1768\n",
    "    Name: pred_loan_status, dtype: int64\n",
    "\n",
    "In the results of .describe() do you see how it's not until 75% that you start to see double-digit numbers? That's because the majority of our test set is non-default loans. Next let's look at how the acceptance rate and threshold split up the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualizing quantiles of acceptance\n",
    "\n",
    "# Plot the predicted probabilities of default\n",
    "plt.hist(clf_gbt_preds, color = 'blue', bins = 40)\n",
    "\n",
    "# Calculate the threshold with quantile\n",
    "threshold = np.quantile(clf_gbt_preds, .85)\n",
    "\n",
    "# Add a reference line to the plot for the threshold\n",
    "plt.axvline(x =threshold, color = 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bad rates\n",
    "\n",
    "# Print the top 5 rows of the new data frame\n",
    "print(test_pred_df.head())\n",
    "\n",
    "# Create a subset of only accepted loans\n",
    "accepted_loans = test_pred_df[test_pred_df['pred_loan_status'] == 0]\n",
    "\n",
    "# Calculate the bad rate\n",
    "print(np.sum(accepted_loans['true_loan_status']) / accepted_loans['true_loan_status'].count())\n",
    "\n",
    "# <script.py> output:\n",
    "#        true_loan_status  prob_default  pred_loan_status\n",
    "#     0                 1      0.982387                 1\n",
    "#     1                 1      0.975163                 1\n",
    "#     2                 0      0.003474                 0\n",
    "#     3                 0      0.005457                 0\n",
    "#     4                 1      0.119876                 0\n",
    "\n",
    "\n",
    "#     0.08256789137380191\n",
    "\n",
    "___________________________________________________________________\n",
    "\n",
    "# This bad rate doesn't look half bad! The bad rate with the threshold \n",
    "# set by the 85% quantile() is about 8%. This means that of all the \n",
    "# loans we've decided to accept from the test set, only 8% were \n",
    "# actual defaults! If we accepted all loans, the percentage \n",
    "# of defaults would be around 22%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Acceptance rate impact\n",
    "\n",
    "# Print the statistics of the loan amount column\n",
    "print(test_pred_df['loan_amnt'].describe())\n",
    "\n",
    "# Store the average loan amount\n",
    "avg_loan = np.mean(test_pred_df['loan_amnt'])\n",
    "\n",
    "# Set the formatting for currency, and print the cross tab\n",
    "pd.options.display.float_format = '${:,.2f}'.format\n",
    "print(pd.crosstab(test_pred_df['true_loan_status'],\n",
    "                 test_pred_df['pred_loan_status_15']).apply(lambda x: x * avg_loan, axis = 0))\n",
    "\n",
    "_______________________________________________________________\n",
    "\n",
    "# <script.py> output:\n",
    "#     count    11784.000000\n",
    "#     mean      9556.283944\n",
    "#     std       6238.005674\n",
    "#     min        500.000000\n",
    "#     25%       5000.000000\n",
    "#     50%       8000.000000\n",
    "#     75%      12000.000000\n",
    "#     max      35000.000000\n",
    "#     Name: loan_amnt, dtype: float64\n",
    "\n",
    "\n",
    "#     pred_loan_status_15              0              1\n",
    "#     true_loan_status                                 \n",
    "#     0                   $87,812,693.16     $86,006.56\n",
    "#     1                    $7,903,046.82 $16,809,503.46\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Making the strategy table\n",
    "\n",
    "accept_rates = [1.0, 0.95, 0.9, 0.85, 0.8, 0.75, 0.7, 0.65, 0.6, 0.55,\n",
    "                0.5, 0.45, 0.4, 0.35, 0.3, 0.25, 0.2, 0.15, 0.1, 0.05]\n",
    "\n",
    "# Create lists to store thresholds and bad rates\n",
    "thresholds = []\n",
    "bad_rates = []\n",
    "\n",
    "# Populate the arrays for the strategy table with a for loop\n",
    "for rate in accept_rates:\n",
    "    \n",
    "    # Calculate the threshold for the acceptance rate\n",
    "    thresh = np.quantile(preds_df_gbt['prob_default'], \\\n",
    "                         rate).round(3)\n",
    "    \n",
    "    # Add the threshold value to the list of thresholds\n",
    "    thresholds.append(np.quantile(preds_df_gbt['prob_default'],\\\n",
    "                                  rate).round(3))\n",
    "    \n",
    "    # Reassign the loan_status value using the threshold\n",
    "    test_pred_df['pred_loan_status'] = test_pred_df['prob_default']\\\n",
    "                                        .apply(lambda x: 1 if x > thresh else 0)\n",
    "    \n",
    "    # Create a set of accepted loans using this acceptance rate\n",
    "    accepted_loans = test_pred_df[test_pred_df['pred_loan_status'] == 0]\n",
    "    \n",
    "    # Calculate and append the bad rate using the acceptance rate\n",
    "    bad_rates.append(np.sum((accepted_loans['true_loan_status']) /\\\n",
    "                            len(accepted_loans['true_loan_status'])).round(3))\n",
    "    \n",
    "# Create a data frame of the strategy table\n",
    "strat_df = pd.DataFrame(zip(accept_rates, thresholds, bad_rates),\n",
    "                        columns = ['Acceptance Rate','Threshold','Bad Rate'])\n",
    "\n",
    "# Print the entire table\n",
    "print(strat_df)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<script.py> output:\n",
    "        Acceptance Rate  Threshold  Bad Rate\n",
    "    0              1.00      1.000     0.219\n",
    "    1              0.95      0.992     0.179\n",
    "    2              0.90      0.976     0.132\n",
    "    3              0.85      0.804     0.083\n",
    "    4              0.80      0.254     0.061\n",
    "    5              0.75      0.178     0.052\n",
    "    6              0.70      0.138     0.043\n",
    "    7              0.65      0.111     0.036\n",
    "    8              0.60      0.093     0.030\n",
    "    9              0.55      0.078     0.027\n",
    "    10             0.50      0.066     0.023\n",
    "    11             0.45      0.055     0.020\n",
    "    12             0.40      0.045     0.017\n",
    "    13             0.35      0.037     0.014\n",
    "    14             0.30      0.030     0.010\n",
    "    15             0.25      0.022     0.008\n",
    "    16             0.20      0.015     0.005\n",
    "    17             0.15      0.008     0.001\n",
    "    18             0.10      0.004     0.000\n",
    "    19             0.05      0.002     0.000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualizing the strategy\n",
    "\n",
    "# Visualize the distributions in the strategy table with a boxplot\n",
    "strat_df.boxplot()\n",
    "plt.show()\n",
    "______________________________________\n",
    "\n",
    "# Plot the strategy curve\n",
    "plt.plot(strat_df['Acceptance Rate'], \n",
    "         strat_df['Bad Rate'])\n",
    "plt.xlabel('Acceptance Rate')\n",
    "plt.ylabel('Bad Rate')\n",
    "plt.title('Acceptance and Bad Rates')\n",
    "plt.axes().yaxis.grid()\n",
    "plt.axes().xaxis.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "Column\t------------ Description\n",
    "Num Accepted Loans\t The number of accepted loans based on the threshold\n",
    "Avg Loan Amnt ------ The average loan amount of the entire test set\n",
    "Estimated value ---- The estimated net value of non-defaults minus defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Estimated value profiling\n",
    "\n",
    "# Print the row with the max estimated value\n",
    "print(strat_df.loc[strat_df['Estimated Value'] == np.max(strat_df['Estimated Value'])])\n",
    "\n",
    "# <script.py> output:\n",
    "#        Acceptance Rate  Threshold  Bad Rate  Num Accepted Loans  Avg Loan Amnt  Estimated Value\n",
    "#     3             0.85      0.804     0.083                9390        9556.28      74837713.31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Total expected loss\n",
    "\n",
    "# Print the first five rows of the data frame\n",
    "print(test_pred_df.head())\n",
    "\n",
    "# Calculate the bank's expected loss and assign it to a new column\n",
    "test_pred_df['expected_loss'] = test_pred_df['prob_default'] * test_pred_df['loss_given_default'] * test_pred_df['loan_amnt']\n",
    "\n",
    "# Calculate the total expected loss to two decimal places\n",
    "tot_exp_loss = round(np.sum(test_pred_df['expected_loss']),2)\n",
    "\n",
    "# Print the total expected loss\n",
    "print('Total expected loss: ', '${:,.2f}'.format(tot_exp_loss))\n",
    "\n",
    "\n",
    "# <script.py> output:\n",
    "#        true_loan_status  prob_default  loan_amnt  loss_given_default\n",
    "#     0                 1      0.982387      15000                 1.0\n",
    "#     1                 1      0.975163      11200                 1.0\n",
    "#     2                 0      0.003474      15000                 1.0\n",
    "#     3                 0      0.005457      10800                 1.0\n",
    "#     4                 1      0.119876       3000                 1.0\n",
    "\n",
    "#     Total expected loss:  $27,084,153.38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu-2.0",
   "language": "python",
   "name": "tf_gpu2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
