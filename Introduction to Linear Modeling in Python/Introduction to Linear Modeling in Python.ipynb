{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Introduction to Linear Modeling in Python"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Course Description\n",
    "One of the primary goals of any scientist is to find patterns in data and build models to describe, predict, and extract insight from those patterns. The most fundamental of these patterns is a linear relationship between two variables. This course provides an introduction to exploring, quantifying, and modeling linear relationships in data, by demonstrating techniques such as least-squares, linear regression, estimatation, and bootstrap resampling. Here you will apply the most powerful modeling tools in the python data science ecosystem, including scipy, statsmodels, and scikit-learn, to build and evaluate linear models. By exploring the concepts and applications of linear models with python, this course serves as both a practical introduction to modeling, and as a foundation for learning more advanced modeling techniques and tools in statistics and machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODULE 1\n",
    "\n",
    "### Exploring Linear Trends"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We start the course with an initial exploration of linear relationships, including some motivating examples of how linear models are used, and demonstrations of data visualization methods from matplotlib. We then use descriptive statistics to quantify the shape of our data and use correlation to quantify the strength of linear relationships between two variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reasons for Modeling: Interpolation\n",
    "    \n",
    "# Compute the total change in distance and change in time\n",
    "total_distance = distances[-1] - distances[0]\n",
    "total_time = times[-1] - times[0]\n",
    "\n",
    "# Estimate the slope of the data from the ratio of the changes\n",
    "average_speed = total_distance / total_time\n",
    "\n",
    "# Predict the distance traveled for a time not measured\n",
    "elapse_time = 2.5\n",
    "distance_traveled = average_speed * elapse_time\n",
    "print(\"The distance traveled is {}\".format(distance_traveled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reasons for Modeling: Extrapolation\n",
    "\n",
    "# Select a time not measured.\n",
    "time = 8\n",
    "\n",
    "# Use the model to compute a predicted distance for that time.\n",
    "distance = model(8)\n",
    "\n",
    "# Inspect the value of the predicted distance traveled.\n",
    "print(distance)\n",
    "\n",
    "# Determine if you will make it without refueling.\n",
    "answer = (distance <= 400)\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reasons for Modeling: Estimating Relationships\n",
    "\n",
    "# Complete the function to model the efficiency.\n",
    "def efficiency_model(miles, gallons):\n",
    "   return np.mean( miles / gallons )\n",
    "\n",
    "# Use the function to estimate the efficiency for each car.\n",
    "car1['mpg'] = efficiency_model( car1['miles'] , car1['gallons'] )\n",
    "car2['mpg'] = efficiency_model( car2['miles'] , car2['gallons'] )\n",
    "\n",
    "# Finish the logic statement to compare the car efficiencies.\n",
    "if car1['mpg'] > car2['mpg'] :\n",
    "    print('car1 is the best')\n",
    "elif car1['mpg'] < car2['mpg'] :\n",
    "    print('car2 is the best')\n",
    "else:\n",
    "    print('the cars have the same efficiency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create figure and axis objects using subplots()\n",
    "fig, axis = plt.subplots()\n",
    "\n",
    "# Plot line using the axis.plot() method\n",
    "line = axis.plot(times , distances , linestyle=\" \", marker=\"o\", color=\"red\")\n",
    "\n",
    "# Use the plt.show() method to display the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting the Model on the Data\n",
    "\n",
    "# Pass times and measured distances into model\n",
    "model_distances = model(times, distances)\n",
    "\n",
    "# Create figure and axis objects and call axis.plot() twice to plot data and model distances versus times\n",
    "fig, axis = plt.subplots()\n",
    "axis.plot(times, measured_distances, linestyle=\" \", marker=\"o\", color=\"black\", label=\"Measured\")\n",
    "axis.plot(times, model_distances, linestyle=\"'-'\", marker=None, color=\"red\", label=\"Modeled\")\n",
    "\n",
    "# Add grid lines and a legend to your plot, and then show to display\n",
    "axis.grid(True)\n",
    "axis.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting the Model on the Data\n",
    "\n",
    "# Pass times and measured distances into model\n",
    "model_distances = model(times, measured_distances)\n",
    "\n",
    "# Create figure and axis objects and call axis.plot() twice to plot data and model distances versus times\n",
    "fig, axis = plt.subplots()\n",
    "axis.plot(times, measured_distances, linestyle=\" \", marker=\"o\", color=\"black\", label=\"Measured\")\n",
    "axis.plot(times, model_distances, linestyle='-', marker=None, color=\"red\", label=\"Modeled\")\n",
    "\n",
    "# Add grid lines and a legend to your plot, and then show to display\n",
    "axis.grid(True)\n",
    "axis.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visually Estimating the Slope & Intercept\n",
    "\n",
    "# Look at the plot data and guess initial trial values\n",
    "trial_slope = 1\n",
    "trial_intercept = 2\n",
    "\n",
    "# input thoses guesses into the model function to compute the model values.\n",
    "xm, ym = model(trial_intercept, trial_slope)\n",
    "\n",
    "# Compare your your model to the data with the plot function\n",
    "fig = plot_data_and_model(xd, yd, xm, ym)\n",
    "plt.show()\n",
    "\n",
    "# Repeat the steps above until your slope and intercept guess makes the model line up with the data.\n",
    "final_slope = 1\n",
    "final_intercept = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Mean, Deviation, & Standard Deviation\n",
    "\n",
    "# Compute the deviations by subtracting the mean offset\n",
    "dx = x - np.mean(x)\n",
    "dy = y - np.mean(y)\n",
    "\n",
    "# Normalize the data by dividing the deviations by the standard deviation\n",
    "zx = dx / np.std(x)\n",
    "zy = dy / np.std(y)\n",
    "\n",
    "# Plot comparisons of the raw data and the normalized data\n",
    "fig = plot_cdfs(dx, dy, zx, zy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Covariance vs Correlation\n",
    "\n",
    "# Compute the covariance from the deviations.\n",
    "dx = x - np.mean(x)\n",
    "dy = y - np.mean(y)\n",
    "covariance = np.mean(dx * dy)\n",
    "print(\"Covariance: \", covariance)\n",
    "\n",
    "# Compute the correlation from the normalized deviations.\n",
    "zx = dx / np.std(x)\n",
    "zy = dy / np.std(y)\n",
    "correlation = np.mean(zx * zy)\n",
    "print(\"Correlation: \", correlation)\n",
    "\n",
    "# Plot the normalized deviations for visual inspection. \n",
    "fig = plot_normalized_deviations(zx, zy)\n",
    "\n",
    "## <script.py> output:\n",
    "##     Covariance:  69.6798182602\n",
    "##     Correlation:  0.982433369757"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Correlation Strength\n",
    "\n",
    "# Complete the function that will compute correlation.\n",
    "def correlation(x,y):\n",
    "    x_dev = x - np.mean(x)\n",
    "    y_dev = y - np.mean(y)\n",
    "    x_norm = x_dev / np.std(x)\n",
    "    y_norm = y_dev / np.std(y)\n",
    "    return np.mean(x_norm * y_norm)\n",
    "\n",
    "# Compute and store the correlation for each data set in the list.\n",
    "for name, data in data_sets.items():\n",
    "    data['correlation'] = correlation(data['x'], data['y'])\n",
    "    print('data set {} has correlation {:.2f}'.format(name, data['correlation']))\n",
    "\n",
    "# Assign the data set with the best correlation.\n",
    "best_data = data_sets['A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MODULE 2\n",
    "\n",
    "## Building Linear Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Here we look at the parts that go into building a linear model. Using the concept of a Taylor Series, we focus on the parameters slope and intercept, how they define the model, and how to interpret the them in several applied contexts. We apply a variety of python modules to find the model that best fits the data, by computing the optimal values of slope and intercept, using least-squares, numpy, statsmodels, and scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Components\n",
    "\n",
    "# Define the general model as a function\n",
    "def model(x, a0=3, a1=2, a2=0):\n",
    "    return a0 + (a1*x) + (a2*x*x)\n",
    "\n",
    "# Generate array x, then predict ym values for specific, non-default a0 and a1\n",
    "x = np.linspace(-10, 10, 21)\n",
    "ym = model(x)\n",
    "\n",
    "# Plot the results, ym versus x\n",
    "fig = plot_prediction(x, ym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Parameters\n",
    "\n",
    "# Complete the plotting function definition\n",
    "def plot_data_with_model(xd, yd, ym):\n",
    "    fig = plot_data(xd, yd)  # plot measured data\n",
    "    fig.axes[0].plot(xd, ym, color='red')  # over-plot modeled data\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "# Select new model parameters a0, a1, and generate modeled `ym` from them.\n",
    "a0 = 128\n",
    "a1 = 25\n",
    "ym = model(xd, a0, a1)\n",
    "\n",
    "# Plot the resulting model to see whether it fits the data\n",
    "fig = plot_data_with_model(xd, yd, ym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Linear Proportionality\n",
    "\n",
    "# Complete the function to convert C to F\n",
    "def convert_scale(temps_C):\n",
    "    (freeze_C, boil_C) = (0, 100)\n",
    "    (freeze_F, boil_F) = (32, 212)\n",
    "    change_in_C = boil_C - freeze_C\n",
    "    change_in_F = boil_F - freeze_F\n",
    "    slope = boil_F/ boil_C\n",
    "    intercept = freeze_F - freeze_C\n",
    "    temps_F = intercept + (slope * temps_C)\n",
    "    return temps_F\n",
    "\n",
    "# Use the convert function to compute values of F and plot them\n",
    "temps_C = np.linspace(0, 100, 101)\n",
    "temps_F = convert_scale(temps_C)\n",
    "fig = plot_temperatures(temps_C, temps_F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Slope and Rates-of-Change\n",
    "\n",
    "# Compute an array of velocities as the slope between each point\n",
    "diff_distances = np.diff(distances)\n",
    "diff_times = np.diff(times)\n",
    "velocities = diff_distances / diff_times\n",
    "\n",
    "# Chracterize the center and spread of the velocities\n",
    "v_avg = np.mean(velocities)\n",
    "v_max = np.max(velocities)\n",
    "v_min = np.min(velocities)\n",
    "v_range = v_max - v_min\n",
    "\n",
    "# Plot the distribution of velocities\n",
    "fig = plot_velocity_timeseries(times[1:], velocities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Intercept and Starting Points\n",
    "\n",
    "# Import ols from statsmodels, and fit a model to the data\n",
    "from statsmodels.formula.api import ols\n",
    "model_fit = ols(formula=\" masses ~ volumes\", data=df)\n",
    "model_fit = model_fit.fit()\n",
    "\n",
    "# Extract the model parameter values, and assign them to a0, a1\n",
    "a0 = model_fit.params['Intercept']\n",
    "a1 = model_fit.params['volumes']\n",
    "\n",
    "# Print model parameter values with meaningful names, and compare to summary()\n",
    "print( \"container_mass   = {:0.4f}\".format(a0) )\n",
    "print( \"solution_density = {:0.4f}\".format(a1) )\n",
    "\n",
    "print( model_fit.summary() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<script.py> output:\n",
    "    container_mass   = 5.4349\n",
    "    solution_density = 1.1029\n",
    "                                OLS Regression Results                            \n",
    "    ==============================================================================\n",
    "    Dep. Variable:                 masses   R-squared:                       0.999\n",
    "    Model:                            OLS   Adj. R-squared:                  0.999\n",
    "    Method:                 Least Squares   F-statistic:                 1.328e+05\n",
    "    Date:                Thu, 16 May 2019   Prob (F-statistic):          1.19e-156\n",
    "    Time:                        19:09:14   Log-Likelihood:                 102.39\n",
    "    No. Observations:                 101   AIC:                            -200.8\n",
    "    Df Residuals:                      99   BIC:                            -195.5\n",
    "    Df Model:                           1                                         \n",
    "    Covariance Type:            nonrobust                                         \n",
    "    ==============================================================================\n",
    "                     coef    std err          t      P>|t|      [0.025      0.975]\n",
    "    ------------------------------------------------------------------------------\n",
    "    Intercept      5.4349      0.023    236.805      0.000       5.389       5.480\n",
    "    volumes        1.1029      0.003    364.408      0.000       1.097       1.109\n",
    "    ==============================================================================\n",
    "    Omnibus:                        0.319   Durbin-Watson:                   2.072\n",
    "    Prob(Omnibus):                  0.852   Jarque-Bera (JB):                0.169\n",
    "    Skew:                           0.100   Prob(JB):                        0.919\n",
    "    Kurtosis:                       3.019   Cond. No.                         20.0\n",
    "    ==============================================================================\n",
    "    \n",
    "    Warnings:\n",
    "    [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Residual Sum of the Squares\n",
    "\n",
    "# Load the data\n",
    "x_data, y_data = load_data()\n",
    "\n",
    "# Model the data with specified values for parameters a0, a1\n",
    "y_model = model(x_data, a0=150, a1=25)\n",
    "\n",
    "# Compute the RSS value for this parameterization of the model\n",
    "rss = np.sum(np.square(y_model - y_data))\n",
    "print(\"RSS = {}\".format(rss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Minimizing the Residuals\n",
    "\n",
    "# Complete function to load data, build model, compute RSS, and plot\n",
    "def compute_rss_and_plot_fit(a0, a1):\n",
    "    xd, yd = load_data()\n",
    "    ym = model(xd, a0, a1)\n",
    "    residuals = ym - yd\n",
    "    rss = np.sum(np.square(residuals))\n",
    "    summary = \"Parameters a0={}, a1={} yield RSS={:0.2f}\".format(a0, a1, rss)\n",
    "    fig = plot_data_with_model(xd, yd, ym, summary)\n",
    "    return rss, summary\n",
    "\n",
    "# Chose model parameter values and pass them into RSS function\n",
    "rss, summary = compute_rss_and_plot_fit(a0=150, a1=25)\n",
    "\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualizing the RSS Minima\n",
    "\n",
    "# Loop over all trial values in a1_array, computing rss for each\n",
    "a1_array = np.linspace(15, 35, 101)\n",
    "for a1_trial in a1_array:\n",
    "    y_model = model(x_data, a0=150, a1=a1_trial)\n",
    "    rss_value = compute_rss(y_data, y_model)\n",
    "    rss_list.append(rss_value)\n",
    "\n",
    "# Find the minimum RSS and the a1 value from whence it came\n",
    "rss_array = np.array(rss_list)\n",
    "best_rss = np.min(rss_array) \n",
    "best_a1 = a1_array[np.where(rss_array==best_rss)]\n",
    "print('The minimum RSS = {}, came from a1 = {}'.format(best_rss, best_a1))\n",
    "\n",
    "# Plot your rss and a1 values to confirm answer\n",
    "fig = plot_rss_vs_a1(a1_array, rss_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Least-Squares with `numpy`\n",
    "\n",
    "# prepare the means and deviations of the two variables\n",
    "x_mean = np.sum(x)/len(x)\n",
    "y_mean = np.sum(y)/len(y)\n",
    "x_dev = x - np.mean(x)\n",
    "y_dev = y - np.mean(y)\n",
    "\n",
    "# Complete least-squares formulae to find the optimal a0, a1\n",
    "a1 = np.sum(x_dev * y_dev) / np.sum( np.square(x_dev) )\n",
    "a0 = np.mean(y) - (a1 * np.mean(x))\n",
    "\n",
    "# Use the those optimal model parameters a0, a1 to build a model\n",
    "y_model = model(x, a0, a1)\n",
    "\n",
    "# plot to verify that the resulting y_model best fits the data y\n",
    "fig, rss = compute_rss_and_plot_fit(a0, a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Optimization with Scipy\n",
    "\n",
    "# Define a model function needed as input to scipy\n",
    "def model_func(x, a0, a1):\n",
    "    return a0 + (a1*x)\n",
    "\n",
    "# Load the measured data you want to model\n",
    "x_data, y_data  = load_data()\n",
    "\n",
    "# call curve_fit, passing in the model function and data; then unpack the results\n",
    "param_opt, param_cov = optimize.curve_fit(model_func, x_data, y_data)\n",
    "a0 = param_opt[0]  # a0 is the intercept in y = a0 + a1*x\n",
    "a1 = param_opt[1]  # a1 is the slope     in y = a0 + a1*x\n",
    "\n",
    "# test that these parameters result in a model that fits the data\n",
    "fig, rss = compute_rss_and_plot_fit(a0, a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Least-Squares with `statsmodels`\n",
    "\n",
    "# Pass data and `formula` into ols(), use and `.fit()` the model to the data\n",
    "model_fit = ols(formula=\"y_column ~ x_column\", data=df).fit()\n",
    "\n",
    "# Use .predict(df) to get y_model values, then over-plot y_data with y_model\n",
    "y_model = model_fit.predict(df)\n",
    "fig = plot_data_with_model(x_data, y_model, y_model)\n",
    "\n",
    "# Extract the a0, a1 values from model_fit.params\n",
    "a0 = model_fit.params['Intercept']\n",
    "a1 = model_fit.params['x_column']\n",
    "\n",
    "# Visually verify that these parameters a0, a1 give the minimum RSS\n",
    "fig, rss = compute_rss_and_plot_fit(a0, a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODULE 3\n",
    "\n",
    "## Making Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Linear Model in Anthropology\n",
    "\n",
    "# import the sklearn class LinearRegression and initialize the model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression(fit_intercept=False)\n",
    "\n",
    "# Prepare the measured data arrays and fit the model to them\n",
    "legs = legs.reshape(len(legs),1)\n",
    "heights = heights.reshape(len(heights),1)\n",
    "model.fit(legs, heights)\n",
    "\n",
    "# Use the fitted model to make a prediction for the found femur\n",
    "fossil_leg = 50.7\n",
    "fossil_height = model.predict(fossil_leg)\n",
    "print(\"Predicted fossil height = {:0.2f} cm\".format(fossil_height[0,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Linear Model in Oceanography\n",
    "\n",
    "# Import LinearRegression class, build a model, fit to the data\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression(fit_intercept=True)\n",
    "model.fit(years, levels)\n",
    "\n",
    "# Use model to make a prediction for one year, 2100\n",
    "future_year = 2100\n",
    "future_level = model.predict(future_year)\n",
    "print(\"Prediction: year = {}, level = {:.02f}\".format(future_year, future_level[0,0]))\n",
    "\n",
    "# Use model to predict for many years, and over-plot with measured data\n",
    "years_forecast = np.linspace(1970, 2100, 131).reshape(-1, 1)\n",
    "levels_forecast = model.predict(years_forecast)\n",
    "fig = plot_data_and_forecast(years, levels, years_forecast, levels_forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Linear Model in Cosmology\n",
    "\n",
    "# Fit the model, based on the form of the formula\n",
    "model_fit = ols(formula=\"velocities ~ distances\", data=df).fit()\n",
    "\n",
    "# Extract the model parameters and associated \"errors\" or uncertainties\n",
    "a0 = model_fit.params['Intercept']\n",
    "a1 = model_fit.params['distances']\n",
    "e0 = model_fit.bse['Intercept']\n",
    "e1 = model_fit.bse['distances']\n",
    "\n",
    "# Print the results\n",
    "print('For slope a1={:.02f}, the uncertainty in a1 is {:.02f}'.format(a1, e1))\n",
    "print('For intercept a0={:.02f}, the uncertainty in a0 is {:.02f}'.format(a0, e0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Interpolation: Inbetween Times\n",
    "\n",
    "# build and fit a model to the df_monthly data\n",
    "model_fit = ols('Close ~ DayCount', data=df_monthly).fit()\n",
    "\n",
    "# Use the model FIT to the MONTHLY data to make a predictions for both monthly and daily data\n",
    "df_monthly['Model'] = model_fit.predict(df_monthly.DayCount)\n",
    "df_daily['Model'] = model_fit.predict(df_daily.DayCount)\n",
    "\n",
    "# Plot the monthly and daily data and model, compare the RSS values seen on the figures\n",
    "fig_monthly = plot_model_with_data(df_monthly)\n",
    "fig_daily = plot_model_with_data(df_daily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extrapolation: Going Over the Edge\n",
    "\n",
    "# Compute the residuals, \"data - model\", and determine where [residuals < tolerance]\n",
    "residuals = np.abs(y_model - y_data)\n",
    "tolerance = 100\n",
    "x_good = x_data[residuals < tolerance]\n",
    "\n",
    "# Find the min and max of the \"good\" values, and plot y_data, y_model, and the tolerance range\n",
    "print('Minimum good x value = {}'.format(np.min(x_good)))\n",
    "print('Maximum good x value = {}'.format(np.max(x_good)))\n",
    "fig = plot_data_model_tolerance(x_data, y_data, y_model, tolerance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RMSE Step-by-step\n",
    "\n",
    "# Build the model and compute the residuals \"model - data\"\n",
    "y_model = model_fit_and_predict(x_data, y_data)\n",
    "residuals = y_model - y_data\n",
    "\n",
    "# Compute the RSS, MSE, and RMSE and print the results\n",
    "RSS = np.sum(np.square(residuals))\n",
    "MSE = RSS/len(residuals)\n",
    "RMSE = np.sqrt(MSE)\n",
    "print('RMSE = {:0.2f}, MSE = {:0.2f}, RSS = {:0.2f}'.format(RMSE, MSE, RSS))\n",
    "\n",
    "# <script.py> output:\n",
    "#     RMSE = 26.23, MSE = 687.83, RSS = 14444.48"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Notice that instead of computing RSS and normalizing with division by len(residuals) to get the MSE, you could have just applied np.mean(np.square()) to the residuals. Another useful point to help you remember; you can think of the MSE like a variance, but instead of differencing the data from its mean, you difference the data and the model. Similarly, think of RMSE as a standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## R-Squared\n",
    "\n",
    "# Compute the residuals and the deviations\n",
    "residuals = y_model - y_data\n",
    "deviations = np.mean(y_data) - y_data\n",
    "\n",
    "# Compute the variance of the residuals and deviations\n",
    "var_residuals = np.sum(np.square(residuals))\n",
    "var_deviations = np.sum(np.square(deviations))\n",
    "\n",
    "# Compute r_squared as 1 - the ratio of RSS/Variance\n",
    "r_squared = 1 - (var_residuals / var_deviations)\n",
    "print('R-squared is {:0.2f}'.format(r_squared))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Variation Around the Trend\n",
    "\n",
    "# Store x_data and y_data, as times and distances, in df, and use ols() to fit a model to it.\n",
    "df = pd.DataFrame(dict(times=x_data, distances=y_data))\n",
    "model_fit = ols(formula=\"distances ~ times\", data=df).fit()\n",
    "\n",
    "# Extact the model parameters and their uncertainties\n",
    "a0 = model_fit.params['Intercept']\n",
    "e0 = model_fit.bse['Intercept']\n",
    "a1 = model_fit.params['times']\n",
    "e1 = model_fit.bse['times']\n",
    "\n",
    "# Print the results with more meaningful names\n",
    "print('Estimate    of the intercept = {:0.2f}'.format(a0))\n",
    "print('Uncertainty of the intercept = {:0.2f}'.format(e0))\n",
    "print('Estimate    of the slope = {:0.2f}'.format(a1))\n",
    "print('Uncertainty of the slope = {:0.2f}'.format(e1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Variation in Two Parts\n",
    "\n",
    "# Build and fit two models, for columns distances1 and distances2 in df\n",
    "model_1 = ols(formula=\"distances1 ~ times\", data=df).fit()\n",
    "model_2 = ols(formula=\"distances2 ~ times\", data=df).fit()\n",
    "\n",
    "# Extract R-squared for each model, and the standard error for each slope\n",
    "se_1 = model_1.bse['times']\n",
    "se_2 = model_2.bse['times']\n",
    "rsquared_1 = model_1.rsquared\n",
    "rsquared_2 = model_2.rsquared\n",
    "\n",
    "# Print the results\n",
    "print('Model 1: SE = {:0.3f}, R-squared = {:0.3f}'.format(se_1, rsquared_1))\n",
    "print('Model 2: SE = {:0.3f}, R-squared = {:0.3f}'.format(se_2, rsquared_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODULE 4\n",
    "\n",
    "### Estimating Model Parameters"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In our final chapter, we introduce concepts from inferential statistics, and use them to explore how maximum likelihood estimation and bootstrap resampling can be used to estimate linear model parameters. We then apply these methods to make probabilistic statements about our confidence in the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sample Statistics versus Population\n",
    "\n",
    "# Compute the population statistics\n",
    "print(\"Population mean {:.1f}, stdev {:.2f}\".format( population.mean(), population.std() ))\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Construct a sample by randomly sampling 31 points from the population\n",
    "sample = np.random.choice(population, size=31)\n",
    "\n",
    "# Compare sample statistics to the population statistics\n",
    "print(\"    Sample mean {:.1f}, stdev {:.2f}\".format( sample.mean(), sample.std() ))\n",
    "\n",
    "# <script.py> output:\n",
    "#     Population mean 100.0, stdev 9.74\n",
    "#         Sample mean 102.1, stdev 9.34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Variation in Sample Statistics\n",
    "\n",
    "# Initialize two arrays of zeros to be used as containers\n",
    "means = np.zeros(num_samples)\n",
    "stdevs = np.zeros(num_samples)\n",
    "\n",
    "# For each iteration, compute and store the sample mean and sample stdev\n",
    "for ns in range(num_samples):\n",
    "    sample = np.random.choice(population, num_pts)\n",
    "    means[ns] = sample.mean()\n",
    "    stdevs[ns] = sample.std()\n",
    "\n",
    "# Compute and print the mean() and std() for the sample statistic distributions\n",
    "print(\"Means:  center={:>6.2f}, spread={:>6.2f}\".format(means.mean(), means.std()))\n",
    "print(\"Stdevs: center={:>6.2f}, spread={:>6.2f}\".format(stdevs.mean(), stdevs.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualizing Variation of a Statistic\n",
    "\n",
    "# Generate sample distribution and associated statistics\n",
    "means, stdevs = get_sample_statistics(population, num_samples=100, num_pts=1000)\n",
    "\n",
    "# Define the binning for the histograms\n",
    "mean_bins = np.linspace(97.5, 102.5, 51)\n",
    "std_bins = np.linspace(7.5, 12.5, 51)\n",
    "\n",
    "# Plot the distribution of means, and the distribution of stdevs\n",
    "fig = plot_hist(data=means, bins=mean_bins, data_name=\"Means\", color='green')\n",
    "fig = plot_hist(data=stdevs, bins=std_bins, data_name=\"Stdevs\", color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Estimation of Population Parameters\n",
    "\n",
    "# Compute the mean and standard deviation of the sample_distances\n",
    "sample_mean = np.mean(sample_distances)\n",
    "sample_stdev = np.std(sample_distances)\n",
    "\n",
    "# Use the sample mean and stdev as estimates of the population model parameters mu and sigma\n",
    "population_model = gaussian_model(sample_distances, mu=sample_mean, sigma=sample_stdev)\n",
    "\n",
    "# Plot the model and data to see how they compare\n",
    "fig = plot_data_and_model(sample_distances, population_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Maximizing Likelihood, Part 1\n",
    "\n",
    "# Compute sample mean and stdev, for use as model parameter value guesses\n",
    "mu_guess = np.mean(sample_distances)\n",
    "sigma_guess = np.std(sample_distances)\n",
    "\n",
    "# For each sample distance, compute the probability modeled by the parameter guesses\n",
    "probs = np.zeros(len(sample_distances))\n",
    "for n, distance in enumerate(sample_distances):\n",
    "    probs[n] = gaussian_model(distance, mu=mu_guess, sigma=sigma_guess)\n",
    "\n",
    "# Compute and print the log-likelihood as the sum() of the log() of the probabilities\n",
    "loglikelihood = np.sum(np.log(probs))\n",
    "print('For guesses mu={:0.2f} and sigma={:0.2f}, the loglikelihood={:0.2f}'.format(mu_guess, sigma_guess, loglikelihood))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Maximizing Likelihood, Part 2\n",
    "\n",
    "# Create an array of mu guesses, centered on sample_mean, spread out +/- by sample_stdev\n",
    "low_guess = sample_mean - 2*sample_stdev\n",
    "high_guess = sample_mean + 2*sample_stdev\n",
    "mu_guesses = np.linspace(low_guess, high_guess, 101)\n",
    "\n",
    "# Compute the loglikelihood for each model created from each guess value\n",
    "loglikelihoods = np.zeros(len(mu_guesses))\n",
    "for n, mu_guess in enumerate(mu_guesses):\n",
    "    loglikelihoods[n] = compute_loglikelihood(sample_distances, \n",
    "                                              mu=mu_guess, sigma=sample_stdev)\n",
    "\n",
    "# Find the best guess by using logical indexing, the print and plot the result\n",
    "best_mu = mu_guesses[loglikelihoods==np.max(loglikelihoods)]\n",
    "print('Maximum loglikelihood found for best mu guess={}'.format(best_mu))\n",
    "fig = plot_loglikelihoods(mu_guesses, loglikelihoods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bootstrap and Standard Error\n",
    "\n",
    "# Use the sample_data as a model for the population\n",
    "population_model = sample_data\n",
    "\n",
    "# Resample the population_model 100 times, computing the mean each sample\n",
    "for nr in range(num_resamples):\n",
    "    bootstrap_sample = np.random.choice(population_model, size=resample_size, replace=True)\n",
    "    bootstrap_means[nr] = np.mean(bootstrap_sample)\n",
    "\n",
    "# Compute and print the mean, stdev of the resample distribution of means\n",
    "distribution_mean = np.mean(bootstrap_means)\n",
    "standard_error = np.std(bootstrap_means)\n",
    "print('Bootstrap Distribution: center={:0.1f}, spread={:0.1f}'.format(distribution_mean, standard_error))\n",
    "\n",
    "# Plot the bootstrap resample distribution of means\n",
    "fig = plot_data_hist(bootstrap_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Estimating Speed and Confidence\n",
    "\n",
    "# Resample each preloaded population, and compute speed distribution\n",
    "population_inds = np.arange(0, 99, dtype=int)\n",
    "for nr in range(num_resamples):\n",
    "    sample_inds = np.random.choice(population_inds, size=100, replace=True)\n",
    "    sample_inds.sort()\n",
    "    sample_distances = distances[sample_inds]\n",
    "    sample_times = times[sample_inds]\n",
    "    a0, a1 = least_squares(sample_times, sample_distances)\n",
    "    resample_speeds[nr] = a1\n",
    "\n",
    "# Compute effect size and confidence interval, and print\n",
    "speed_estimate = np.mean(resample_speeds)\n",
    "ci_90 = np.percentile(resample_speeds, [5, 95])\n",
    "print('Speed Estimate = {:0.2f}, 90% Confidence Interval: {:0.2f}, {:0.2f} '.format(speed_estimate, ci_90[0], ci_90[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize the Bootstrap\n",
    "\n",
    "# Create the bootstrap distribution of speeds\n",
    "resample_speeds = compute_resample_speeds(distances, times)\n",
    "speed_estimate = np.mean(resample_speeds)\n",
    "percentiles = np.percentile(resample_speeds, [5, 95])\n",
    "\n",
    "# Plot the histogram with the estimate and confidence interval\n",
    "fig, axis = plt.subplots()\n",
    "hist_bin_edges = np.linspace(0.0, 4.0, 21)\n",
    "axis.hist(resample_speeds, hist_bin_edges, color='green', alpha=0.35, rwidth=0.8)\n",
    "axis.axvline(speed_estimate, label='Estimate', color='black')\n",
    "axis.axvline(percentiles[0], label=' 5th', color='blue')\n",
    "axis.axvline(percentiles[1], label='95th', color='blue')\n",
    "axis.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test Statistics and Effect Size\n",
    "\n",
    "# Create two poulations, sample_distances for early and late sample_times.\n",
    "# Then resample with replacement, taking 500 random draws from each population.\n",
    "group_duration_short = sample_distances[sample_times < 5]\n",
    "group_duration_long = sample_distances[sample_times > 5]\n",
    "resample_short = np.random.choice(group_duration_short, size=500, replace=True)\n",
    "resample_long = np.random.choice(group_duration_long, size=500, replace=True)\n",
    "\n",
    "# Difference the resamples to compute a test statistic distribution, then compute its mean and stdev\n",
    "test_statistic = resample_long - resample_short\n",
    "effect_size = np.mean(test_statistic)\n",
    "standard_error = np.std(test_statistic)\n",
    "\n",
    "# Print and plot the results\n",
    "print('Test Statistic: mean={:0.2f}, stdev={:0.2f}'.format(effect_size, standard_error))\n",
    "fig = plot_test_statistic(test_statistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Null Hypothesis\n",
    " \n",
    "# In this exercise, we formulate the null hypothesis as\n",
    "# short and long time durations have no effect on total distance traveled.\n",
    "\n",
    "# We interpret the \"zero effect size\" to mean that if we shuffled samples \n",
    "# between short and long times, so that two new samples each have a mix of \n",
    "# short and long duration trips, and then compute the test statistic, on average it will be zero.\n",
    "\n",
    "# In this exercise, your goal is to perform the shuffling and resampling. Start with \n",
    "# the predefined group_duration_short and group_duration_long which are the \n",
    "# un-shuffled time duration groups.\n",
    "\n",
    "\n",
    "# Shuffle the time-ordered distances, then slice the result into two populations.\n",
    "shuffle_bucket = np.concatenate((group_duration_short, group_duration_long))\n",
    "np.random.shuffle(shuffle_bucket)\n",
    "slice_index = len(shuffle_bucket)//2\n",
    "shuffled_half1 = shuffle_bucket[0:slice_index]\n",
    "shuffled_half2 = shuffle_bucket[slice_index+1:]\n",
    "\n",
    "# Create new samples from each shuffled population, and compute the test statistic\n",
    "resample_half1 = np.random.choice(shuffled_half1, size=500, replace=True)\n",
    "resample_half2 = np.random.choice(shuffled_half2, size=500, replace=True)\n",
    "test_statistic = resample_half2 - resample_half1\n",
    "\n",
    "# Compute and print the effect size\n",
    "effect_size = np.mean(test_statistic)\n",
    "print('Test Statistic, after shuffling, mean = {}'.format(effect_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualizing Test Statistics\n",
    "\n",
    "# From the unshuffled groups, compute the test statistic distribution\n",
    "resample_short = np.random.choice(group_duration_short, size=500, replace=True)\n",
    "resample_long = np.random.choice(group_duration_long, size=500, replace=True)\n",
    "test_statistic_unshuffled = resample_long - resample_short\n",
    "\n",
    "# Shuffle two populations, cut in half, and recompute the test statistic\n",
    "shuffled_half1, shuffled_half2 = shuffle_and_split(group_duration_short, group_duration_long)\n",
    "resample_half1 = np.random.choice(shuffled_half1, size=500, replace=True)\n",
    "resample_half2 = np.random.choice(shuffled_half2, size=500, replace=True)\n",
    "test_statistic_shuffled = resample_half2 - resample_half1\n",
    "\n",
    "# Plot both the unshuffled and shuffled results and compare\n",
    "fig = plot_test_statistic(test_statistic_unshuffled, label='Unshuffled')\n",
    "fig = plot_test_statistic(test_statistic_shuffled, label='Shuffled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualizing the P-Value\n",
    "\n",
    "# Compute the test stat distribution and effect size for two population groups\n",
    "test_statistic_unshuffled = compute_test_statistic(group_duration_short, group_duration_long)\n",
    "effect_size = np.mean(test_statistic_unshuffled)\n",
    "\n",
    "# Randomize the two populations, and recompute the test stat distribution\n",
    "shuffled_half1, shuffled_half2 = shuffle_and_split(group_duration_short, group_duration_long)\n",
    "test_statistic_shuffled = compute_test_statistic(shuffled_half1, shuffled_half2)\n",
    "\n",
    "# Compute the p-value as the proportion of shuffled test stat values >= the effect size\n",
    "condition = test_statistic_shuffled >= effect_size\n",
    "p_value = len(test_statistic_shuffled[condition]) / len(test_statistic_shuffled)\n",
    "\n",
    "# Print p-value and overplot the shuffled and unshuffled test statistic distributions\n",
    "print(\"The p-value is = {}\".format(p_value))\n",
    "fig = plot_test_stats_and_pvalue(test_statistic_unshuffled, test_statistic_shuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
