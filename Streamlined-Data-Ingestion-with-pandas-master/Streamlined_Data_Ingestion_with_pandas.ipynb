{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data from Flat Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data from CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas as pd\n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV and assign it to the variable data\n",
    "data = pd.read_csv('vt_tax_data_2016.csv')\n",
    "\n",
    "# View the first few lines of data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data from other flat files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas with the alias pd\n",
    "import pandas as pd\n",
    "\n",
    "# Load TSV using the sep keyword argument to set delimiter\n",
    "data = pd.read_csv('vt_tax_data_2016.tsv', sep ='\\t')\n",
    "\n",
    "# Plot the total number of tax returns by income group\n",
    "counts = data.groupby(\"agi_stub\").N1.sum()\n",
    "counts.plot.bar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import a subset of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of columns to use\n",
    "cols = [\"zipcode\", \"agi_stub\", \"mars1\", \"MARS2\", \"NUMDEP\"]\n",
    "\n",
    "# Create data frame from csv using only selected columns\n",
    "data = pd.read_csv(\"vt_tax_data_2016.csv\", usecols=cols)\n",
    "\n",
    "# View counts of dependents and tax returns by income level\n",
    "print(data.groupby(\"agi_stub\").sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import a file in chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data frame of next 500 rows with labeled columns\n",
    "vt_data_next500 = pd.read_csv(\"vt_tax_data_2016.csv\", \n",
    "                       \t\t  nrows=500,\n",
    "                       \t\t  skiprows=500,\n",
    "                       \t\t  header=None,\n",
    "                       \t\t  names = list(vt_data_first500.columns))\n",
    "\n",
    "# View the Vermont data frames to confirm they're different\n",
    "print(vt_data_first500.head())\n",
    "print(vt_data_next500.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dict specifying data types for agi_stub and zipcode\n",
    "data_types = {'agi_stub': 'category',\n",
    "\t\t\t  'zipcode':'str'}\n",
    "\n",
    "# Load csv using dtype to set correct data types\n",
    "data = pd.read_csv(\"vt_tax_data_2016.csv\", dtype = data_types)\n",
    "\n",
    "# Print data types of resulting frame\n",
    "print(data.dtypes.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set custom NA values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dict specifying that 0s in zipcode are NA values\n",
    "null_values = {'zipcode': 0}\n",
    "\n",
    "# Load csv using na_values keyword argument\n",
    "data = pd.read_csv(\"vt_tax_data_2016.csv\", \n",
    "                   na_values = null_values)\n",
    "\n",
    "# View rows with NA ZIP codes\n",
    "print(data[data.zipcode.isna()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip bad data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  # Set warn_bad_lines to issue warnings about bad records\n",
    "    data = pd.read_csv(\"vt_tax_data_2016_corrupt.csv\", \n",
    "                     error_bad_lines=False, \n",
    "                     warn_bad_lines = True)\n",
    "\n",
    "  # View first 5 records\n",
    "    print(data.head())\n",
    "except pd.io.common.CParserError:\n",
    "    print(\"Your data contained rows that could not be parsed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data From Excel Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data from a spreadsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pandas as pd\n",
    "import pandas as pd\n",
    "\n",
    "# Read spreadsheet and assign it to survey_responses\n",
    "survey_responses = pd.read_excel('fcc_survey.xlsx')\n",
    "\n",
    "# View the head of the data frame\n",
    "print(survey_responses.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a portion of a spreadsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create string of lettered columns to load\n",
    "col_string = \"AD, AW:BA\"\n",
    "\n",
    "# Load data with skiprows and usecols set\n",
    "survey_responses = pd.read_excel(\"fcc_survey_headers.xlsx\", \n",
    "                                 skiprows=2, \n",
    "                                 usecols=col_string)\n",
    "\n",
    "# View the names of the columns selected\n",
    "print(survey_responses.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select a single sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create df from second worksheet by referencing its position\n",
    "responses_2017 = pd.read_excel(\"fcc_survey.xlsx\",\n",
    "                               sheet_name = '2017')\n",
    "\n",
    "# Graph where people would like to get a developer job\n",
    "job_prefs = responses_2017.groupby(\"JobPref\").JobPref.count()\n",
    "job_prefs.plot.barh()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select multiple sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load both the 2016 and 2017 sheets by name\n",
    "all_survey_data = pd.read_excel(\"fcc_survey.xlsx\",\n",
    "                                sheet_name=['2016', '2017'])\n",
    "\n",
    "# View the data type of all_survey_data\n",
    "print(type(all_survey_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all sheets in the Excel file\n",
    "all_survey_data = pd.read_excel(\"fcc_survey.xlsx\",\n",
    "                                sheet_name = [0, '2017'])\n",
    "\n",
    "# View the sheet names in all_survey_data\n",
    "print(all_survey_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all sheets in the Excel file\n",
    "all_survey_data = pd.read_excel(\"fcc_survey.xlsx\",\n",
    "                                sheet_name = None)\n",
    "\n",
    "# View the sheet names in all_survey_data\n",
    "print(all_survey_data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work with multiple spreadsheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty data frame\n",
    "all_responses = pd.DataFrame()\n",
    "\n",
    "# Set up for loop to iterate through values in responses\n",
    "for df in responses.values():\n",
    "  # Print the number of rows being added\n",
    "  print(\"Adding {} rows\".format(df.shape[0]))\n",
    "  # Append df to all_responses, assign result\n",
    "  all_responses = all_responses.append(df)\n",
    "\n",
    "# Graph employment statuses in sample\n",
    "counts = all_responses.groupby(\"EmploymentStatus\").EmploymentStatus.count()\n",
    "counts.plot.barh()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Boolean columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set dtype to load appropriate column(s) as Boolean data\n",
    "survey_data = pd.read_excel(\"fcc_survey_subset.xlsx\",\n",
    "                            dtype = {'HasDebt': 'bool'})\n",
    "\n",
    "# View financial burdens by Boolean group\n",
    "print(survey_data.groupby('HasDebt').sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set custom true/false values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load file with Yes as a True value and No as a False value\n",
    "survey_subset = pd.read_excel(\"fcc_survey_yn_data.xlsx\",\n",
    "                              dtype={\"HasDebt\": bool,\n",
    "                              \"AttendedBootCampYesNo\": bool},\n",
    "                              true_values=[\"Yes\"],\n",
    "                              false_values=[\"No\"])\n",
    "\n",
    "# View the data\n",
    "print(survey_subset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse simple dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load file, with Part1StartTime parsed as datetime data\n",
    "survey_data = pd.read_excel(\"fcc_survey.xlsx\",\n",
    "                            parse_dates = ['Part1StartTime'])\n",
    "\n",
    "# Print first few values of Part1StartTime\n",
    "print(survey_data.Part1StartTime.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get datetimes from multiple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dict of columns to combine into new datetime column\n",
    "datetime_cols = {\"Part2Start\": ['Part2StartDate', 'Part2StartTime']}\n",
    "\n",
    "\n",
    "# Load file, supplying the dict to parse_dates\n",
    "survey_data = pd.read_excel(\"fcc_survey_dts.xlsx\",\n",
    "                            parse_dates = datetime_cols)\n",
    "\n",
    "# View summary statistics about Part2Start\n",
    "print(survey_data.Part2Start.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse non-standard date formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse datetimes and assign result back to Part2EndTime\n",
    "survey_data[\"Part2EndTime\"] = pd.to_datetime(survey_data[\"Part2EndTime\"], \n",
    "                                             format=\"%m%d%Y %H:%M:%S\")\n",
    "\n",
    "# Print first few values of Part2EndTime\n",
    "print(survey_data[\"Part2EndTime\"].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data from Databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to a database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import sqlalchemy's create_engine() function\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Create the database engine\n",
    "engine = create_engine('sqlite:///data.db')\n",
    "\n",
    "# View the tables in the database\n",
    "print(engine.table_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load entire tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Create the database engine\n",
    "engine = create_engine('sqlite:///data.db')\n",
    "\n",
    "# Load hpd311calls without any SQL\n",
    "hpd_calls = pd.read_sql('hpd311calls',  engine)\n",
    "\n",
    "# View the first few rows of data\n",
    "print(hpd_calls.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the database engine\n",
    "engine = create_engine(\"sqlite:///data.db\")\n",
    "\n",
    "# Create a SQL query to load the entire weather table\n",
    "query = \"\"\"\n",
    "SELECT * \n",
    "  FROM weather;\n",
    "\"\"\"\n",
    "\n",
    "# Load weather with the SQL query\n",
    "weather = pd.read_sql(query, engine)\n",
    "\n",
    "# View the first few rows of data\n",
    "print(weather.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting columns with SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create database engine for data.db\n",
    "engine = create_engine('sqlite:///data.db')\n",
    "\n",
    "# Write query to get date, tmax, and tmin from weather\n",
    "query = \"\"\"\n",
    "SELECT date, tmax, tmin\n",
    "FROM weather;\n",
    "\"\"\"\n",
    "\n",
    "# Make a data frame by passing query and engine to read_sql()\n",
    "temperatures = pd.read_sql(query, engine)\n",
    "\n",
    "# View the resulting data frame\n",
    "print(temperatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create query to get hpd311calls records about safety\n",
    "query = \"\"\"SELECT *\n",
    "\t\t     FROM hpd311calls\n",
    "            WHERE complaint_type = 'SAFETY';\"\"\"\n",
    "\n",
    "# Query the database and assign result to safety_calls\n",
    "safety_calls = pd.read_sql(query, engine)\n",
    "\n",
    "# Graph the number of safety calls by borough\n",
    "call_counts = safety_calls.groupby('borough').unique_key.count()\n",
    "call_counts.plot.barh()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering on multiple conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create query for records with max temps <= 32 or snow >= 1\n",
    "query = \"\"\"\n",
    "SELECT *\n",
    "  FROM weather\n",
    "WHERE tmax <= 32 OR\n",
    "snow >=1;\n",
    "\"\"\"\n",
    "\n",
    "# Query database and assign result to wintry_days\n",
    "wintry_days = pd.read_sql(query, engine)\n",
    "\n",
    "# View summary stats about the temperatures\n",
    "print(wintry_days.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting distinct values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create query for unique combinations of borough and complaint_type\n",
    "query = \"\"\"\n",
    "SELECT DISTINCT borough, \n",
    "       complaint_type\n",
    "  FROM hpd311calls;\n",
    "\"\"\"\n",
    "\n",
    "# Load results of query to a data frame\n",
    "issues_and_boros = pd.read_sql(query, engine)\n",
    "\n",
    "# Check assumption about issues and boroughs\n",
    "print(issues_and_boros)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting in groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create query to get call counts by complaint_type\n",
    "query = \"\"\"\n",
    "SELECT complaint_type, \n",
    "COUNT(*)\n",
    "  FROM hpd311calls\n",
    "GROUP BY complaint_type;\n",
    "\"\"\"\n",
    "\n",
    "# Create data frame of call counts by issue\n",
    "calls_by_issue = pd.read_sql(query, engine)\n",
    "\n",
    "# Graph the number of calls for each housing issue\n",
    "calls_by_issue.plot.barh(x=\"complaint_type\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with aggregate functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create query to get temperature and precipitation by month\n",
    "query = \"\"\"\n",
    "SELECT month, \n",
    "        MAX(tmax), \n",
    "        MIN(tmin),\n",
    "        SUM(prcp)\n",
    "  FROM weather \n",
    " GROUP BY month;\n",
    "\"\"\"\n",
    "\n",
    "# Get data frame of monthly weather stats\n",
    "weather_by_month = pd.read_sql(query, engine)\n",
    "\n",
    "# View weather stats by month\n",
    "print(weather_by_month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query to join weather to call records by date columns\n",
    "query = \"\"\"\n",
    "SELECT * \n",
    "  FROM hpd311calls\n",
    "  JOIN weather\n",
    "  ON hpd311calls.created_date = weather.date;\n",
    "\"\"\"\n",
    "\n",
    "# Create data frame of joined tables\n",
    "calls_with_weather = pd.read_sql(query, engine)\n",
    "\n",
    "# View the data frame to make sure all columns were joined\n",
    "print(calls_with_weather.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining and filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query to get water leak calls and daily precipitation\n",
    "query = \"\"\"\n",
    "SELECT hpd311calls.*, weather.prcp\n",
    "  FROM hpd311calls\n",
    "  JOIN weather\n",
    "    ON hpd311calls.created_date = weather.date\n",
    " WHERE hpd311calls.complaint_type = 'WATER LEAK';\n",
    "\"\"\"\n",
    "\n",
    "# Load query results into the leak_calls data frame\n",
    "leak_calls = pd.read_sql(query, engine)\n",
    "\n",
    "# View the data frame\n",
    "print(leak_calls.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining, filtering, and aggregating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify query to join tmax and tmin from weather by date\n",
    "query = \"\"\"\n",
    "SELECT created_date, \n",
    "       COUNT(*), \n",
    "       weather.tmax, \n",
    "       weather.tmin\n",
    "  FROM hpd311calls \n",
    "       JOIN weather \n",
    "       ON hpd311calls.created_date = weather.date\n",
    " WHERE complaint_type = 'HEAT/HOT WATER' \n",
    " GROUP BY created_date;\"\"\"\n",
    "\n",
    "# Query database and save results as df\n",
    "df = pd.read_sql(query, engine)\n",
    "\n",
    "# View first 5 records\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing JSON Data and Working with APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load JSON data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pandas as pd\n",
    "import pandas as pd\n",
    "\n",
    "# Load the daily report to a data frame\n",
    "pop_in_shelters = pd.read_json('dhs_daily_report.json')\n",
    "\n",
    "# View summary stats about pop_in_shelters\n",
    "print(pop_in_shelters.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work with JSON orientations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Load the JSON with orient specified\n",
    "    df = pd.read_json(\"dhs_report_reformatted.json\",\n",
    "                      orient = 'split')\n",
    "    \n",
    "    # Plot total population in shelters over time\n",
    "    df[\"date_of_census\"] = pd.to_datetime(df[\"date_of_census\"])\n",
    "    df.plot(x=\"date_of_census\", \n",
    "            y=\"total_individuals_in_shelter\")\n",
    "    plt.show()\n",
    "    \n",
    "except ValueError:\n",
    "    print(\"pandas could not parse the JSON.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data from an API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_url = \"https://api.yelp.com/v3/businesses/search\"\n",
    "\n",
    "# Get data about NYC cafes from the Yelp API\n",
    "response = requests.get(api_url, \n",
    "                headers=headers, \n",
    "                params=params)\n",
    "\n",
    "# Extract JSON data from the response\n",
    "data = response.json()\n",
    "\n",
    "# Load data to a data frame\n",
    "cafes = pd.DataFrame(data['businesses'])\n",
    "\n",
    "# View the data's dtypes\n",
    "print(cafes.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set API parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary to query API for cafes in NYC\n",
    "parameters = {'term': 'cafe',\n",
    "          \t  'location': 'NYC'}\n",
    "\n",
    "# Query the Yelp API with headers and params set\n",
    "response = requests.get(api_url,\n",
    "                headers = headers,\n",
    "                params = parameters)\n",
    "\n",
    "# Extract JSON data from response\n",
    "data = response.json()\n",
    "\n",
    "# Load \"businesses\" values to a data frame and print head\n",
    "cafes = pd.DataFrame(data['businesses'])\n",
    "print(cafes.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set request headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary that passes Authorization and key string\n",
    "headers = {\"Authorization\": \"Bearer {}\".format(api_key)}\n",
    "\n",
    "# Query the Yelp API with headers and params set\n",
    "response = requests.get(api_url, \n",
    "                        headers=headers, \n",
    "                        params=params)\n",
    "\n",
    "# Extract JSON data from response\n",
    "data = response.json()\n",
    "\n",
    "# Load \"businesses\" values to a data frame and print names\n",
    "cafes = pd.DataFrame(data[\"businesses\"])\n",
    "print(cafes.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten nested JSONs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load json_normalize()\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "# Isolate the JSON data from the API response\n",
    "data = response.json()\n",
    "\n",
    "# Flatten business data into a data frame, replace separator\n",
    "cafes = json_normalize(data[\"businesses\"],\n",
    "             sep = '_')\n",
    "\n",
    "# View data\n",
    "print(cafes.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle deeply nested data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load other business attributes and set meta prefix\n",
    "flat_cafes = json_normalize(data[\"businesses\"],\n",
    "                            sep=\"_\",\n",
    "                    \t\trecord_path=\"categories\",\n",
    "                    \t\tmeta=['name', \n",
    "                                  'alias',  \n",
    "                                  'rating',\n",
    "                          \t\t  ['coordinates', 'latitude'], \n",
    "                          \t\t  ['coordinates', 'longitude']],\n",
    "                    \t\tmeta_prefix='biz_')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# View the data\n",
    "print(flat_cafes.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Append data frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add an offset parameter to get cafes 51-100\n",
    "params = {\"term\": \"cafe\", \n",
    "          \"location\": \"NYC\",\n",
    "          \"sort_by\": \"rating\", \n",
    "          \"limit\": 50,\n",
    "          \"offset\": 50}\n",
    "\n",
    "result = requests.get(api_url, headers=headers, params=params)\n",
    "next_50_cafes = json_normalize(result.json()[\"businesses\"])\n",
    "\n",
    "# Append the results, setting ignore_index to renumber rows\n",
    "cafes = top_50_cafes.append(next_50_cafes, ignore_index = True)\n",
    "\n",
    "# Print shape of cafes\n",
    "print(cafes.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the crosswalk data into cafes on zip code fields\n",
    "cafes_with_pumas = cafes.merge(crosswalk, \n",
    "                   \t\t\t   left_on='location_zip_code', \n",
    "                               right_on='zipcode')\n",
    "\n",
    "# Merge in population data on puma field\n",
    "cafes_with_pop = cafes_with_pumas.merge(pop_data, on='puma')\n",
    "\n",
    "# View the data\n",
    "print(cafes_with_pop.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
